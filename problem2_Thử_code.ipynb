{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a2bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mã HOSE: 415\n",
      "Fetching data, it may take a while. Please wait...\n",
      "History ban đầu:   ticker         timestamp      open      high       low     close     volume  \\\n",
      "0    AAA  2023-01-03 00:00  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA  2023-01-04 00:00  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA  2023-01-05 00:00  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA  2023-01-06 00:00  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA  2023-01-09 00:00  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs           fn  \n",
      "0  938600.0  504700.0   40579000.0  899404000.0  \n",
      "1  462900.0  780600.0  151639000.0   36850000.0  \n",
      "2  487200.0  473700.0  343911000.0  -59103000.0  \n",
      "3  564300.0  828300.0  345999000.0 -294312000.0  \n",
      "4  414000.0  631800.0  514557000.0 -483197000.0  \n"
     ]
    }
   ],
   "source": [
    "# Block 1 — Login & Lấy dữ liệu tất cả HOSE/HNX/UPCOM\n",
    "import pandas as pd\n",
    "from FiinQuantX import FiinSession, BarDataUpdate\n",
    "\n",
    "# --- Login ---\n",
    "username = \"DSTC_18@fiinquant.vn\"\n",
    "password = \"Fiinquant0606\"\n",
    "\n",
    "client = FiinSession(\n",
    "    username=username,\n",
    "    password=password\n",
    ").login()\n",
    "\n",
    "# --- Lấy danh sách cổ phiếu từng sàn ---\n",
    "tickers_hose  = list(client.TickerList(ticker=\"VNINDEX\"))     # HOSE\n",
    "print(f\"Số mã HOSE: {len(tickers_hose)}\")\n",
    "\n",
    "# --- Lấy dữ liệu lịch sử toàn bộ (có thể nặng, nên lấy theo batch nếu cần) ---\n",
    "event_history = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2023-01-01\"   # backtest từ 2023 tới nay\n",
    ")\n",
    "\n",
    "df_all = event_history.get_data()\n",
    "print(\"History ban đầu:\", df_all.head())\n",
    "\n",
    "# --- Callback realtime ---\n",
    "def onDataUpdate(data: BarDataUpdate):\n",
    "    global df_all\n",
    "    df_update = data.to_dataFrame()\n",
    "    df_all = pd.concat([df_all, df_update])\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    print(\"Realtime update:\")\n",
    "    print(df_update.head())\n",
    "\n",
    "# --- Bật realtime nối tiếp dữ liệu ---\n",
    "event_realtime = client.Fetch_Trading_Data(\n",
    "    realtime=True,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    period=1,\n",
    "    callback=onDataUpdate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c93b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Lỗi khi lấy FA cho FUETPVND: 'FUETPVND'\n",
      "Số mã HOSE ban đầu: 415\n",
      "Số mã có dữ liệu FA: 392\n",
      "FA Data sample:\n",
      "   organizationId ticker  year  quarter  \\\n",
      "0          894364    CCC  2023        4   \n",
      "1          894364    CCC  2024        1   \n",
      "2          894364    CCC  2024        2   \n",
      "3          894364    CCC  2024        3   \n",
      "4          894364    CCC  2024        4   \n",
      "\n",
      "                                              ratios ReportDate  \n",
      "0  {'SolvencyRatio': {'DebtToEquityRatio': 1.5102...        NaT  \n",
      "1  {'SolvencyRatio': {'DebtToEquityRatio': 0.7722...        NaT  \n",
      "2  {'SolvencyRatio': {'DebtToEquityRatio': 0.7357...        NaT  \n",
      "3  {'SolvencyRatio': {'DebtToEquityRatio': 0.7914...        NaT  \n",
      "4  {'SolvencyRatio': {'DebtToEquityRatio': 0.6437...        NaT  \n"
     ]
    }
   ],
   "source": [
    "# Block 2 — Lấy dữ liệu FA theo quý (HOSE only)\n",
    "\n",
    "def fetch_fa_quarterly(ticker, latest_year=2025, n_periods=32):\n",
    "    try:\n",
    "        fi_list = client.FundamentalAnalysis().get_ratios(\n",
    "            tickers=[ticker],\n",
    "            TimeFilter=\"Quarterly\",\n",
    "            LatestYear=latest_year,\n",
    "            NumberOfPeriod=n_periods,\n",
    "            Consolidated=True\n",
    "        )\n",
    "\n",
    "        # Nếu không có dữ liệu thì bỏ qua\n",
    "        if not fi_list or not isinstance(fi_list, list):\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(fi_list)\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df[\"ticker\"] = ticker\n",
    "        if \"ReportDate\" in df.columns:\n",
    "            df[\"ReportDate\"] = pd.to_datetime(df[\"ReportDate\"])\n",
    "        else:\n",
    "            # Nếu không có ReportDate thì tạo cột null để tránh lỗi concat\n",
    "            df[\"ReportDate\"] = pd.NaT\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi lấy FA cho {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- Lọc danh sách: chỉ giữ những mã có dữ liệu FA ---\n",
    "fa_list = []\n",
    "valid_tickers = []\n",
    "\n",
    "for t in tickers_hose:   # lấy theo danh sách HOSE từ Block 1\n",
    "    df_fa = fetch_fa_quarterly(t, latest_year=2025, n_periods=32)\n",
    "    if not df_fa.empty:\n",
    "        fa_list.append(df_fa)\n",
    "        valid_tickers.append(t)\n",
    "\n",
    "# --- Gộp DataFrame ---\n",
    "if fa_list:\n",
    "    fa_data = pd.concat(fa_list, ignore_index=True)\n",
    "else:\n",
    "    fa_data = pd.DataFrame()\n",
    "\n",
    "print(f\"Số mã HOSE ban đầu: {len(tickers_hose)}\")\n",
    "print(f\"Số mã có dữ liệu FA: {len(valid_tickers)}\")\n",
    "print(\"FA Data sample:\")\n",
    "print(fa_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831a3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample merged:\n",
      "  ticker  timestamp      open      high       low     close     volume  \\\n",
      "0    AAA 2023-01-03  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA 2023-01-04  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA 2023-01-05  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA 2023-01-06  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA 2023-01-09  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs  ...  DebtToEquityRatio  EBITMargin  \\\n",
      "0  938600.0  504700.0   40579000.0  ...           0.507521   -0.049731   \n",
      "1  462900.0  780600.0  151639000.0  ...           0.507521   -0.049731   \n",
      "2  487200.0  473700.0  343911000.0  ...           0.507521   -0.049731   \n",
      "3  564300.0  828300.0  345999000.0  ...           0.507521   -0.049731   \n",
      "4  414000.0  631800.0  514557000.0  ...           0.507521   -0.049731   \n",
      "\n",
      "        ROA       ROE      ROIC    BasicEPS  PriceToBook  PriceToEarning  \\\n",
      "0  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "1  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "2  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "3  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "4  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "\n",
      "   NetRevenueGrowthYoY  GrossProfitGrowthYoY  \n",
      "0             -0.18869             -0.910016  \n",
      "1             -0.18869             -0.910016  \n",
      "2             -0.18869             -0.910016  \n",
      "3             -0.18869             -0.910016  \n",
      "4             -0.18869             -0.910016  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Số mã merge thành công: 391\n"
     ]
    }
   ],
   "source": [
    "# Block 3 — Chuẩn hoá FA + Merge với giá (HOSE only, dựa theo Block 2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Các chỉ số FA cần lấy ---\n",
    "fa_fields = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "]\n",
    "\n",
    "# --- Hàm nổ ratios ---\n",
    "def explode_ratios(df, fa_fields):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        d = {\n",
    "            \"ticker\": row[\"ticker\"],\n",
    "            \"fa_year\": int(row[\"year\"]),\n",
    "            \"fa_quarter\": int(row[\"quarter\"])\n",
    "        }\n",
    "        ratios = row.get(\"ratios\", {})\n",
    "        if isinstance(ratios, dict):   # ✅ fix chỗ lỗi\n",
    "            for f in fa_fields:\n",
    "                val = None\n",
    "                for section in ratios.values():\n",
    "                    if isinstance(section, dict) and f in section:\n",
    "                        val = section[f]\n",
    "                d[f] = val\n",
    "        else:\n",
    "            # nếu ratios không phải dict thì gán NaN hết\n",
    "            for f in fa_fields:\n",
    "                d[f] = None\n",
    "        records.append(d)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --- Chuẩn hoá FA ---\n",
    "fa_clean = explode_ratios(fa_data, fa_fields)\n",
    "\n",
    "# --- Chuẩn hoá giá ---\n",
    "df_price = df_all[df_all[\"ticker\"].isin(valid_tickers)].copy()\n",
    "df_price[\"timestamp\"] = pd.to_datetime(df_price[\"timestamp\"])\n",
    "df_price = df_price.sort_values([\"ticker\",\"timestamp\"])\n",
    "\n",
    "# tạo key (fa_year, fa_quarter) = quý trước\n",
    "pi = df_price[\"timestamp\"].dt.to_period(\"Q\")\n",
    "prev_pi = pi - 1\n",
    "df_price[\"fa_year\"] = prev_pi.dt.year.astype(int)\n",
    "df_price[\"fa_quarter\"] = prev_pi.dt.quarter.astype(int)\n",
    "\n",
    "# --- Xử lý FA: giữ duy nhất bản cuối cùng mỗi quý\n",
    "fa_clean = (\n",
    "    fa_clean.sort_values([\"ticker\",\"fa_year\",\"fa_quarter\"])\n",
    "            .drop_duplicates(subset=[\"ticker\",\"fa_year\",\"fa_quarter\"], keep=\"last\")\n",
    ")\n",
    "\n",
    "# --- Merge giá + FA ---\n",
    "df_merged = df_price.merge(\n",
    "    fa_clean,\n",
    "    on=[\"ticker\",\"fa_year\",\"fa_quarter\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# FFill theo thời gian trong từng ticker để lấp chỗ trống\n",
    "df_merged = df_merged.sort_values([\"ticker\",\"timestamp\"])\n",
    "df_merged[fa_fields] = df_merged.groupby(\"ticker\")[fa_fields].ffill()\n",
    "\n",
    "print(\"Sample merged:\")\n",
    "print(df_merged.head())\n",
    "print(\"Số mã merge thành công:\", df_merged[\"ticker\"].nunique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73580df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_all\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0de3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with TA:\n",
      "  ticker  timestamp      open      high       low     close     volume  \\\n",
      "0    AAA 2023-01-03  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA 2023-01-04  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA 2023-01-05  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA 2023-01-06  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA 2023-01-09  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs  ...  ema_50  macd  macd_signal  macd_diff  \\\n",
      "0  938600.0  504700.0   40579000.0  ...     NaN   NaN          NaN        NaN   \n",
      "1  462900.0  780600.0  151639000.0  ...     NaN   NaN          NaN        NaN   \n",
      "2  487200.0  473700.0  343911000.0  ...     NaN   NaN          NaN        NaN   \n",
      "3  564300.0  828300.0  345999000.0  ...     NaN   NaN          NaN        NaN   \n",
      "4  414000.0  631800.0  514557000.0  ...     NaN   NaN          NaN        NaN   \n",
      "\n",
      "   rsi  bollinger_hband  bollinger_lband  atr        obv  vwap  \n",
      "0  NaN              NaN              NaN  NaN  1543984.0   NaN  \n",
      "1  NaN              NaN              NaN  NaN   241479.0   NaN  \n",
      "2  NaN              NaN              NaN  NaN  1221952.0   NaN  \n",
      "3  NaN              NaN              NaN  NaN  -209747.0   NaN  \n",
      "4  NaN              NaN              NaN  NaN -1331132.0   NaN  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Shape sau khi thêm TA: (256151, 35)\n"
     ]
    }
   ],
   "source": [
    "# Block 4 — Tính các chỉ số TA (trên df_merged từ Block 3)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Khởi tạo Indicator ---\n",
    "fi = client.FiinIndicator()\n",
    "\n",
    "# --- Hàm tính TA theo từng ticker ---\n",
    "def add_ta_indicators(df):\n",
    "    df = df.sort_values(\"timestamp\").copy()\n",
    "\n",
    "    # EMA\n",
    "    df['ema_5']  = fi.ema(df['close'], window=5)\n",
    "    df['ema_20'] = fi.ema(df['close'], window=20)\n",
    "    df['ema_50'] = fi.ema(df['close'], window=50)\n",
    "\n",
    "    # MACD\n",
    "    df['macd']        = fi.macd(df['close'], window_fast=12, window_slow=26)\n",
    "    df['macd_signal'] = fi.macd_signal(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "    df['macd_diff']   = fi.macd_diff(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "\n",
    "    # RSI\n",
    "    df['rsi'] = fi.rsi(df['close'], window=14)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df['bollinger_hband'] = fi.bollinger_hband(df['close'], window=20, window_dev=2)\n",
    "    df['bollinger_lband'] = fi.bollinger_lband(df['close'], window=20, window_dev=2)\n",
    "\n",
    "    # ATR\n",
    "    df['atr'] = fi.atr(df['high'], df['low'], df['close'], window=14)\n",
    "\n",
    "    # OBV\n",
    "    df['obv'] = fi.obv(df['close'], df['volume'])\n",
    "\n",
    "    # VWAP\n",
    "    df['vwap'] = fi.vwap(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Áp dụng cho toàn bộ df_merged ---\n",
    "df_with_ta = df_merged.groupby(\"ticker\", group_keys=False).apply(add_ta_indicators)\n",
    "\n",
    "print(\"Sample with TA:\")\n",
    "print(df_with_ta.head())\n",
    "print(\"Shape sau khi thêm TA:\", df_with_ta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26204a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_merged\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features:\n",
      "  ticker  timestamp  DebtToEquityRatio  EBITMargin       ROA       ROE  \\\n",
      "0    AAA 2023-06-14           0.559115    0.978956  0.368731  0.415728   \n",
      "1    AAA 2023-06-15           0.559115    0.978956  0.368731  0.415728   \n",
      "2    AAA 2023-06-16           0.559115    0.978956  0.368731  0.415728   \n",
      "3    AAA 2023-06-19           0.559115    0.978956  0.368731  0.415728   \n",
      "4    AAA 2023-06-20           0.559115    0.978956  0.368731  0.415728   \n",
      "\n",
      "     ROIC  BasicEPS  PriceToBook  PriceToEarning  ...  ema_50_z    macd_z  \\\n",
      "0  0.7533  0.158046      0.37764        0.537353  ...  1.799046 -0.008320   \n",
      "1  0.7533  0.158046      0.37764        0.537353  ...  1.761009 -0.314808   \n",
      "2  0.7533  0.158046      0.37764        0.537353  ...  1.701399 -0.891565   \n",
      "3  0.7533  0.158046      0.37764        0.537353  ...  1.651219 -1.278916   \n",
      "4  0.7533  0.158046      0.37764        0.537353  ...  1.609327 -1.509982   \n",
      "\n",
      "   macd_signal_z  macd_diff_z     rsi_z  bollinger_hband_z  bollinger_lband_z  \\\n",
      "0       0.591579    -1.164222 -1.530688           1.363061           1.839777   \n",
      "1       0.399630    -1.467105 -1.541599           1.317089           1.758209   \n",
      "2       0.112479    -2.139400 -2.893380           1.289737           1.634182   \n",
      "3      -0.209795    -2.311526 -2.429426           1.248184           1.569059   \n",
      "4      -0.526222    -2.190019 -2.020537           1.199384           1.537532   \n",
      "\n",
      "      atr_z     obv_z    vwap_z  \n",
      "0  0.589827  1.405288  1.603355  \n",
      "1  0.534358  1.364100  1.554836  \n",
      "2  0.710743  0.982966  1.479150  \n",
      "3  0.593083  1.154836  1.414003  \n",
      "4  0.484151  1.326566  1.346550  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Shape sau khi scaling & dropna: (181828, 24)\n"
     ]
    }
   ],
   "source": [
    "# Block 5 — Feature engineering & scaling\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Danh sách cột FA & TA ---\n",
    "fa_features = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "]\n",
    "\n",
    "ta_features = [\n",
    "    \"ema_5\",\"ema_20\",\"ema_50\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"rsi\",\"bollinger_hband\",\"bollinger_lband\",\"atr\",\"obv\",\"vwap\"\n",
    "]\n",
    "\n",
    "# --- Chuẩn hoá FA: cross-section min-max scaling theo ngày ---\n",
    "def scale_fa_minmax(df):\n",
    "    df_scaled = df.copy()\n",
    "    for f in fa_features:\n",
    "        vals = df[f].astype(float)\n",
    "        vmin, vmax = vals.min(), vals.max()\n",
    "        if np.isfinite(vmin) and np.isfinite(vmax) and vmax > vmin:\n",
    "            df_scaled[f] = (vals - vmin) / (vmax - vmin)\n",
    "        else:\n",
    "            df_scaled[f] = np.nan\n",
    "    return df_scaled\n",
    "\n",
    "df_scaled_fa = df_with_ta.groupby(\"timestamp\", group_keys=False).apply(scale_fa_minmax)\n",
    "\n",
    "# --- Chuẩn hoá TA: rolling z-score theo từng ticker ---\n",
    "def zscore_rolling(series, window=60):\n",
    "    return (series - series.rolling(window).mean()) / series.rolling(window).std()\n",
    "\n",
    "df_scaled = df_scaled_fa.groupby(\"ticker\", group_keys=False).apply(\n",
    "    lambda g: g.assign(**{f\"{col}_z\": zscore_rolling(g[col], 60) for col in ta_features})\n",
    ")\n",
    "\n",
    "# --- Drop các cột gốc TA, giữ bản z-score ---\n",
    "keep_cols = [\"ticker\",\"timestamp\"] + fa_features + [f\"{col}_z\" for col in ta_features]\n",
    "df_features = df_scaled[keep_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(df_features.head())\n",
    "print(\"Shape sau khi scaling & dropna:\", df_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee3d19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_with_ta, df_scaled, df_scaled_fa\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sample:\n",
      "  ticker  timestamp  cluster     tsne_x     tsne_y    month\n",
      "0    AAA 2023-06-14       -1  34.417667 -21.641058  2023-06\n",
      "1    AAA 2023-06-15       -1  34.361568 -21.848602  2023-06\n",
      "2    AAA 2023-06-16       -1  35.918636 -42.906532  2023-06\n",
      "3    AAA 2023-06-19       -1  35.490566 -42.649422  2023-06\n",
      "4    AAA 2023-06-20       -1  34.553066 -42.459969  2023-06\n",
      "Số cụm mỗi tháng:\n",
      "month\n",
      "2023-06     16\n",
      "2023-07     67\n",
      "2023-08     78\n",
      "2023-09     40\n",
      "2023-10     83\n",
      "2023-11     55\n",
      "2023-12     83\n",
      "2024-01     90\n",
      "2024-02     31\n",
      "2024-03     64\n",
      "2024-04     43\n",
      "2024-05     66\n",
      "2024-06     94\n",
      "2024-07     87\n",
      "2024-08     65\n",
      "2024-09     86\n",
      "2024-10    104\n",
      "2024-11     80\n",
      "2024-12     74\n",
      "2025-01     49\n",
      "2025-02     68\n",
      "2025-03     93\n",
      "2025-04     52\n",
      "2025-05     79\n",
      "2025-06     95\n",
      "2025-07    110\n",
      "2025-08     74\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Block 6 — Dimensionality reduction & Clustering (t-SNE + DBSCAN)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# --- Chọn các cột features để clustering ---\n",
    "feature_cols = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "] + [c for c in df_features.columns if c.endswith(\"_z\")]\n",
    "\n",
    "# --- Thêm cột tháng để snapshot ---\n",
    "df_features[\"month\"] = df_features[\"timestamp\"].dt.to_period(\"M\")\n",
    "\n",
    "cluster_results = []\n",
    "\n",
    "for (month, g) in df_features.groupby(\"month\"):\n",
    "    if len(g) < 10:   # quá ít cổ phiếu thì bỏ\n",
    "        continue\n",
    "\n",
    "    X = g[feature_cols].values\n",
    "\n",
    "    # --- t-SNE giảm chiều còn 2D ---\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"random\", random_state=42)\n",
    "    X_emb = tsne.fit_transform(X)\n",
    "\n",
    "    # --- DBSCAN clustering ---\n",
    "    db = DBSCAN(eps=0.5, min_samples=5).fit(X_emb)\n",
    "    labels = db.labels_\n",
    "\n",
    "    temp = g[[\"ticker\",\"timestamp\"]].copy()\n",
    "    temp[\"cluster\"] = labels\n",
    "    temp[\"tsne_x\"] = X_emb[:,0]\n",
    "    temp[\"tsne_y\"] = X_emb[:,1]\n",
    "    temp[\"month\"]  = str(month)\n",
    "\n",
    "    cluster_results.append(temp)\n",
    "\n",
    "df_clusters = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(\"Cluster sample:\")\n",
    "print(df_clusters.head())\n",
    "print(\"Số cụm mỗi tháng:\")\n",
    "print(df_clusters.groupby(\"month\")[\"cluster\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a62783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: tensor (490, 64, 21, 22), mask (490, 64, 21, 22) saved.\n",
      "Cluster 1: tensor (490, 64, 15, 22), mask (490, 64, 15, 22) saved.\n",
      "Cluster 2: tensor (490, 64, 15, 22), mask (490, 64, 15, 22) saved.\n",
      "Cluster 3: tensor (490, 64, 17, 22), mask (490, 64, 17, 22) saved.\n",
      "Cluster 4: tensor (490, 64, 19, 22), mask (490, 64, 19, 22) saved.\n",
      "Cluster 5: tensor (490, 64, 21, 22), mask (490, 64, 21, 22) saved.\n",
      "Cluster 6: tensor (490, 64, 23, 22), mask (490, 64, 23, 22) saved.\n",
      "Cluster 7: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 8: tensor (490, 64, 21, 22), mask (490, 64, 21, 22) saved.\n",
      "Cluster 9: tensor (490, 64, 22, 22), mask (490, 64, 22, 22) saved.\n",
      "Cluster 10: tensor (490, 64, 23, 22), mask (490, 64, 23, 22) saved.\n",
      "Cluster 11: tensor (490, 64, 19, 22), mask (490, 64, 19, 22) saved.\n",
      "Cluster 12: tensor (490, 64, 28, 22), mask (490, 64, 28, 22) saved.\n",
      "Cluster 13: tensor (490, 64, 29, 22), mask (490, 64, 29, 22) saved.\n",
      "Cluster 14: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 15: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 16: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 17: tensor (490, 64, 26, 22), mask (490, 64, 26, 22) saved.\n",
      "Cluster 18: tensor (490, 64, 23, 22), mask (490, 64, 23, 22) saved.\n",
      "Cluster 19: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 20: tensor (490, 64, 27, 22), mask (490, 64, 27, 22) saved.\n",
      "Cluster 21: tensor (490, 64, 22, 22), mask (490, 64, 22, 22) saved.\n",
      "Cluster 22: tensor (490, 64, 26, 22), mask (490, 64, 26, 22) saved.\n",
      "Cluster 23: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 24: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 25: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 26: tensor (490, 64, 29, 22), mask (490, 64, 29, 22) saved.\n",
      "Cluster 27: tensor (490, 64, 28, 22), mask (490, 64, 28, 22) saved.\n",
      "Cluster 28: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 29: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 30: tensor (490, 64, 21, 22), mask (490, 64, 21, 22) saved.\n",
      "Cluster 31: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 32: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 33: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 34: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 35: tensor (490, 64, 26, 22), mask (490, 64, 26, 22) saved.\n",
      "Cluster 36: tensor (490, 64, 26, 22), mask (490, 64, 26, 22) saved.\n",
      "Cluster 37: tensor (490, 64, 29, 22), mask (490, 64, 29, 22) saved.\n",
      "Cluster 38: tensor (490, 64, 23, 22), mask (490, 64, 23, 22) saved.\n",
      "Cluster 39: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 40: tensor (490, 64, 23, 22), mask (490, 64, 23, 22) saved.\n",
      "Cluster 41: tensor (490, 64, 22, 22), mask (490, 64, 22, 22) saved.\n",
      "Cluster 42: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 43: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 44: tensor (490, 64, 27, 22), mask (490, 64, 27, 22) saved.\n",
      "Cluster 45: tensor (490, 64, 25, 22), mask (490, 64, 25, 22) saved.\n",
      "Cluster 46: tensor (490, 64, 23, 22), mask (490, 64, 23, 22) saved.\n",
      "Cluster 47: tensor (490, 64, 22, 22), mask (490, 64, 22, 22) saved.\n",
      "Cluster 48: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 49: tensor (490, 64, 20, 22), mask (490, 64, 20, 22) saved.\n",
      "Cluster 50: tensor (490, 64, 22, 22), mask (490, 64, 22, 22) saved.\n",
      "Cluster 51: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 52: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 53: tensor (490, 64, 24, 22), mask (490, 64, 24, 22) saved.\n",
      "Cluster 54: tensor (490, 64, 20, 22), mask (490, 64, 20, 22) saved.\n",
      "Cluster 55: tensor (490, 64, 20, 22), mask (490, 64, 20, 22) saved.\n",
      "Cluster 56: tensor (490, 64, 18, 22), mask (490, 64, 18, 22) saved.\n",
      "Cluster 57: tensor (490, 64, 18, 22), mask (490, 64, 18, 22) saved.\n",
      "Cluster 58: tensor (490, 64, 19, 22), mask (490, 64, 19, 22) saved.\n",
      "Cluster 59: tensor (490, 64, 19, 22), mask (490, 64, 19, 22) saved.\n",
      "Cluster 60: tensor (490, 64, 22, 22), mask (490, 64, 22, 22) saved.\n",
      "Cluster 61: tensor (490, 64, 20, 22), mask (490, 64, 20, 22) saved.\n",
      "Cluster 62: tensor (490, 64, 21, 22), mask (490, 64, 21, 22) saved.\n",
      "Cluster 63: tensor (490, 64, 19, 22), mask (490, 64, 19, 22) saved.\n",
      "Cluster 64: tensor (490, 64, 18, 22), mask (490, 64, 18, 22) saved.\n",
      "Cluster 65: tensor (490, 64, 17, 22), mask (490, 64, 17, 22) saved.\n",
      "Cluster 66: tensor (490, 64, 15, 22), mask (490, 64, 15, 22) saved.\n",
      "Cluster 67: tensor (490, 64, 16, 22), mask (490, 64, 16, 22) saved.\n",
      "Cluster 68: tensor (490, 64, 16, 22), mask (490, 64, 16, 22) saved.\n",
      "Cluster 69: tensor (490, 64, 14, 22), mask (490, 64, 14, 22) saved.\n",
      "Cluster 70: tensor (490, 64, 14, 22), mask (490, 64, 14, 22) saved.\n",
      "Cluster 71: tensor (490, 64, 16, 22), mask (490, 64, 16, 22) saved.\n",
      "Cluster 72: tensor (490, 64, 13, 22), mask (490, 64, 13, 22) saved.\n",
      "Cluster 73: tensor (490, 64, 13, 22), mask (490, 64, 13, 22) saved.\n",
      "Cluster 74: tensor (490, 64, 15, 22), mask (490, 64, 15, 22) saved.\n",
      "Cluster 75: tensor (490, 64, 14, 22), mask (490, 64, 14, 22) saved.\n",
      "Cluster 76: tensor (490, 64, 13, 22), mask (490, 64, 13, 22) saved.\n",
      "Cluster 77: tensor (490, 64, 12, 22), mask (490, 64, 12, 22) saved.\n",
      "Cluster 78: tensor (490, 64, 11, 22), mask (490, 64, 11, 22) saved.\n",
      "Cluster 79: tensor (490, 64, 9, 22), mask (490, 64, 9, 22) saved.\n",
      "Cluster 80: tensor (490, 64, 12, 22), mask (490, 64, 12, 22) saved.\n",
      "Cluster 81: tensor (490, 64, 12, 22), mask (490, 64, 12, 22) saved.\n",
      "Cluster 82: tensor (490, 64, 8, 22), mask (490, 64, 8, 22) saved.\n",
      "Cluster 83: tensor (490, 64, 9, 22), mask (490, 64, 9, 22) saved.\n",
      "Cluster 84: tensor (490, 64, 9, 22), mask (490, 64, 9, 22) saved.\n",
      "Cluster 85: tensor (490, 64, 8, 22), mask (490, 64, 8, 22) saved.\n",
      "Cluster 86: tensor (490, 64, 6, 22), mask (490, 64, 6, 22) saved.\n",
      "Cluster 87: tensor (490, 64, 6, 22), mask (490, 64, 6, 22) saved.\n",
      "Cluster 88: tensor (490, 64, 8, 22), mask (490, 64, 8, 22) saved.\n",
      "Cluster 89: tensor (490, 64, 5, 22), mask (490, 64, 5, 22) saved.\n",
      "Cluster 90: tensor (490, 64, 5, 22), mask (490, 64, 5, 22) saved.\n",
      "Cluster 91: tensor (490, 64, 6, 22), mask (490, 64, 6, 22) saved.\n",
      "Cluster 92: tensor (490, 64, 4, 22), mask (490, 64, 4, 22) saved.\n",
      "Cluster 93: tensor (490, 64, 3, 22), mask (490, 64, 3, 22) saved.\n",
      "Cluster 94: tensor (490, 64, 3, 22), mask (490, 64, 3, 22) saved.\n",
      "Cluster 95: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 96: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 97: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 98: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 99: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 100: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 101: tensor (490, 64, 3, 22), mask (490, 64, 3, 22) saved.\n",
      "Cluster 102: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 103: tensor (490, 64, 1, 22), mask (490, 64, 1, 22) saved.\n",
      "Cluster 104: tensor (490, 64, 1, 22), mask (490, 64, 1, 22) saved.\n",
      "Cluster 105: tensor (490, 64, 1, 22), mask (490, 64, 1, 22) saved.\n",
      "Cluster 106: tensor (490, 64, 1, 22), mask (490, 64, 1, 22) saved.\n",
      "Cluster 107: tensor (490, 64, 2, 22), mask (490, 64, 2, 22) saved.\n",
      "Cluster 108: tensor (490, 64, 1, 22), mask (490, 64, 1, 22) saved.\n",
      "✅ Done Block 7: tensors + masks saved for all clusters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#block 7\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, json\n",
    "\n",
    "LOOKBACK = 64   # window size\n",
    "DATA_DIR = \"./tensors/\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- chọn feature columns (bỏ các cột không phải feature) ---\n",
    "feature_cols = [c for c in df_features.columns if c not in [\"ticker\",\"timestamp\",\"cluster\",\"month\"]]\n",
    "\n",
    "tensor_index = []\n",
    "\n",
    "# --- Lặp qua từng cluster ---\n",
    "for c_id, g in df_clusters.groupby(\"cluster\"):\n",
    "    if c_id == -1:   # DBSCAN noise bỏ qua\n",
    "        continue\n",
    "\n",
    "    tickers = sorted(g[\"ticker\"].unique())\n",
    "    g_feat = df_features[df_features[\"ticker\"].isin(tickers)].copy()\n",
    "\n",
    "    # Pivot: index = timestamp, columns = MultiIndex (ticker, feature)\n",
    "    pivoted = g_feat.pivot(index=\"timestamp\", columns=\"ticker\", values=feature_cols)\n",
    "    pivoted.columns = pd.MultiIndex.from_product([tickers, feature_cols])\n",
    "\n",
    "    # Mask: 1 = có dữ liệu, 0 = NaN\n",
    "    mask_df = ~pivoted.isna()\n",
    "\n",
    "    # Fill NaN để reshape được (mask vẫn giữ thông tin missing)\n",
    "    pivoted_filled = pivoted.ffill().bfill()\n",
    "\n",
    "    T, N, F = len(pivoted_filled.index), len(tickers), len(feature_cols)\n",
    "\n",
    "    X = pivoted_filled.values.reshape(T, N, F)\n",
    "    M = mask_df.values.reshape(T, N, F).astype(int)\n",
    "\n",
    "    cluster_tensors, cluster_masks = [], []\n",
    "    for i in range(LOOKBACK, T):\n",
    "        cluster_tensors.append(X[i-LOOKBACK:i])\n",
    "        cluster_masks.append(M[i-LOOKBACK:i])\n",
    "\n",
    "    if cluster_tensors:\n",
    "        X_arr, M_arr = np.array(cluster_tensors), np.array(cluster_masks)\n",
    "\n",
    "        # Save file\n",
    "        tensor_file = f\"cluster_{c_id}_tensor.npy\"\n",
    "        mask_file   = f\"cluster_{c_id}_mask.npy\"\n",
    "        np.save(os.path.join(DATA_DIR, tensor_file), X_arr)\n",
    "        np.save(os.path.join(DATA_DIR, mask_file), M_arr)\n",
    "\n",
    "        tensor_index.append({\n",
    "            \"cluster\": int(c_id),\n",
    "            \"tickers\": tickers,\n",
    "            \"dates\": [str(d) for d in pivoted_filled.index[LOOKBACK:]],\n",
    "            \"tensor_file\": tensor_file,\n",
    "            \"mask_file\": mask_file\n",
    "        })\n",
    "\n",
    "        print(f\"Cluster {c_id}: tensor {X_arr.shape}, mask {M_arr.shape} saved.\")\n",
    "\n",
    "    # Giải phóng RAM\n",
    "    del g_feat, pivoted, pivoted_filled, mask_df, X, M, cluster_tensors, cluster_masks\n",
    "    gc.collect()\n",
    "\n",
    "# --- Lưu metadata ---\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"w\") as f:\n",
    "    json.dump(tensor_index, f, indent=2)\n",
    "\n",
    "print(\"✅ Done Block 7: tensors + masks saved for all clusters.\")\n",
    "\n",
    "# Sau Block 7 có thể xoá df_features cho nhẹ RAM\n",
    "del df_features\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf1ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done Block 7.5: df_backtest sẵn sàng cho reward.\n",
      "Kích thước df_backtest: (256151, 3)\n",
      "Tickers unique: 391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 7.5 — Chuẩn bị dữ liệu backtest cho reward thật\n",
    "import gc\n",
    "\n",
    "# Chỉ giữ dữ liệu cần thiết để tính reward (close price)\n",
    "# df_price có từ Block 1 (OHLCV đầy đủ)\n",
    "df_backtest = df_price[[\"ticker\", \"timestamp\", \"close\"]].copy()\n",
    "\n",
    "# Ép timestamp về dạng datetime để đồng bộ\n",
    "df_backtest[\"timestamp\"] = pd.to_datetime(df_backtest[\"timestamp\"])\n",
    "\n",
    "print(\"✅ Done Block 7.5: df_backtest sẵn sàng cho reward.\")\n",
    "print(\"Kích thước df_backtest:\", df_backtest.shape)\n",
    "print(\"Tickers unique:\", df_backtest[\"ticker\"].nunique())\n",
    "\n",
    "# Xóa những biến không còn cần để tiết kiệm RAM\n",
    "del df_price\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1c49599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 | X=(490, 64, 21, 22)\n",
      "  Epoch 1/3, Loss=-0.8344\n",
      "  Epoch 2/3, Loss=-0.4268\n",
      "  Epoch 3/3, Loss=-0.4869\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_0.pt\n",
      "Cluster 1 | X=(490, 64, 15, 22)\n",
      "  Epoch 1/3, Loss=-0.6337\n",
      "  Epoch 2/3, Loss=-0.3487\n",
      "  Epoch 3/3, Loss=-0.3366\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_1.pt\n",
      "Cluster 2 | X=(490, 64, 15, 22)\n",
      "  Epoch 1/3, Loss=-0.0938\n",
      "  Epoch 2/3, Loss=-0.2452\n",
      "  Epoch 3/3, Loss=-0.2638\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_2.pt\n",
      "Cluster 3 | X=(490, 64, 17, 22)\n",
      "  Epoch 1/3, Loss=0.3028\n",
      "  Epoch 2/3, Loss=-0.2032\n",
      "  Epoch 3/3, Loss=-0.3266\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_3.pt\n",
      "Cluster 4 | X=(490, 64, 19, 22)\n",
      "  Epoch 1/3, Loss=-0.2497\n",
      "  Epoch 2/3, Loss=-0.3891\n",
      "  Epoch 3/3, Loss=-0.3604\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_4.pt\n",
      "Cluster 5 | X=(490, 64, 21, 22)\n",
      "  Epoch 1/3, Loss=0.0066\n",
      "  Epoch 2/3, Loss=-0.4097\n",
      "  Epoch 3/3, Loss=-0.4271\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_5.pt\n",
      "Cluster 6 | X=(490, 64, 23, 22)\n",
      "  Epoch 1/3, Loss=-0.6678\n",
      "  Epoch 2/3, Loss=-0.4547\n",
      "  Epoch 3/3, Loss=-0.4917\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_6.pt\n",
      "Cluster 7 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.6243\n",
      "  Epoch 2/3, Loss=-0.4318\n",
      "  Epoch 3/3, Loss=-0.5795\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_7.pt\n",
      "Cluster 8 | X=(490, 64, 21, 22)\n",
      "  Epoch 1/3, Loss=-0.3756\n",
      "  Epoch 2/3, Loss=-0.4501\n",
      "  Epoch 3/3, Loss=-0.4307\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_8.pt\n",
      "Cluster 9 | X=(490, 64, 22, 22)\n",
      "  Epoch 1/3, Loss=0.1832\n",
      "  Epoch 2/3, Loss=-0.6139\n",
      "  Epoch 3/3, Loss=-0.3491\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_9.pt\n",
      "Cluster 10 | X=(490, 64, 23, 22)\n",
      "  Epoch 1/3, Loss=-0.1915\n",
      "  Epoch 2/3, Loss=-0.4615\n",
      "  Epoch 3/3, Loss=-0.4841\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_10.pt\n",
      "Cluster 11 | X=(490, 64, 19, 22)\n",
      "  Epoch 1/3, Loss=0.2744\n",
      "  Epoch 2/3, Loss=-0.2748\n",
      "  Epoch 3/3, Loss=-0.3922\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_11.pt\n",
      "Cluster 12 | X=(490, 64, 28, 22)\n",
      "  Epoch 1/3, Loss=-0.5255\n",
      "  Epoch 2/3, Loss=-0.6282\n",
      "  Epoch 3/3, Loss=-0.5553\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_12.pt\n",
      "Cluster 13 | X=(490, 64, 29, 22)\n",
      "  Epoch 1/3, Loss=-0.4793\n",
      "  Epoch 2/3, Loss=-0.6355\n",
      "  Epoch 3/3, Loss=-0.5627\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_13.pt\n",
      "Cluster 14 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=0.1318\n",
      "  Epoch 2/3, Loss=-0.5682\n",
      "  Epoch 3/3, Loss=-0.4886\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_14.pt\n",
      "Cluster 15 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.4550\n",
      "  Epoch 2/3, Loss=-0.5104\n",
      "  Epoch 3/3, Loss=-0.4539\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_15.pt\n",
      "Cluster 16 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.8171\n",
      "  Epoch 2/3, Loss=-0.4736\n",
      "  Epoch 3/3, Loss=-0.5264\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_16.pt\n",
      "Cluster 17 | X=(490, 64, 26, 22)\n",
      "  Epoch 1/3, Loss=-0.2933\n",
      "  Epoch 2/3, Loss=-0.6722\n",
      "  Epoch 3/3, Loss=-0.4962\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_17.pt\n",
      "Cluster 18 | X=(490, 64, 23, 22)\n",
      "  Epoch 1/3, Loss=-0.7062\n",
      "  Epoch 2/3, Loss=-0.4546\n",
      "  Epoch 3/3, Loss=-0.4762\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_18.pt\n",
      "Cluster 19 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.8139\n",
      "  Epoch 2/3, Loss=-0.5013\n",
      "  Epoch 3/3, Loss=-0.5228\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_19.pt\n",
      "Cluster 20 | X=(490, 64, 27, 22)\n",
      "  Epoch 1/3, Loss=-0.7552\n",
      "  Epoch 2/3, Loss=-0.5395\n",
      "  Epoch 3/3, Loss=-0.5815\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_20.pt\n",
      "Cluster 21 | X=(490, 64, 22, 22)\n",
      "  Epoch 1/3, Loss=-0.5011\n",
      "  Epoch 2/3, Loss=-0.2487\n",
      "  Epoch 3/3, Loss=-0.4846\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_21.pt\n",
      "Cluster 22 | X=(490, 64, 26, 22)\n",
      "  Epoch 1/3, Loss=-0.2476\n",
      "  Epoch 2/3, Loss=-0.4994\n",
      "  Epoch 3/3, Loss=-0.5209\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_22.pt\n",
      "Cluster 23 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.5838\n",
      "  Epoch 2/3, Loss=-0.4732\n",
      "  Epoch 3/3, Loss=-0.4769\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_23.pt\n",
      "Cluster 24 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=0.1655\n",
      "  Epoch 2/3, Loss=-0.4241\n",
      "  Epoch 3/3, Loss=-0.4907\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_24.pt\n",
      "Cluster 25 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=0.0258\n",
      "  Epoch 2/3, Loss=-0.4871\n",
      "  Epoch 3/3, Loss=-0.4709\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_25.pt\n",
      "Cluster 26 | X=(490, 64, 29, 22)\n",
      "  Epoch 1/3, Loss=-0.8988\n",
      "  Epoch 2/3, Loss=-0.4883\n",
      "  Epoch 3/3, Loss=-0.6537\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_26.pt\n",
      "Cluster 27 | X=(490, 64, 28, 22)\n",
      "  Epoch 1/3, Loss=-0.7471\n",
      "  Epoch 2/3, Loss=-0.5080\n",
      "  Epoch 3/3, Loss=-0.6421\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_27.pt\n",
      "Cluster 28 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.2993\n",
      "  Epoch 2/3, Loss=-0.5350\n",
      "  Epoch 3/3, Loss=-0.4884\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_28.pt\n",
      "Cluster 29 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-1.1548\n",
      "  Epoch 2/3, Loss=-0.5246\n",
      "  Epoch 3/3, Loss=-0.5451\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_29.pt\n",
      "Cluster 30 | X=(490, 64, 21, 22)\n",
      "  Epoch 1/3, Loss=-0.6561\n",
      "  Epoch 2/3, Loss=-0.4517\n",
      "  Epoch 3/3, Loss=-0.4479\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_30.pt\n",
      "Cluster 31 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.4690\n",
      "  Epoch 2/3, Loss=-0.4983\n",
      "  Epoch 3/3, Loss=-0.5414\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_31.pt\n",
      "Cluster 32 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.0777\n",
      "  Epoch 2/3, Loss=-0.4391\n",
      "  Epoch 3/3, Loss=-0.4767\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_32.pt\n",
      "Cluster 33 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.6162\n",
      "  Epoch 2/3, Loss=-0.5738\n",
      "  Epoch 3/3, Loss=-0.4967\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_33.pt\n",
      "Cluster 34 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.8356\n",
      "  Epoch 2/3, Loss=-0.4581\n",
      "  Epoch 3/3, Loss=-0.5429\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_34.pt\n",
      "Cluster 35 | X=(490, 64, 26, 22)\n",
      "  Epoch 1/3, Loss=-0.8503\n",
      "  Epoch 2/3, Loss=-0.5625\n",
      "  Epoch 3/3, Loss=-0.5204\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_35.pt\n",
      "Cluster 36 | X=(490, 64, 26, 22)\n",
      "  Epoch 1/3, Loss=-0.1722\n",
      "  Epoch 2/3, Loss=-0.4502\n",
      "  Epoch 3/3, Loss=-0.5023\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_36.pt\n",
      "Cluster 37 | X=(490, 64, 29, 22)\n",
      "  Epoch 1/3, Loss=-0.1895\n",
      "  Epoch 2/3, Loss=-0.4739\n",
      "  Epoch 3/3, Loss=-0.5924\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_37.pt\n",
      "Cluster 38 | X=(490, 64, 23, 22)\n",
      "  Epoch 1/3, Loss=-0.5039\n",
      "  Epoch 2/3, Loss=-0.4496\n",
      "  Epoch 3/3, Loss=-0.4695\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_38.pt\n",
      "Cluster 39 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.0017\n",
      "  Epoch 2/3, Loss=-0.5029\n",
      "  Epoch 3/3, Loss=-0.4667\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_39.pt\n",
      "Cluster 40 | X=(490, 64, 23, 22)\n",
      "  Epoch 1/3, Loss=0.3680\n",
      "  Epoch 2/3, Loss=-0.3906\n",
      "  Epoch 3/3, Loss=-0.4532\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_40.pt\n",
      "Cluster 41 | X=(490, 64, 22, 22)\n",
      "  Epoch 1/3, Loss=-0.2143\n",
      "  Epoch 2/3, Loss=-0.5266\n",
      "  Epoch 3/3, Loss=0.1158\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_41.pt\n",
      "Cluster 42 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.3142\n",
      "  Epoch 2/3, Loss=-0.4960\n",
      "  Epoch 3/3, Loss=-0.5000\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_42.pt\n",
      "Cluster 43 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.3053\n",
      "  Epoch 2/3, Loss=-0.4396\n",
      "  Epoch 3/3, Loss=-0.4716\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_43.pt\n",
      "Cluster 44 | X=(490, 64, 27, 22)\n",
      "  Epoch 1/3, Loss=-0.6724\n",
      "  Epoch 2/3, Loss=-0.5299\n",
      "  Epoch 3/3, Loss=-0.5182\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_44.pt\n",
      "Cluster 45 | X=(490, 64, 25, 22)\n",
      "  Epoch 1/3, Loss=-0.5752\n",
      "  Epoch 2/3, Loss=-0.5803\n",
      "  Epoch 3/3, Loss=-0.4897\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_45.pt\n",
      "Cluster 46 | X=(490, 64, 23, 22)\n",
      "  Epoch 1/3, Loss=-0.4501\n",
      "  Epoch 2/3, Loss=-0.4232\n",
      "  Epoch 3/3, Loss=-0.4897\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_46.pt\n",
      "Cluster 47 | X=(490, 64, 22, 22)\n",
      "  Epoch 1/3, Loss=-0.6453\n",
      "  Epoch 2/3, Loss=-0.0242\n",
      "  Epoch 3/3, Loss=-0.7751\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_47.pt\n",
      "Cluster 48 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.2424\n",
      "  Epoch 2/3, Loss=-0.4168\n",
      "  Epoch 3/3, Loss=-0.4679\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_48.pt\n",
      "Cluster 49 | X=(490, 64, 20, 22)\n",
      "  Epoch 1/3, Loss=-0.3244\n",
      "  Epoch 2/3, Loss=-0.3720\n",
      "  Epoch 3/3, Loss=-0.4209\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_49.pt\n",
      "Cluster 50 | X=(490, 64, 22, 22)\n",
      "  Epoch 1/3, Loss=-0.0974\n",
      "  Epoch 2/3, Loss=-0.7443\n",
      "  Epoch 3/3, Loss=-0.2751\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_50.pt\n",
      "Cluster 51 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.6685\n",
      "  Epoch 2/3, Loss=-0.3308\n",
      "  Epoch 3/3, Loss=-0.5381\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_51.pt\n",
      "Cluster 52 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.6188\n",
      "  Epoch 2/3, Loss=-0.4754\n",
      "  Epoch 3/3, Loss=-0.4964\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_52.pt\n",
      "Cluster 53 | X=(490, 64, 24, 22)\n",
      "  Epoch 1/3, Loss=-0.6357\n",
      "  Epoch 2/3, Loss=-0.4831\n",
      "  Epoch 3/3, Loss=-0.4922\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_53.pt\n",
      "Cluster 54 | X=(490, 64, 20, 22)\n",
      "  Epoch 1/3, Loss=-0.1150\n",
      "  Epoch 2/3, Loss=-0.2978\n",
      "  Epoch 3/3, Loss=-0.3693\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_54.pt\n",
      "Cluster 55 | X=(490, 64, 20, 22)\n",
      "  Epoch 1/3, Loss=-0.7924\n",
      "  Epoch 2/3, Loss=-0.4667\n",
      "  Epoch 3/3, Loss=-0.4123\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_55.pt\n",
      "Cluster 56 | X=(490, 64, 18, 22)\n",
      "  Epoch 1/3, Loss=-1.4375\n",
      "  Epoch 2/3, Loss=-0.3327\n",
      "  Epoch 3/3, Loss=-0.3770\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_56.pt\n",
      "Cluster 57 | X=(490, 64, 18, 22)\n",
      "  Epoch 1/3, Loss=-0.4671\n",
      "  Epoch 2/3, Loss=-0.3926\n",
      "  Epoch 3/3, Loss=-0.3735\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_57.pt\n",
      "Cluster 58 | X=(490, 64, 19, 22)\n",
      "  Epoch 1/3, Loss=0.0065\n",
      "  Epoch 2/3, Loss=-0.3555\n",
      "  Epoch 3/3, Loss=-0.3453\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_58.pt\n",
      "Cluster 59 | X=(490, 64, 19, 22)\n",
      "  Epoch 1/3, Loss=-0.4024\n",
      "  Epoch 2/3, Loss=-0.3709\n",
      "  Epoch 3/3, Loss=-0.4065\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_59.pt\n",
      "Cluster 60 | X=(490, 64, 22, 22)\n",
      "  Epoch 1/3, Loss=-0.3364\n",
      "  Epoch 2/3, Loss=-0.2995\n",
      "  Epoch 3/3, Loss=-0.3690\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_60.pt\n",
      "Cluster 61 | X=(490, 64, 20, 22)\n",
      "  Epoch 1/3, Loss=-0.3417\n",
      "  Epoch 2/3, Loss=-0.4084\n",
      "  Epoch 3/3, Loss=-0.4509\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_61.pt\n",
      "Cluster 62 | X=(490, 64, 21, 22)\n",
      "  Epoch 1/3, Loss=-0.2816\n",
      "  Epoch 2/3, Loss=-0.4769\n",
      "  Epoch 3/3, Loss=-0.4135\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_62.pt\n",
      "Cluster 63 | X=(490, 64, 19, 22)\n",
      "  Epoch 1/3, Loss=-0.5037\n",
      "  Epoch 2/3, Loss=-0.4200\n",
      "  Epoch 3/3, Loss=-0.4116\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_63.pt\n",
      "Cluster 64 | X=(490, 64, 18, 22)\n",
      "  Epoch 1/3, Loss=0.0012\n",
      "  Epoch 2/3, Loss=-0.3004\n",
      "  Epoch 3/3, Loss=-0.3914\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_64.pt\n",
      "Cluster 65 | X=(490, 64, 17, 22)\n",
      "  Epoch 1/3, Loss=-0.1341\n",
      "  Epoch 2/3, Loss=-0.3143\n",
      "  Epoch 3/3, Loss=-0.3107\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_65.pt\n",
      "Cluster 66 | X=(490, 64, 15, 22)\n",
      "  Epoch 1/3, Loss=-0.2307\n",
      "  Epoch 2/3, Loss=-0.3592\n",
      "  Epoch 3/3, Loss=-0.3037\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_66.pt\n",
      "Cluster 67 | X=(490, 64, 16, 22)\n",
      "  Epoch 1/3, Loss=0.0207\n",
      "  Epoch 2/3, Loss=-0.3200\n",
      "  Epoch 3/3, Loss=-0.2803\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_67.pt\n",
      "Cluster 68 | X=(490, 64, 16, 22)\n",
      "  Epoch 1/3, Loss=-0.1128\n",
      "  Epoch 2/3, Loss=-0.2037\n",
      "  Epoch 3/3, Loss=-0.3305\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_68.pt\n",
      "Cluster 69 | X=(490, 64, 14, 22)\n",
      "  Epoch 1/3, Loss=-0.2761\n",
      "  Epoch 2/3, Loss=-0.3052\n",
      "  Epoch 3/3, Loss=-0.2864\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_69.pt\n",
      "Cluster 70 | X=(490, 64, 14, 22)\n",
      "  Epoch 1/3, Loss=0.2325\n",
      "  Epoch 2/3, Loss=-0.2789\n",
      "  Epoch 3/3, Loss=-0.2536\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_70.pt\n",
      "Cluster 71 | X=(490, 64, 16, 22)\n",
      "  Epoch 1/3, Loss=-0.3180\n",
      "  Epoch 2/3, Loss=-0.3150\n",
      "  Epoch 3/3, Loss=-0.3115\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_71.pt\n",
      "Cluster 72 | X=(490, 64, 13, 22)\n",
      "  Epoch 1/3, Loss=-0.6742\n",
      "  Epoch 2/3, Loss=-0.2077\n",
      "  Epoch 3/3, Loss=-0.2776\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_72.pt\n",
      "Cluster 73 | X=(490, 64, 13, 22)\n",
      "  Epoch 1/3, Loss=0.1354\n",
      "  Epoch 2/3, Loss=-0.3017\n",
      "  Epoch 3/3, Loss=-0.2389\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_73.pt\n",
      "Cluster 74 | X=(490, 64, 15, 22)\n",
      "  Epoch 1/3, Loss=-0.2344\n",
      "  Epoch 2/3, Loss=-0.2513\n",
      "  Epoch 3/3, Loss=-0.2659\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_74.pt\n",
      "Cluster 75 | X=(490, 64, 14, 22)\n",
      "  Epoch 1/3, Loss=-1.0456\n",
      "  Epoch 2/3, Loss=-0.1972\n",
      "  Epoch 3/3, Loss=-0.3647\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_75.pt\n",
      "Cluster 76 | X=(490, 64, 13, 22)\n",
      "  Epoch 1/3, Loss=-0.3472\n",
      "  Epoch 2/3, Loss=-0.2930\n",
      "  Epoch 3/3, Loss=-0.2413\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_76.pt\n",
      "Cluster 77 | X=(490, 64, 12, 22)\n",
      "  Epoch 1/3, Loss=-0.2682\n",
      "  Epoch 2/3, Loss=-0.0960\n",
      "  Epoch 3/3, Loss=-0.2897\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_77.pt\n",
      "Cluster 78 | X=(490, 64, 11, 22)\n",
      "  Epoch 1/3, Loss=-0.4981\n",
      "  Epoch 2/3, Loss=-0.0496\n",
      "  Epoch 3/3, Loss=-0.3093\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_78.pt\n",
      "Cluster 79 | X=(490, 64, 9, 22)\n",
      "  Epoch 1/3, Loss=0.3874\n",
      "  Epoch 2/3, Loss=-0.4716\n",
      "  Epoch 3/3, Loss=-0.1319\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_79.pt\n",
      "Cluster 80 | X=(490, 64, 12, 22)\n",
      "  Epoch 1/3, Loss=0.0011\n",
      "  Epoch 2/3, Loss=-0.2669\n",
      "  Epoch 3/3, Loss=-0.1876\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_80.pt\n",
      "Cluster 81 | X=(490, 64, 12, 22)\n",
      "  Epoch 1/3, Loss=-0.7143\n",
      "  Epoch 2/3, Loss=-0.2018\n",
      "  Epoch 3/3, Loss=-0.2586\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_81.pt\n",
      "Cluster 82 | X=(490, 64, 8, 22)\n",
      "  Epoch 1/3, Loss=0.0965\n",
      "  Epoch 2/3, Loss=-0.1892\n",
      "  Epoch 3/3, Loss=-0.0318\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_82.pt\n",
      "Cluster 83 | X=(490, 64, 9, 22)\n",
      "  Epoch 1/3, Loss=-0.3551\n",
      "  Epoch 2/3, Loss=-0.1579\n",
      "  Epoch 3/3, Loss=-0.2081\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_83.pt\n",
      "Cluster 84 | X=(490, 64, 9, 22)\n",
      "  Epoch 1/3, Loss=-0.2648\n",
      "  Epoch 2/3, Loss=-0.1211\n",
      "  Epoch 3/3, Loss=-0.2490\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_84.pt\n",
      "Cluster 85 | X=(490, 64, 8, 22)\n",
      "  Epoch 1/3, Loss=-0.1468\n",
      "  Epoch 2/3, Loss=-0.0999\n",
      "  Epoch 3/3, Loss=-0.2211\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_85.pt\n",
      "Cluster 86 | X=(490, 64, 6, 22)\n",
      "  Epoch 1/3, Loss=-0.1325\n",
      "  Epoch 2/3, Loss=-0.3283\n",
      "  Epoch 3/3, Loss=-0.0688\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_86.pt\n",
      "Cluster 87 | X=(490, 64, 6, 22)\n",
      "  Epoch 1/3, Loss=-0.2874\n",
      "  Epoch 2/3, Loss=-0.2682\n",
      "  Epoch 3/3, Loss=0.0180\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_87.pt\n",
      "Cluster 88 | X=(490, 64, 8, 22)\n",
      "  Epoch 1/3, Loss=0.0981\n",
      "  Epoch 2/3, Loss=-0.2406\n",
      "  Epoch 3/3, Loss=-0.0618\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_88.pt\n",
      "Cluster 89 | X=(490, 64, 5, 22)\n",
      "  Epoch 1/3, Loss=-0.0016\n",
      "  Epoch 2/3, Loss=-0.1439\n",
      "  Epoch 3/3, Loss=-0.0963\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_89.pt\n",
      "Cluster 90 | X=(490, 64, 5, 22)\n",
      "  Epoch 1/3, Loss=-0.0398\n",
      "  Epoch 2/3, Loss=-0.0786\n",
      "  Epoch 3/3, Loss=-0.1417\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_90.pt\n",
      "Cluster 91 | X=(490, 64, 6, 22)\n",
      "  Epoch 1/3, Loss=0.0177\n",
      "  Epoch 2/3, Loss=0.1052\n",
      "  Epoch 3/3, Loss=-0.3204\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_91.pt\n",
      "Cluster 92 | X=(490, 64, 4, 22)\n",
      "  Epoch 1/3, Loss=-0.4994\n",
      "  Epoch 2/3, Loss=0.0436\n",
      "  Epoch 3/3, Loss=-0.2179\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_92.pt\n",
      "Cluster 93 | X=(490, 64, 3, 22)\n",
      "  Epoch 1/3, Loss=0.0290\n",
      "  Epoch 2/3, Loss=-0.1734\n",
      "  Epoch 3/3, Loss=-0.0069\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_93.pt\n",
      "Cluster 94 | X=(490, 64, 3, 22)\n",
      "  Epoch 1/3, Loss=-0.1508\n",
      "  Epoch 2/3, Loss=-0.0942\n",
      "  Epoch 3/3, Loss=-0.0299\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_94.pt\n",
      "Cluster 95 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=-0.1643\n",
      "  Epoch 2/3, Loss=-0.0723\n",
      "  Epoch 3/3, Loss=-0.0080\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_95.pt\n",
      "Cluster 96 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=-0.1337\n",
      "  Epoch 2/3, Loss=0.0485\n",
      "  Epoch 3/3, Loss=0.0072\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_96.pt\n",
      "Cluster 97 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=0.1702\n",
      "  Epoch 2/3, Loss=-0.0836\n",
      "  Epoch 3/3, Loss=-0.0801\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_97.pt\n",
      "Cluster 98 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=0.4611\n",
      "  Epoch 2/3, Loss=0.0027\n",
      "  Epoch 3/3, Loss=-0.1840\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_98.pt\n",
      "Cluster 99 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=-0.2581\n",
      "  Epoch 2/3, Loss=-0.0862\n",
      "  Epoch 3/3, Loss=-0.0863\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_99.pt\n",
      "Cluster 100 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=0.1953\n",
      "  Epoch 2/3, Loss=-0.0973\n",
      "  Epoch 3/3, Loss=-0.1211\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_100.pt\n",
      "Cluster 101 | X=(490, 64, 3, 22)\n",
      "  Epoch 1/3, Loss=-0.0606\n",
      "  Epoch 2/3, Loss=-0.0611\n",
      "  Epoch 3/3, Loss=-0.0431\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_101.pt\n",
      "Cluster 102 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=0.0610\n",
      "  Epoch 2/3, Loss=-0.0880\n",
      "  Epoch 3/3, Loss=-0.0307\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_102.pt\n",
      "Cluster 103 | X=(490, 64, 1, 22)\n",
      "  Epoch 1/3, Loss=-0.0861\n",
      "  Epoch 2/3, Loss=0.0059\n",
      "  Epoch 3/3, Loss=0.0244\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_103.pt\n",
      "Cluster 104 | X=(490, 64, 1, 22)\n",
      "  Epoch 1/3, Loss=0.0018\n",
      "  Epoch 2/3, Loss=-0.0152\n",
      "  Epoch 3/3, Loss=-0.0361\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_104.pt\n",
      "Cluster 105 | X=(490, 64, 1, 22)\n",
      "  Epoch 1/3, Loss=0.0241\n",
      "  Epoch 2/3, Loss=-0.0561\n",
      "  Epoch 3/3, Loss=-0.0355\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_105.pt\n",
      "Cluster 106 | X=(490, 64, 1, 22)\n",
      "  Epoch 1/3, Loss=-0.0864\n",
      "  Epoch 2/3, Loss=0.0355\n",
      "  Epoch 3/3, Loss=0.0395\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_106.pt\n",
      "Cluster 107 | X=(490, 64, 2, 22)\n",
      "  Epoch 1/3, Loss=-0.0641\n",
      "  Epoch 2/3, Loss=0.0199\n",
      "  Epoch 3/3, Loss=-0.0876\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_107.pt\n",
      "Cluster 108 | X=(490, 64, 1, 22)\n",
      "  Epoch 1/3, Loss=-0.1931\n",
      "  Epoch 2/3, Loss=-0.0208\n",
      "  Epoch 3/3, Loss=0.0298\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_108.pt\n",
      "✅ Done Block 8: signals saved to ./signals/a3c_signals.csv, models in ./models/\n"
     ]
    }
   ],
   "source": [
    "# Block 8 — A3C multi-stock per-cluster (save models + signals)\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "SIG_DIR  = \"./signals/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Reset signals file ---\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# --- Load metadata ---\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "def a3c_loss(logits, values, actions, rewards, beta=0.01):\n",
    "    adv = rewards - values.squeeze(-1)\n",
    "    critic = adv.pow(2).mean()\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    actor = -(logp.gather(1, actions.unsqueeze(1)).squeeze(1) * adv.detach()).mean()\n",
    "    entropy = -(torch.softmax(logits, dim=-1) * logp).sum(-1).mean()\n",
    "    return actor + 0.5*critic - beta*entropy\n",
    "\n",
    "# --- Train + inference per cluster ---\n",
    "def process_cluster(meta, epochs=3, lr=1e-3, batch_size=256):\n",
    "    c_id, tickers, dates = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # rewards (B,N)\n",
    "    px = []\n",
    "    for tk in tickers:\n",
    "        s = df_backtest[df_backtest[\"ticker\"]==tk].set_index(\"timestamp\")[\"close\"]\n",
    "        s = s.reindex(dates).ffill().bfill().values\n",
    "        px.append(s)\n",
    "    px = np.stack(px, axis=1)\n",
    "    r = np.zeros_like(px, dtype=np.float32)\n",
    "    r[1:] = np.log(px[1:]/np.maximum(px[:-1],1e-9))\n",
    "\n",
    "    # model + optimizer\n",
    "    model = A3CNet(F).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # mini-batch generator\n",
    "    total = B*N\n",
    "    def iterator():\n",
    "        for start in range(0, total, batch_size):\n",
    "            end = min(total, start+batch_size)\n",
    "            xb, rb, idx = [], [], []\n",
    "            for s in range(start,end):\n",
    "                b, n = divmod(s,N)\n",
    "                xb.append(X[b,:,n,:]); rb.append(r[b,n]); idx.append((b,n))\n",
    "            yield np.stack(xb), np.array(rb), idx\n",
    "\n",
    "    # --- train ---\n",
    "    for ep in range(epochs):\n",
    "        loss_ep = 0\n",
    "        for xb, rb, _ in iterator():\n",
    "            xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "            rb = torch.tensor(rb, dtype=torch.float32).to(device)\n",
    "            act = torch.randint(0,3,(len(xb),),dtype=torch.long).to(device)\n",
    "            logits, vals = model(xb)\n",
    "            loss = a3c_loss(logits, vals, act, rb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            loss_ep += loss.item()\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Loss={loss_ep:.4f}\")\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # --- save model checkpoint ---\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"  ✅ Saved model checkpoint: {model_path}\")\n",
    "\n",
    "    # --- inference & save signals ---\n",
    "    with open(SIG_FILE,\"a\",newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for xb, _, idx in iterator():\n",
    "                xb = torch.tensor(xb,dtype=torch.float32).to(device)\n",
    "                act = torch.argmax(model(xb)[0], dim=-1).cpu().numpy()-1\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    w.writerow([dates[b], tickers[n], int(act[k])])\n",
    "                del xb, act\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, px, r, model, opt\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run all clusters ---\n",
    "for meta in tensor_index:\n",
    "    process_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 8: signals saved to {SIG_FILE}, models in {MODEL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8b9877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inference] Cluster 0 | X=(490, 64, 21, 22)\n",
      "[Inference] Cluster 1 | X=(490, 64, 15, 22)\n",
      "[Inference] Cluster 2 | X=(490, 64, 15, 22)\n",
      "[Inference] Cluster 3 | X=(490, 64, 17, 22)\n",
      "[Inference] Cluster 4 | X=(490, 64, 19, 22)\n",
      "[Inference] Cluster 5 | X=(490, 64, 21, 22)\n",
      "[Inference] Cluster 6 | X=(490, 64, 23, 22)\n",
      "[Inference] Cluster 7 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 8 | X=(490, 64, 21, 22)\n",
      "[Inference] Cluster 9 | X=(490, 64, 22, 22)\n",
      "[Inference] Cluster 10 | X=(490, 64, 23, 22)\n",
      "[Inference] Cluster 11 | X=(490, 64, 19, 22)\n",
      "[Inference] Cluster 12 | X=(490, 64, 28, 22)\n",
      "[Inference] Cluster 13 | X=(490, 64, 29, 22)\n",
      "[Inference] Cluster 14 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 15 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 16 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 17 | X=(490, 64, 26, 22)\n",
      "[Inference] Cluster 18 | X=(490, 64, 23, 22)\n",
      "[Inference] Cluster 19 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 20 | X=(490, 64, 27, 22)\n",
      "[Inference] Cluster 21 | X=(490, 64, 22, 22)\n",
      "[Inference] Cluster 22 | X=(490, 64, 26, 22)\n",
      "[Inference] Cluster 23 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 24 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 25 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 26 | X=(490, 64, 29, 22)\n",
      "[Inference] Cluster 27 | X=(490, 64, 28, 22)\n",
      "[Inference] Cluster 28 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 29 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 30 | X=(490, 64, 21, 22)\n",
      "[Inference] Cluster 31 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 32 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 33 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 34 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 35 | X=(490, 64, 26, 22)\n",
      "[Inference] Cluster 36 | X=(490, 64, 26, 22)\n",
      "[Inference] Cluster 37 | X=(490, 64, 29, 22)\n",
      "[Inference] Cluster 38 | X=(490, 64, 23, 22)\n",
      "[Inference] Cluster 39 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 40 | X=(490, 64, 23, 22)\n",
      "[Inference] Cluster 41 | X=(490, 64, 22, 22)\n",
      "[Inference] Cluster 42 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 43 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 44 | X=(490, 64, 27, 22)\n",
      "[Inference] Cluster 45 | X=(490, 64, 25, 22)\n",
      "[Inference] Cluster 46 | X=(490, 64, 23, 22)\n",
      "[Inference] Cluster 47 | X=(490, 64, 22, 22)\n",
      "[Inference] Cluster 48 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 49 | X=(490, 64, 20, 22)\n",
      "[Inference] Cluster 50 | X=(490, 64, 22, 22)\n",
      "[Inference] Cluster 51 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 52 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 53 | X=(490, 64, 24, 22)\n",
      "[Inference] Cluster 54 | X=(490, 64, 20, 22)\n",
      "[Inference] Cluster 55 | X=(490, 64, 20, 22)\n",
      "[Inference] Cluster 56 | X=(490, 64, 18, 22)\n",
      "[Inference] Cluster 57 | X=(490, 64, 18, 22)\n",
      "[Inference] Cluster 58 | X=(490, 64, 19, 22)\n",
      "[Inference] Cluster 59 | X=(490, 64, 19, 22)\n",
      "[Inference] Cluster 60 | X=(490, 64, 22, 22)\n",
      "[Inference] Cluster 61 | X=(490, 64, 20, 22)\n",
      "[Inference] Cluster 62 | X=(490, 64, 21, 22)\n",
      "[Inference] Cluster 63 | X=(490, 64, 19, 22)\n",
      "[Inference] Cluster 64 | X=(490, 64, 18, 22)\n",
      "[Inference] Cluster 65 | X=(490, 64, 17, 22)\n",
      "[Inference] Cluster 66 | X=(490, 64, 15, 22)\n",
      "[Inference] Cluster 67 | X=(490, 64, 16, 22)\n",
      "[Inference] Cluster 68 | X=(490, 64, 16, 22)\n",
      "[Inference] Cluster 69 | X=(490, 64, 14, 22)\n",
      "[Inference] Cluster 70 | X=(490, 64, 14, 22)\n",
      "[Inference] Cluster 71 | X=(490, 64, 16, 22)\n",
      "[Inference] Cluster 72 | X=(490, 64, 13, 22)\n",
      "[Inference] Cluster 73 | X=(490, 64, 13, 22)\n",
      "[Inference] Cluster 74 | X=(490, 64, 15, 22)\n",
      "[Inference] Cluster 75 | X=(490, 64, 14, 22)\n",
      "[Inference] Cluster 76 | X=(490, 64, 13, 22)\n",
      "[Inference] Cluster 77 | X=(490, 64, 12, 22)\n",
      "[Inference] Cluster 78 | X=(490, 64, 11, 22)\n",
      "[Inference] Cluster 79 | X=(490, 64, 9, 22)\n",
      "[Inference] Cluster 80 | X=(490, 64, 12, 22)\n",
      "[Inference] Cluster 81 | X=(490, 64, 12, 22)\n",
      "[Inference] Cluster 82 | X=(490, 64, 8, 22)\n",
      "[Inference] Cluster 83 | X=(490, 64, 9, 22)\n",
      "[Inference] Cluster 84 | X=(490, 64, 9, 22)\n",
      "[Inference] Cluster 85 | X=(490, 64, 8, 22)\n",
      "[Inference] Cluster 86 | X=(490, 64, 6, 22)\n",
      "[Inference] Cluster 87 | X=(490, 64, 6, 22)\n",
      "[Inference] Cluster 88 | X=(490, 64, 8, 22)\n",
      "[Inference] Cluster 89 | X=(490, 64, 5, 22)\n",
      "[Inference] Cluster 90 | X=(490, 64, 5, 22)\n",
      "[Inference] Cluster 91 | X=(490, 64, 6, 22)\n",
      "[Inference] Cluster 92 | X=(490, 64, 4, 22)\n",
      "[Inference] Cluster 93 | X=(490, 64, 3, 22)\n",
      "[Inference] Cluster 94 | X=(490, 64, 3, 22)\n",
      "[Inference] Cluster 95 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 96 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 97 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 98 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 99 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 100 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 101 | X=(490, 64, 3, 22)\n",
      "[Inference] Cluster 102 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 103 | X=(490, 64, 1, 22)\n",
      "[Inference] Cluster 104 | X=(490, 64, 1, 22)\n",
      "[Inference] Cluster 105 | X=(490, 64, 1, 22)\n",
      "[Inference] Cluster 106 | X=(490, 64, 1, 22)\n",
      "[Inference] Cluster 107 | X=(490, 64, 2, 22)\n",
      "[Inference] Cluster 108 | X=(490, 64, 1, 22)\n",
      "✅ Done Block 9: inference signals saved to ./signals/a3c_signals_infer.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 9 — Inference từ checkpoint A3C (load saved models)\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "SIG_DIR   = \"./signals/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Reset file signals ---\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# --- Load metadata ---\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model định nghĩa lại (giống Block 8) ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Inference function ---\n",
    "def infer_cluster(meta, batch_size=256):\n",
    "    c_id, tickers, dates = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"[Inference] Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # load model checkpoint\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️ Model checkpoint not found: {model_path}, skip\")\n",
    "        return\n",
    "    model = A3CNet(F).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # mini-batch inference & save signals\n",
    "    total = B * N\n",
    "    with open(SIG_FILE, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, total, batch_size):\n",
    "                end = min(total, start+batch_size)\n",
    "                xb, idx = [], []\n",
    "                for s in range(start, end):\n",
    "                    b, n = divmod(s, N)\n",
    "                    xb.append(X[b, :, n, :])\n",
    "                    idx.append((b, n))\n",
    "                xb = torch.tensor(np.stack(xb), dtype=torch.float32).to(device)\n",
    "                acts = torch.argmax(model(xb)[0], dim=-1).cpu().numpy() - 1\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    w.writerow([dates[b], tickers[n], int(acts[k])])\n",
    "                del xb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, model\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run inference all clusters ---\n",
    "for meta in tensor_index:\n",
    "    infer_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 9: inference signals saved to {SIG_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b2d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Fetching VNINDEX benchmark...\n",
      "Fetching data, it may take a while. Please wait...\n",
      "✅ Done Block 10: Portfolio backtest + benchmark computed.\n"
     ]
    }
   ],
   "source": [
    "# Block 10 — Portfolio Construction & Backtest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from FiinQuantX import FiinSession\n",
    "\n",
    "SIG_DIR = \"./signals/\"\n",
    "OUTPUT_DIR = \"./backtest/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "INIT_CAPITAL = 10_000  # vốn ban đầu\n",
    "BENCHMARK = \"VNINDEX\"\n",
    "\n",
    "# --- Load signals từ Block 9 ---\n",
    "a3c_signals = pd.read_csv(os.path.join(SIG_DIR, \"a3c_signals_infer.csv\"))\n",
    "a3c_signals[\"date\"] = pd.to_datetime(a3c_signals[\"date\"])\n",
    "\n",
    "# --- Merge signals với giá close ---\n",
    "df_merge = pd.merge(\n",
    "    a3c_signals,\n",
    "    df_backtest.rename(columns={\"timestamp\": \"date\"}),\n",
    "    on=[\"date\", \"ticker\"],\n",
    "    how=\"left\"\n",
    ").sort_values([\"date\", \"ticker\"])\n",
    "\n",
    "# --- Tính return của từng mã ---\n",
    "df_merge[\"return\"] = df_merge.groupby(\"ticker\")[\"close\"].pct_change().fillna(0)\n",
    "\n",
    "# --- Return có trọng số theo tín hiệu ---\n",
    "df_merge[\"weighted_ret\"] = df_merge[\"signal\"] * df_merge[\"return\"]\n",
    "\n",
    "# --- Portfolio return = trung bình theo ngày ---\n",
    "port_ret = df_merge.groupby(\"date\")[\"weighted_ret\"].mean()\n",
    "\n",
    "# --- Portfolio equity ---\n",
    "portfolio_value = (1 + port_ret).cumprod() * INIT_CAPITAL\n",
    "\n",
    "# --- Benchmark: VNINDEX từ API ---\n",
    "print(\"🔄 Fetching VNINDEX benchmark...\")\n",
    "username = \"DSTC_18@fiinquant.vn\"\n",
    "password = \"Fiinquant0606\"\n",
    "client = FiinSession(username=username, password=password).login()\n",
    "\n",
    "event_history = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=BENCHMARK,\n",
    "    fields=['close'],\n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2023-01-01\"\n",
    ")\n",
    "\n",
    "bench_df = event_history.get_data()\n",
    "bench_df[\"date\"] = pd.to_datetime(bench_df[\"timestamp\"])\n",
    "bench_df = bench_df.set_index(\"date\")[\"close\"]\n",
    "\n",
    "bench_ret = bench_df.pct_change().fillna(0)\n",
    "benchmark_value = (1 + bench_ret).cumprod() * INIT_CAPITAL\n",
    "\n",
    "print(\"✅ Done Block 10: Portfolio backtest + benchmark computed.\")\n",
    "\n",
    "# --- Save outputs ---\n",
    "df_merge.to_csv(os.path.join(OUTPUT_DIR, \"signals_full.csv\"), index=False)\n",
    "port_ret.to_frame(\"port_ret\").to_csv(os.path.join(OUTPUT_DIR, \"port_ret.csv\"))\n",
    "portfolio_value.to_frame(\"portfolio_value\").to_csv(os.path.join(OUTPUT_DIR, \"portfolio_value.csv\"))\n",
    "benchmark_value.to_frame(\"benchmark_value\").to_csv(os.path.join(OUTPUT_DIR, \"benchmark_value.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841ea381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train split done: 2023-09-14 00:00:00 → 2024-06-06 00:00:00\n",
      "✅ Val split done: 2024-06-06 00:00:00 → 2024-12-31 00:00:00\n",
      "✅ Test split done: 2024-12-31 00:00:00 → 2025-08-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Block 11 — Walk-Forward Validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from FiinQuantX import FiinSession\n",
    "\n",
    "OUTPUT_DIR = \"./backtest/\"\n",
    "WFV_DIR = \"./walkforward/\"\n",
    "os.makedirs(WFV_DIR, exist_ok=True)\n",
    "\n",
    "INIT_CAPITAL = 10_000\n",
    "BENCHMARK = \"VNINDEX\"\n",
    "\n",
    "# --- Load signals từ Block 10 ---\n",
    "df_merge = pd.read_csv(os.path.join(OUTPUT_DIR, \"signals_full.csv\"))\n",
    "df_merge[\"date\"] = pd.to_datetime(df_merge[\"date\"])\n",
    "\n",
    "# --- Load lại df_backtest ---\n",
    "df_backtest[\"timestamp\"] = pd.to_datetime(df_backtest[\"timestamp\"])\n",
    "\n",
    "# --- Mốc thời gian WFV ---\n",
    "train_end = \"2024-06-06\"\n",
    "val_end   = \"2024-12-31\"\n",
    "test_end  = df_merge[\"date\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "splits = {\n",
    "    \"train\": (df_merge[\"date\"].min(), train_end),\n",
    "    \"val\": (train_end, val_end),\n",
    "    \"test\": (val_end, test_end)\n",
    "}\n",
    "\n",
    "# --- Hàm backtest theo split ---\n",
    "def run_backtest(split_name, start, end):\n",
    "    sub = df_merge[(df_merge[\"date\"] > start) & (df_merge[\"date\"] <= end)].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"⚠️ No data for {split_name}\")\n",
    "        return\n",
    "\n",
    "    # return có trọng số\n",
    "    sub[\"weighted_ret\"] = sub[\"signal\"] * sub.groupby(\"ticker\")[\"close\"].pct_change().fillna(0)\n",
    "\n",
    "    # portfolio return & equity\n",
    "    port_ret = sub.groupby(\"date\")[\"weighted_ret\"].mean()\n",
    "    port_val = (1 + port_ret).cumprod() * INIT_CAPITAL\n",
    "\n",
    "    # benchmark subset\n",
    "    bench_sub = bench_ret.loc[start:end]\n",
    "    bench_val = (1 + bench_sub).cumprod() * INIT_CAPITAL\n",
    "\n",
    "    # save\n",
    "    port_ret.to_frame(\"port_ret\").to_csv(os.path.join(WFV_DIR, f\"{split_name}_port_ret.csv\"))\n",
    "    port_val.to_frame(\"portfolio_value\").to_csv(os.path.join(WFV_DIR, f\"{split_name}_portfolio_value.csv\"))\n",
    "    bench_val.to_frame(\"benchmark_value\").to_csv(os.path.join(WFV_DIR, f\"{split_name}_benchmark_value.csv\"))\n",
    "    sub.to_csv(os.path.join(WFV_DIR, f\"{split_name}_signals.csv\"), index=False)\n",
    "\n",
    "    print(f\"✅ {split_name.capitalize()} split done: {start} → {end}\")\n",
    "\n",
    "# --- Run WFV ---\n",
    "for split, (start, end) in splits.items():\n",
    "    run_backtest(split, pd.to_datetime(start), pd.to_datetime(end))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8769c87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Full Backtest Stats:\n",
      "Final Value          10029.894326\n",
      "ROI (%)                  0.298943\n",
      "Sharpe                   0.542019\n",
      "Sortino                  0.905769\n",
      "Max Drawdown (%)        -0.204617\n",
      "Trades                 489.000000\n",
      "Win Rate (%)            49.693252\n",
      "Benchmark ROI (%)       61.146662\n",
      "dtype: float64\n",
      "\n",
      "📊 Walk-Forward Validation Stats:\n",
      "                          train           val         test\n",
      "Final Value        10021.122617  10013.570994  9995.183885\n",
      "ROI (%)                0.211226      0.135710    -0.048161\n",
      "Sharpe                 0.914405      0.907077    -0.287799\n",
      "Sortino                1.410890      1.741976    -0.496889\n",
      "Max Drawdown (%)      -0.184534     -0.108936    -0.197115\n",
      "Trades               179.000000    145.000000   162.000000\n",
      "Win Rate (%)          53.072626     48.275862    46.913580\n",
      "Benchmark ROI (%)      4.882294     -1.307302    32.794171\n",
      "\n",
      "📊 Special Period (26/03/2025 → 15/04/2025) Stats:\n",
      "Final Value          10047.490851\n",
      "ROI (%)                  0.040172\n",
      "Sharpe                   3.480985\n",
      "Sortino                  7.258920\n",
      "Max Drawdown (%)        -0.056261\n",
      "Trades                  14.000000\n",
      "Win Rate (%)            50.000000\n",
      "Benchmark ROI (%)       -7.412770\n",
      "dtype: float64\n",
      "\n",
      "✅ All stats saved to ./backtest/all_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 12 — Visualization & Performance Stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, gc\n",
    "\n",
    "OUTPUT_DIR = \"./backtest/\"\n",
    "WFV_DIR = \"./walkforward/\"\n",
    "STATS_FILE = os.path.join(OUTPUT_DIR, \"all_stats.csv\")\n",
    "\n",
    "INIT_CAPITAL = 10_000\n",
    "\n",
    "# --- Helper: max drawdown ---\n",
    "def max_drawdown(series):\n",
    "    cummax = series.cummax()\n",
    "    drawdown = (series - cummax) / cummax\n",
    "    return drawdown.min()\n",
    "\n",
    "# --- Helper: performance metrics ---\n",
    "def compute_stats(port_val, port_ret, bench_val=None):\n",
    "    stats = {}\n",
    "    stats[\"Final Value\"] = port_val.iloc[-1]\n",
    "    stats[\"ROI (%)\"] = (port_val.iloc[-1] / port_val.iloc[0] - 1) * 100\n",
    "    stats[\"Sharpe\"] = port_ret.mean() / port_ret.std() * np.sqrt(252) if port_ret.std() > 0 else 0\n",
    "    stats[\"Sortino\"] = port_ret.mean() / port_ret[port_ret < 0].std() * np.sqrt(252) if port_ret[port_ret < 0].std() > 0 else 0\n",
    "    stats[\"Max Drawdown (%)\"] = max_drawdown(port_val) * 100\n",
    "    stats[\"Trades\"] = (port_ret != 0).sum()\n",
    "    stats[\"Win Rate (%)\"] = (port_ret > 0).sum() / max(1, (port_ret != 0).sum()) * 100\n",
    "    if bench_val is not None:\n",
    "        stats[\"Benchmark ROI (%)\"] = (bench_val.iloc[-1] / bench_val.iloc[0] - 1) * 100\n",
    "    return stats\n",
    "\n",
    "# --- Plotting helper ---\n",
    "def plot_equity_curve(port_val, bench_val, title, save_path):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(port_val.index, port_val.values, label=\"Portfolio\")\n",
    "    if bench_val is not None:\n",
    "        plt.plot(bench_val.index, bench_val.values, label=\"Benchmark (VNINDEX)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_histogram(port_ret, title, save_path):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(port_ret, bins=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Daily Return\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# --- Load full backtest (Block 10) ---\n",
    "# --- Load full backtest (Block 10) ---\n",
    "port_val = pd.read_csv(os.path.join(OUTPUT_DIR, \"portfolio_value.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "bench_val = pd.read_csv(os.path.join(OUTPUT_DIR, \"benchmark_value.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "\n",
    "# ✅ Tính lại daily return\n",
    "port_ret = port_val.pct_change().fillna(0)\n",
    "\n",
    "\n",
    "stats_full = compute_stats(port_val, port_ret, bench_val)\n",
    "plot_equity_curve(port_val, bench_val, \"Full Backtest Equity Curve\", os.path.join(OUTPUT_DIR, \"equity_full.png\"))\n",
    "plot_histogram(port_ret, \"Full Backtest Daily Returns\", os.path.join(OUTPUT_DIR, \"hist_full.png\"))\n",
    "\n",
    "print(\"📊 Full Backtest Stats:\")\n",
    "print(pd.Series(stats_full))\n",
    "\n",
    "# --- Load Walk-Forward splits (Block 11) ---\n",
    "stats_wfv = {}\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    try:\n",
    "        pv = pd.read_csv(os.path.join(WFV_DIR, f\"{split}_portfolio_value.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "        pr = pd.read_csv(os.path.join(WFV_DIR, f\"{split}_port_ret.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "        bv = pd.read_csv(os.path.join(WFV_DIR, f\"{split}_benchmark_value.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "        stats_wfv[split] = compute_stats(pv, pr, bv)\n",
    "\n",
    "        plot_equity_curve(pv, bv, f\"{split.capitalize()} Split Equity Curve\", os.path.join(WFV_DIR, f\"equity_{split}.png\"))\n",
    "        plot_histogram(pr, f\"{split.capitalize()} Split Daily Returns\", os.path.join(WFV_DIR, f\"hist_{split}.png\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skip {split}: {e}\")\n",
    "\n",
    "print(\"\\n📊 Walk-Forward Validation Stats:\")\n",
    "print(pd.DataFrame(stats_wfv))\n",
    "\n",
    "# --- Special Period Analysis: 26/03/2025 → 15/04/2025 ---\n",
    "special_start, special_end = \"2025-03-26\", \"2025-04-15\"\n",
    "sub_val = port_val.loc[special_start:special_end]\n",
    "sub_ret = port_ret.loc[special_start:special_end]\n",
    "sub_bench = bench_val.loc[special_start:special_end]\n",
    "\n",
    "stats_special = {}\n",
    "if not sub_val.empty:\n",
    "    stats_special = compute_stats(sub_val, sub_ret, sub_bench)\n",
    "    plot_equity_curve(sub_val, sub_bench, \"Special Period Equity Curve (Tax Shock)\", os.path.join(OUTPUT_DIR, \"equity_special.png\"))\n",
    "    plot_histogram(sub_ret, \"Special Period Daily Returns\", os.path.join(OUTPUT_DIR, \"hist_special.png\"))\n",
    "\n",
    "    print(\"\\n📊 Special Period (26/03/2025 → 15/04/2025) Stats:\")\n",
    "    print(pd.Series(stats_special))\n",
    "else:\n",
    "    print(\"⚠️ No data in special period (26/03/2025 → 15/04/2025)\")\n",
    "\n",
    "# --- Save all stats to CSV ---\n",
    "all_stats = {\"full\": stats_full}\n",
    "all_stats.update({f\"wfv_{k}\": v for k,v in stats_wfv.items()})\n",
    "if stats_special:\n",
    "    all_stats[\"special\"] = stats_special\n",
    "\n",
    "df_stats = pd.DataFrame(all_stats).T\n",
    "df_stats.to_csv(STATS_FILE)\n",
    "print(f\"\\n✅ All stats saved to {STATS_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
