{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4d6cad",
   "metadata": {},
   "source": [
    "**HƯỚNG DẪN CHẠY**\n",
    "\n",
    "*Nhóm chạy code theo thứ tự từng cell từ trên xuống xuống dưới*\n",
    "\n",
    "**Một số điểm lưu ý:**\n",
    "\n",
    "- *Thời gian chạy các block đa số lâu (Khoảng 10 phút riêng block 6 khoảng 25 phút)*\n",
    "\n",
    "- *Các block 8,9,10,11 có lưu kết quả file csv* \n",
    "\n",
    "- **Các file csv kết quả nhóm có upload lên github**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bbe97",
   "metadata": {},
   "source": [
    "**Tải các thư viện cần thiết** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch scikit-learn matplotlib\n",
    "!pip install --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx\n",
    "!pip install --upgrade --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565db80b",
   "metadata": {},
   "source": [
    "**Block 1: tải dữ liệu lịch sử và realtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a2bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mã HOSE: 415\n",
      "Fetching data, it may take a while. Please wait...\n",
      "History ban đầu:   ticker         timestamp      open      high       low     close     volume  \\\n",
      "0    AAA  2023-01-03 00:00  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA  2023-01-04 00:00  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA  2023-01-05 00:00  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA  2023-01-06 00:00  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA  2023-01-09 00:00  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs           fn  \n",
      "0  938600.0  504700.0   40579000.0  899404000.0  \n",
      "1  462900.0  780600.0  151639000.0   36850000.0  \n",
      "2  487200.0  473700.0  343911000.0  -59103000.0  \n",
      "3  564300.0  828300.0  345999000.0 -294312000.0  \n",
      "4  414000.0  631800.0  514557000.0 -483197000.0  \n"
     ]
    }
   ],
   "source": [
    "# Block 1 — Login & Lấy dữ liệu tất cả HOSE/HNX/UPCOM\n",
    "import pandas as pd\n",
    "from FiinQuantX import FiinSession, BarDataUpdate\n",
    "# --- Login ---\n",
    "username = \"DSTC_18@fiinquant.vn\"\n",
    "password = \"Fiinquant0606\"\n",
    "\n",
    "client = FiinSession(\n",
    "    username=username,\n",
    "    password=password\n",
    ").login()\n",
    "\n",
    "# --- Lấy danh sách cổ phiếu từng sàn ---\n",
    "tickers_hose  = list(client.TickerList(ticker=\"VNINDEX\"))     # HOSE\n",
    "print(f\"Số mã HOSE: {len(tickers_hose)}\")\n",
    "\n",
    "# --- Lấy dữ liệu lịch sử toàn bộ (có thể nặng, nên lấy theo batch nếu cần) ---\n",
    "event_history = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2023-01-01\"   # backtest từ 2023 tới nay\n",
    ")\n",
    "\n",
    "df_all = event_history.get_data()\n",
    "print(\"History ban đầu:\", df_all.head())\n",
    "\n",
    "# --- Callback realtime ---\n",
    "def onDataUpdate(data: BarDataUpdate):\n",
    "    global df_all\n",
    "    df_update = data.to_dataFrame()\n",
    "    df_all = pd.concat([df_all, df_update])\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    print(\"Realtime update:\")\n",
    "    print(df_update.head())\n",
    "\n",
    "# --- Bật realtime nối tiếp dữ liệu ---\n",
    "event_realtime = client.Fetch_Trading_Data(\n",
    "    realtime=True,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    period=1,\n",
    "    callback=onDataUpdate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1122224",
   "metadata": {},
   "source": [
    "**Block 2: lấy dữ liệu FA, lọc các mã không hợp lệ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c93b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Lỗi khi lấy FA cho FUETPVND: 'FUETPVND'\n",
      "Số mã HOSE ban đầu: 415\n",
      "Số mã có dữ liệu FA: 392\n",
      "FA Data sample:\n",
      "   organizationId ticker  year  quarter  \\\n",
      "0          894364    CCC  2023        4   \n",
      "1          894364    CCC  2024        1   \n",
      "2          894364    CCC  2024        2   \n",
      "3          894364    CCC  2024        3   \n",
      "4          894364    CCC  2024        4   \n",
      "\n",
      "                                              ratios ReportDate  \n",
      "0  {'SolvencyRatio': {'DebtToEquityRatio': 1.5102...        NaT  \n",
      "1  {'SolvencyRatio': {'DebtToEquityRatio': 0.7722...        NaT  \n",
      "2  {'SolvencyRatio': {'DebtToEquityRatio': 0.7357...        NaT  \n",
      "3  {'SolvencyRatio': {'DebtToEquityRatio': 0.7914...        NaT  \n",
      "4  {'SolvencyRatio': {'DebtToEquityRatio': 0.6437...        NaT  \n"
     ]
    }
   ],
   "source": [
    "# Block 2 — Lấy dữ liệu FA theo quý (HOSE only)\n",
    "\n",
    "def fetch_fa_quarterly(ticker, latest_year=2025, n_periods=32):\n",
    "    try:\n",
    "        fi_list = client.FundamentalAnalysis().get_ratios(\n",
    "            tickers=[ticker],\n",
    "            TimeFilter=\"Quarterly\",\n",
    "            LatestYear=latest_year,\n",
    "            NumberOfPeriod=n_periods,\n",
    "            Consolidated=True\n",
    "        )\n",
    "\n",
    "        # Nếu không có dữ liệu thì bỏ qua\n",
    "        if not fi_list or not isinstance(fi_list, list):\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(fi_list)\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df[\"ticker\"] = ticker\n",
    "        if \"ReportDate\" in df.columns:\n",
    "            df[\"ReportDate\"] = pd.to_datetime(df[\"ReportDate\"])\n",
    "        else:\n",
    "            # Nếu không có ReportDate thì tạo cột null để tránh lỗi concat\n",
    "            df[\"ReportDate\"] = pd.NaT\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi lấy FA cho {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- Lọc danh sách: chỉ giữ những mã có dữ liệu FA ---\n",
    "fa_list = []\n",
    "valid_tickers = []\n",
    "\n",
    "for t in tickers_hose:   # lấy theo danh sách HOSE từ Block 1\n",
    "    df_fa = fetch_fa_quarterly(t, latest_year=2025, n_periods=32)\n",
    "    if not df_fa.empty:\n",
    "        fa_list.append(df_fa)\n",
    "        valid_tickers.append(t)\n",
    "\n",
    "# --- Gộp DataFrame ---\n",
    "if fa_list:\n",
    "    fa_data = pd.concat(fa_list, ignore_index=True)\n",
    "else:\n",
    "    fa_data = pd.DataFrame()\n",
    "\n",
    "print(f\"Số mã HOSE ban đầu: {len(tickers_hose)}\")\n",
    "print(f\"Số mã có dữ liệu FA: {len(valid_tickers)}\")\n",
    "print(\"FA Data sample:\")\n",
    "print(fa_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9ef83",
   "metadata": {},
   "source": [
    "**Block 3: Chuẩn hóa FA và gộp dữ liệu với giá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831a3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample merged:\n",
      "  ticker  timestamp      open      high       low     close     volume  \\\n",
      "0    AAA 2023-01-03  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA 2023-01-04  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA 2023-01-05  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA 2023-01-06  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA 2023-01-09  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs  ...  DebtToEquityRatio  EBITMargin  \\\n",
      "0  938600.0  504700.0   40579000.0  ...           0.507521   -0.049731   \n",
      "1  462900.0  780600.0  151639000.0  ...           0.507521   -0.049731   \n",
      "2  487200.0  473700.0  343911000.0  ...           0.507521   -0.049731   \n",
      "3  564300.0  828300.0  345999000.0  ...           0.507521   -0.049731   \n",
      "4  414000.0  631800.0  514557000.0  ...           0.507521   -0.049731   \n",
      "\n",
      "        ROA       ROE      ROIC    BasicEPS  PriceToBook  PriceToEarning  \\\n",
      "0  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "1  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "2  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "3  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "4  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "\n",
      "   NetRevenueGrowthYoY  GrossProfitGrowthYoY  \n",
      "0             -0.18869             -0.910016  \n",
      "1             -0.18869             -0.910016  \n",
      "2             -0.18869             -0.910016  \n",
      "3             -0.18869             -0.910016  \n",
      "4             -0.18869             -0.910016  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Số mã merge thành công: 391\n"
     ]
    }
   ],
   "source": [
    "# Block 3 — Chuẩn hoá FA + Merge với giá (HOSE only, dựa theo Block 2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Các chỉ số FA cần lấy ---\n",
    "fa_fields = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "]\n",
    "\n",
    "# --- Hàm nổ ratios ---\n",
    "def explode_ratios(df, fa_fields):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        d = {\n",
    "            \"ticker\": row[\"ticker\"],\n",
    "            \"fa_year\": int(row[\"year\"]),\n",
    "            \"fa_quarter\": int(row[\"quarter\"])\n",
    "        }\n",
    "        ratios = row.get(\"ratios\", {})\n",
    "        if isinstance(ratios, dict):   # ✅ fix chỗ lỗi\n",
    "            for f in fa_fields:\n",
    "                val = None\n",
    "                for section in ratios.values():\n",
    "                    if isinstance(section, dict) and f in section:\n",
    "                        val = section[f]\n",
    "                d[f] = val\n",
    "        else:\n",
    "            # nếu ratios không phải dict thì gán NaN hết\n",
    "            for f in fa_fields:\n",
    "                d[f] = None\n",
    "        records.append(d)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --- Chuẩn hoá FA ---\n",
    "fa_clean = explode_ratios(fa_data, fa_fields)\n",
    "\n",
    "# --- Chuẩn hoá giá ---\n",
    "df_price = df_all[df_all[\"ticker\"].isin(valid_tickers)].copy()\n",
    "df_price[\"timestamp\"] = pd.to_datetime(df_price[\"timestamp\"])\n",
    "df_price = df_price.sort_values([\"ticker\",\"timestamp\"])\n",
    "\n",
    "# tạo key (fa_year, fa_quarter) = quý trước\n",
    "pi = df_price[\"timestamp\"].dt.to_period(\"Q\")\n",
    "prev_pi = pi - 1\n",
    "df_price[\"fa_year\"] = prev_pi.dt.year.astype(int)\n",
    "df_price[\"fa_quarter\"] = prev_pi.dt.quarter.astype(int)\n",
    "\n",
    "# --- Xử lý FA: giữ duy nhất bản cuối cùng mỗi quý\n",
    "fa_clean = (\n",
    "    fa_clean.sort_values([\"ticker\",\"fa_year\",\"fa_quarter\"])\n",
    "            .drop_duplicates(subset=[\"ticker\",\"fa_year\",\"fa_quarter\"], keep=\"last\")\n",
    ")\n",
    "\n",
    "# --- Merge giá + FA ---\n",
    "df_merged = df_price.merge(\n",
    "    fa_clean,\n",
    "    on=[\"ticker\",\"fa_year\",\"fa_quarter\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# FFill theo thời gian trong từng ticker để lấp chỗ trống\n",
    "df_merged = df_merged.sort_values([\"ticker\",\"timestamp\"])\n",
    "df_merged[fa_fields] = df_merged.groupby(\"ticker\")[fa_fields].ffill()\n",
    "\n",
    "print(\"Sample merged:\")\n",
    "print(df_merged.head())\n",
    "print(\"Số mã merge thành công:\", df_merged[\"ticker\"].nunique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08faae38",
   "metadata": {},
   "source": [
    "**Xóa biến df_all không cần thiết nữa để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73580df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_all\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331523dd",
   "metadata": {},
   "source": [
    "**Block 4: Tính các chỉ số TA dựa vào thư viện FiinQuant và ghép dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0de3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with TA:\n",
      "  ticker  timestamp      open      high       low     close     volume  \\\n",
      "0    AAA 2023-01-03  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA 2023-01-04  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA 2023-01-05  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA 2023-01-06  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA 2023-01-09  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs  ...  ema_50  macd  macd_signal  macd_diff  \\\n",
      "0  938600.0  504700.0   40579000.0  ...     NaN   NaN          NaN        NaN   \n",
      "1  462900.0  780600.0  151639000.0  ...     NaN   NaN          NaN        NaN   \n",
      "2  487200.0  473700.0  343911000.0  ...     NaN   NaN          NaN        NaN   \n",
      "3  564300.0  828300.0  345999000.0  ...     NaN   NaN          NaN        NaN   \n",
      "4  414000.0  631800.0  514557000.0  ...     NaN   NaN          NaN        NaN   \n",
      "\n",
      "   rsi  bollinger_hband  bollinger_lband  atr        obv  vwap  \n",
      "0  NaN              NaN              NaN  NaN  1543984.0   NaN  \n",
      "1  NaN              NaN              NaN  NaN   241479.0   NaN  \n",
      "2  NaN              NaN              NaN  NaN  1221952.0   NaN  \n",
      "3  NaN              NaN              NaN  NaN  -209747.0   NaN  \n",
      "4  NaN              NaN              NaN  NaN -1331132.0   NaN  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Shape sau khi thêm TA: (256151, 35)\n"
     ]
    }
   ],
   "source": [
    "# Block 4 — Tính các chỉ số TA (trên df_merged từ Block 3)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Khởi tạo Indicator ---\n",
    "fi = client.FiinIndicator()\n",
    "\n",
    "# --- Hàm tính TA theo từng ticker ---\n",
    "def add_ta_indicators(df):\n",
    "    df = df.sort_values(\"timestamp\").copy()\n",
    "\n",
    "    # EMA\n",
    "    df['ema_5']  = fi.ema(df['close'], window=5)\n",
    "    df['ema_20'] = fi.ema(df['close'], window=20)\n",
    "    df['ema_50'] = fi.ema(df['close'], window=50)\n",
    "\n",
    "    # MACD\n",
    "    df['macd']        = fi.macd(df['close'], window_fast=12, window_slow=26)\n",
    "    df['macd_signal'] = fi.macd_signal(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "    df['macd_diff']   = fi.macd_diff(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "\n",
    "    # RSI\n",
    "    df['rsi'] = fi.rsi(df['close'], window=14)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df['bollinger_hband'] = fi.bollinger_hband(df['close'], window=20, window_dev=2)\n",
    "    df['bollinger_lband'] = fi.bollinger_lband(df['close'], window=20, window_dev=2)\n",
    "\n",
    "    # ATR\n",
    "    df['atr'] = fi.atr(df['high'], df['low'], df['close'], window=14)\n",
    "\n",
    "    # OBV\n",
    "    df['obv'] = fi.obv(df['close'], df['volume'])\n",
    "\n",
    "    # VWAP\n",
    "    df['vwap'] = fi.vwap(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Áp dụng cho toàn bộ df_merged ---\n",
    "df_with_ta = df_merged.groupby(\"ticker\", group_keys=False).apply(add_ta_indicators)\n",
    "\n",
    "print(\"Sample with TA:\")\n",
    "print(df_with_ta.head())\n",
    "print(\"Shape sau khi thêm TA:\", df_with_ta.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235303e",
   "metadata": {},
   "source": [
    "**Xóa bớt biến df_merged không còn cần thiết để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26204a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_merged\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614073e",
   "metadata": {},
   "source": [
    "**Block 5: Chuẩn hóa dữ liệu FA và TA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features:\n",
      "  ticker  timestamp  DebtToEquityRatio  EBITMargin       ROA       ROE  \\\n",
      "0    AAA 2023-06-14           0.559115    0.978956  0.368731  0.415728   \n",
      "1    AAA 2023-06-15           0.559115    0.978956  0.368731  0.415728   \n",
      "2    AAA 2023-06-16           0.559115    0.978956  0.368731  0.415728   \n",
      "3    AAA 2023-06-19           0.559115    0.978956  0.368731  0.415728   \n",
      "4    AAA 2023-06-20           0.559115    0.978956  0.368731  0.415728   \n",
      "\n",
      "     ROIC  BasicEPS  PriceToBook  PriceToEarning  ...  ema_50_z    macd_z  \\\n",
      "0  0.7533  0.158046      0.37764        0.537353  ...  1.799046 -0.008320   \n",
      "1  0.7533  0.158046      0.37764        0.537353  ...  1.761009 -0.314808   \n",
      "2  0.7533  0.158046      0.37764        0.537353  ...  1.701399 -0.891565   \n",
      "3  0.7533  0.158046      0.37764        0.537353  ...  1.651219 -1.278916   \n",
      "4  0.7533  0.158046      0.37764        0.537353  ...  1.609327 -1.509982   \n",
      "\n",
      "   macd_signal_z  macd_diff_z     rsi_z  bollinger_hband_z  bollinger_lband_z  \\\n",
      "0       0.591579    -1.164222 -1.530688           1.363061           1.839777   \n",
      "1       0.399630    -1.467105 -1.541599           1.317089           1.758209   \n",
      "2       0.112479    -2.139400 -2.893380           1.289737           1.634182   \n",
      "3      -0.209795    -2.311526 -2.429426           1.248184           1.569059   \n",
      "4      -0.526222    -2.190019 -2.020537           1.199384           1.537532   \n",
      "\n",
      "      atr_z     obv_z    vwap_z  \n",
      "0  0.589827  1.405288  1.603355  \n",
      "1  0.534358  1.364100  1.554836  \n",
      "2  0.710743  0.982966  1.479150  \n",
      "3  0.593083  1.154836  1.414003  \n",
      "4  0.484151  1.326566  1.346550  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Shape sau khi scaling & dropna: (181828, 24)\n"
     ]
    }
   ],
   "source": [
    "# Block 5 — Feature engineering & scaling\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Danh sách cột FA & TA ---\n",
    "fa_features = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "]\n",
    "\n",
    "ta_features = [\n",
    "    \"ema_5\",\"ema_20\",\"ema_50\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"rsi\",\"bollinger_hband\",\"bollinger_lband\",\"atr\",\"obv\",\"vwap\"\n",
    "]\n",
    "\n",
    "# --- Chuẩn hoá FA: cross-section min-max scaling theo ngày ---\n",
    "def scale_fa_minmax(df):\n",
    "    df_scaled = df.copy()\n",
    "    for f in fa_features:\n",
    "        vals = df[f].astype(float)\n",
    "        vmin, vmax = vals.min(), vals.max()\n",
    "        if np.isfinite(vmin) and np.isfinite(vmax) and vmax > vmin:\n",
    "            df_scaled[f] = (vals - vmin) / (vmax - vmin)\n",
    "        else:\n",
    "            df_scaled[f] = np.nan\n",
    "    return df_scaled\n",
    "\n",
    "df_scaled_fa = df_with_ta.groupby(\"timestamp\", group_keys=False).apply(scale_fa_minmax)\n",
    "\n",
    "# --- Chuẩn hoá TA: rolling z-score theo từng ticker ---\n",
    "def zscore_rolling(series, window=60):\n",
    "    return (series - series.rolling(window).mean()) / series.rolling(window).std()\n",
    "\n",
    "df_scaled = df_scaled_fa.groupby(\"ticker\", group_keys=False).apply(\n",
    "    lambda g: g.assign(**{f\"{col}_z\": zscore_rolling(g[col], 60) for col in ta_features})\n",
    ")\n",
    "\n",
    "# --- Drop các cột gốc TA, giữ bản z-score ---\n",
    "keep_cols = [\"ticker\",\"timestamp\"] + fa_features + [f\"{col}_z\" for col in ta_features]\n",
    "df_features = df_scaled[keep_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(df_features.head())\n",
    "print(\"Shape sau khi scaling & dropna:\", df_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f5d94",
   "metadata": {},
   "source": [
    "**Xóa các biến df_with_ta, df_scaled, df_scaled_fa không cần thiết nữa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee3d19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_with_ta, df_scaled, df_scaled_fa\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245a6ca",
   "metadata": {},
   "source": [
    "**Block 6: Giảm chiều dữ liệu bằng t-SNE và phân cụm bằng DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sample:\n",
      "  ticker  timestamp  cluster     tsne_x     tsne_y    month\n",
      "0    AAA 2023-06-14       -1  34.417667 -21.641058  2023-06\n",
      "1    AAA 2023-06-15       -1  34.361568 -21.848602  2023-06\n",
      "2    AAA 2023-06-16       -1  35.918636 -42.906532  2023-06\n",
      "3    AAA 2023-06-19       -1  35.490566 -42.649422  2023-06\n",
      "4    AAA 2023-06-20       -1  34.553066 -42.459969  2023-06\n",
      "Số cụm mỗi tháng:\n",
      "month\n",
      "2023-06     16\n",
      "2023-07     67\n",
      "2023-08     78\n",
      "2023-09     40\n",
      "2023-10     83\n",
      "2023-11     55\n",
      "2023-12     83\n",
      "2024-01     90\n",
      "2024-02     31\n",
      "2024-03     64\n",
      "2024-04     43\n",
      "2024-05     66\n",
      "2024-06     94\n",
      "2024-07     87\n",
      "2024-08     65\n",
      "2024-09     86\n",
      "2024-10    104\n",
      "2024-11     80\n",
      "2024-12     74\n",
      "2025-01     49\n",
      "2025-02     68\n",
      "2025-03     93\n",
      "2025-04     52\n",
      "2025-05     79\n",
      "2025-06     95\n",
      "2025-07    110\n",
      "2025-08     74\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Block 6 — Dimensionality reduction & Clustering (t-SNE + DBSCAN)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# --- Chọn các cột features để clustering ---\n",
    "feature_cols = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "] + [c for c in df_features.columns if c.endswith(\"_z\")]\n",
    "\n",
    "# --- Thêm cột tháng để snapshot ---\n",
    "df_features[\"month\"] = df_features[\"timestamp\"].dt.to_period(\"M\")\n",
    "\n",
    "cluster_results = []\n",
    "\n",
    "for (month, g) in df_features.groupby(\"month\"):\n",
    "    if len(g) < 10:   # quá ít cổ phiếu thì bỏ\n",
    "        continue\n",
    "\n",
    "    X = g[feature_cols].values\n",
    "\n",
    "    # --- t-SNE giảm chiều còn 2D ---\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"random\", random_state=42)\n",
    "    X_emb = tsne.fit_transform(X)\n",
    "\n",
    "    # --- DBSCAN clustering ---\n",
    "    db = DBSCAN(eps=0.5, min_samples=5).fit(X_emb)\n",
    "    labels = db.labels_\n",
    "\n",
    "    temp = g[[\"ticker\",\"timestamp\"]].copy()\n",
    "    temp[\"cluster\"] = labels\n",
    "    temp[\"tsne_x\"] = X_emb[:,0]\n",
    "    temp[\"tsne_y\"] = X_emb[:,1]\n",
    "    temp[\"month\"]  = str(month)\n",
    "\n",
    "    cluster_results.append(temp)\n",
    "\n",
    "df_clusters = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(\"Cluster sample:\")\n",
    "print(df_clusters.head())\n",
    "print(\"Số cụm mỗi tháng:\")\n",
    "print(df_clusters.groupby(\"month\")[\"cluster\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde0916",
   "metadata": {},
   "source": [
    "**Block 7: Xây tensors (clusters mapping) và masks (active stocks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfadfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7 — Tensors & Masks \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, json\n",
    "\n",
    "LOOKBACK = 64   # window size\n",
    "DATA_DIR = \"./tensors/\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in [\"ticker\",\"timestamp\",\"cluster\",\"month\"]]\n",
    "\n",
    "tensor_index = []\n",
    "\n",
    "for c_id, g in df_clusters.groupby(\"cluster\"):\n",
    "    if c_id == -1:   # noise bỏ qua\n",
    "        continue\n",
    "\n",
    "    tickers = sorted(g[\"ticker\"].unique())\n",
    "    g_feat = df_features[df_features[\"ticker\"].isin(tickers)].copy()\n",
    "\n",
    "    # Pivot: index = timestamp, columns = (ticker, feature)\n",
    "    pivoted = g_feat.pivot(index=\"timestamp\", columns=\"ticker\", values=feature_cols)\n",
    "    pivoted.columns = pd.MultiIndex.from_product([tickers, feature_cols])\n",
    "\n",
    "    # Mask\n",
    "    mask_df = ~pivoted.isna()\n",
    "    pivoted_filled = pivoted.ffill().bfill()\n",
    "\n",
    "    T, N, F = len(pivoted_filled.index), len(tickers), len(feature_cols)\n",
    "    X = pivoted_filled.values.reshape(T, N, F)\n",
    "    M = mask_df.values.reshape(T, N, F).astype(np.int8)\n",
    "\n",
    "    cluster_tensors, cluster_masks, cluster_dates = [], [], []\n",
    "    for i in range(LOOKBACK, T):\n",
    "        cluster_tensors.append(X[i-LOOKBACK:i])\n",
    "        cluster_masks.append(M[i-LOOKBACK:i])\n",
    "        cluster_dates.append(pivoted_filled.index[i])  # ngày cuối của window\n",
    "\n",
    "    if cluster_tensors:\n",
    "        X_arr = np.array(cluster_tensors, dtype=np.float16)  # tiết kiệm RAM\n",
    "        M_arr = np.array(cluster_masks, dtype=np.int8)\n",
    "\n",
    "        tensor_file = f\"cluster_{c_id}_tensor.npy\"\n",
    "        mask_file   = f\"cluster_{c_id}_mask.npy\"\n",
    "        np.save(os.path.join(DATA_DIR, tensor_file), X_arr)\n",
    "        np.save(os.path.join(DATA_DIR, mask_file), M_arr)\n",
    "\n",
    "        tensor_index.append({\n",
    "            \"cluster\": int(c_id),\n",
    "            \"tickers\": tickers,\n",
    "            \"dates\": [str(d) for d in cluster_dates],   # ngày T\n",
    "            \"dates_shifted\": [str(d+pd.Timedelta(days=1)) for d in cluster_dates],  # T+1 (cho reward)\n",
    "            \"tensor_file\": tensor_file,\n",
    "            \"mask_file\": mask_file\n",
    "        })\n",
    "\n",
    "        print(f\"Cluster {c_id}: tensor {X_arr.shape}, mask {M_arr.shape} saved.\")\n",
    "\n",
    "    del g_feat, pivoted, pivoted_filled, mask_df, X, M, cluster_tensors, cluster_masks\n",
    "    gc.collect()\n",
    "\n",
    "# Save metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"w\") as f:\n",
    "    json.dump(tensor_index, f, indent=2)\n",
    "\n",
    "print(\"✅ Done Block 7: tensors + masks saved for all clusters.\")\n",
    "\n",
    "# Xoá df_features giảm RAM\n",
    "del df_features\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95417aff",
   "metadata": {},
   "source": [
    "**Block 7.5: Chuẩn bị dữ liệu backtest(loại bỏ các cột dữ liệu không cần thiết nữa)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf1ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done Block 7.5: df_backtest sẵn sàng cho reward.\n",
      "Kích thước df_backtest: (256151, 3)\n",
      "Tickers unique: 391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 7.5 — Chuẩn bị dữ liệu backtest cho reward thật\n",
    "import gc\n",
    "\n",
    "# Chỉ giữ dữ liệu cần thiết để tính reward (close price)\n",
    "# df_price có từ Block 1 (OHLCV đầy đủ)\n",
    "df_backtest = df_price[[\"ticker\", \"timestamp\", \"close\"]].copy()\n",
    "\n",
    "# Ép timestamp về dạng datetime để đồng bộ\n",
    "df_backtest[\"timestamp\"] = pd.to_datetime(df_backtest[\"timestamp\"])\n",
    "\n",
    "print(\"✅ Done Block 7.5: df_backtest sẵn sàng cho reward.\")\n",
    "print(\"Kích thước df_backtest:\", df_backtest.shape)\n",
    "print(\"Tickers unique:\", df_backtest[\"ticker\"].nunique())\n",
    "\n",
    "# Xóa những biến không còn cần để tiết kiệm RAM\n",
    "del df_price\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7aa4e",
   "metadata": {},
   "source": [
    "**Block 8: Huấn luyện A3C theo từng cụm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8 — A3C multi-stock per-cluster \n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "SIG_DIR  = \"./signals/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reset signals file\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Loss ---\n",
    "def a3c_loss(logits, values, actions, rewards, beta=0.01):\n",
    "    adv = rewards - values.squeeze(-1)\n",
    "    critic = adv.pow(2).mean()\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    actor = -(logp.gather(1, actions.unsqueeze(1)).squeeze(1) * adv.detach()).mean()\n",
    "    entropy = -(torch.softmax(logits, dim=-1) * logp).sum(-1).mean()\n",
    "    return actor + 0.5*critic - beta*entropy\n",
    "\n",
    "# --- Training & inference ---\n",
    "def process_cluster(meta, epochs=3, lr=1e-3, batch_size=256):\n",
    "    c_id, tickers, dates, dates_shifted = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"], meta[\"dates_shifted\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # --- Lấy giá để tính reward ---\n",
    "    px = []\n",
    "    for tk in tickers:\n",
    "        s = df_backtest[df_backtest[\"ticker\"]==tk].set_index(\"timestamp\")[\"close\"]\n",
    "        s = s.reindex(dates_shifted).ffill().bfill().values  # dùng dates_shifted\n",
    "        px.append(s)\n",
    "    px = np.stack(px, axis=1)  # shape (B, N)\n",
    "    r = np.zeros_like(px, dtype=np.float32)\n",
    "    r[1:] = np.log(px[1:] / np.maximum(px[:-1], 1e-9))  # daily log-return\n",
    "\n",
    "    # --- Model + optimizer ---\n",
    "    model = A3CNet(F).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Mini-batch generator\n",
    "    total = B*N\n",
    "    def iterator():\n",
    "        for start in range(0, total, batch_size):\n",
    "            end = min(total, start+batch_size)\n",
    "            xb, mb, rb, idx = [], [], [], []\n",
    "            for s in range(start,end):\n",
    "                b, n = divmod(s, N)\n",
    "                xb.append(X[b,:,n,:])\n",
    "                mb.append(M[b,:,n,:])\n",
    "                rb.append(r[b,n])\n",
    "                idx.append((b,n))\n",
    "            yield np.stack(xb), np.stack(mb), np.array(rb), idx\n",
    "\n",
    "    # --- Train ---\n",
    "    for ep in range(epochs):\n",
    "        loss_ep = 0\n",
    "        for xb, mb, rb, _ in iterator():\n",
    "            xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "            rb = torch.tensor(rb, dtype=torch.float32).to(device)\n",
    "            # mask vào input (loại bỏ chỗ NaN fill)\n",
    "            xb = xb * torch.tensor(mb, dtype=torch.float32).to(device)\n",
    "\n",
    "            logits, vals = model(xb)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            act = dist.sample()\n",
    "            # reward theo action\n",
    "            reward = torch.where(act==2, rb, torch.where(act==0, -rb, torch.zeros_like(rb)))\n",
    "            loss = a3c_loss(logits, vals, act, reward)\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            loss_ep += loss.item()\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Loss={loss_ep:.4f}\")\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Save model ---\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"  ✅ Saved model checkpoint: {model_path}\")\n",
    "\n",
    "    # --- Inference & save signals ---\n",
    "    with open(SIG_FILE,\"a\",newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for xb, mb, _, idx in iterator():\n",
    "                xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "                xb = xb * torch.tensor(mb, dtype=torch.float32).to(device)\n",
    "                acts = torch.argmax(model(xb)[0], dim=-1).cpu().numpy() - 1  # (-1,0,1)\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    w.writerow([dates[b], tickers[n], int(acts[k])])\n",
    "                del xb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, M, px, r, model, opt\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run all clusters ---\n",
    "for meta in tensor_index:\n",
    "    process_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 8: signals saved to {SIG_FILE}, models in {MODEL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418f9a1",
   "metadata": {},
   "source": [
    "**Block 9: Suy luận từ mô hình A3C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9 — Inference từ checkpoint A3C\n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "SIG_DIR   = \"./signals/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reset signals file\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model định nghĩa lại (giống Block 8) ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Inference function ---\n",
    "def infer_cluster(meta, batch_size=256):\n",
    "    c_id, tickers, dates, dates_shifted = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"], meta[\"dates_shifted\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"[Inference] Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # Load model checkpoint\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️ Model checkpoint not found: {model_path}, skip\")\n",
    "        return\n",
    "    model = A3CNet(F).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Inference & save signals\n",
    "    total = B * N\n",
    "    with open(SIG_FILE, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, total, batch_size):\n",
    "                end = min(total, start+batch_size)\n",
    "                xb, mb, idx = [], [], []\n",
    "                for s in range(start, end):\n",
    "                    b, n = divmod(s, N)\n",
    "                    xb.append(X[b, :, n, :])\n",
    "                    mb.append(M[b, :, n, :])\n",
    "                    idx.append((b, n))\n",
    "                xb = torch.tensor(np.stack(xb), dtype=torch.float32).to(device)\n",
    "                mb = torch.tensor(np.stack(mb), dtype=torch.float32).to(device)\n",
    "\n",
    "                # Áp dụng mask\n",
    "                xb = xb * mb\n",
    "\n",
    "                acts = torch.argmax(model(xb)[0], dim=-1).cpu().numpy() - 1  # (-1,0,1)\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    # dùng dates[b] (ngày cuối window), nhưng reward tính T+1 (dates_shifted)\n",
    "                    w.writerow([dates[b], tickers[n], int(acts[k])])\n",
    "                del xb, mb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, M, model\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run inference all clusters ---\n",
    "for meta in tensor_index:\n",
    "    infer_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 9: inference signals saved to {SIG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f8af",
   "metadata": {},
   "source": [
    "**Block 10 : Huấn luyện Cluster DDPG (chỉ với trường hợp vị thế long)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10 — Cluster DDPG + Execution Lag + Turnover Cost \n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from FiinQuantX import FiinSession\n",
    "\n",
    "# ===== Paths =====\n",
    "DATA_DIR   = \"./tensors/\"\n",
    "SIG_DIR    = \"./signals/\"\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== Hyper-params & config =====\n",
    "INIT_CAPITAL   = 10_000\n",
    "BENCHMARK_TKR  = \"VNINDEX\"\n",
    "EXECUTION_LAG  = 1         # tín hiệu T -> return T+1\n",
    "COST_BPS       = 30        # phí theo turnover 0.30% = 30bps\n",
    "STATE_LKBK     = 5         # số ngày lịch sử dùng làm state\n",
    "MIN_NAMES_PER_CLUSTER = 2  # tối thiểu số mã active trong 1 cụm\n",
    "SEED = 42\n",
    "\n",
    "# DDPG \n",
    "EPOCHS       = 8\n",
    "BATCH_SIZE   = 64\n",
    "LR_ACTOR     = 1e-4\n",
    "LR_CRITIC    = 5e-4\n",
    "GAMMA        = 0.99\n",
    "TAU          = 5e-3\n",
    "NOISE_STD    = 0.03   # nhiễu nhẹ trên logits\n",
    "HIDDEN       = 96\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ===== Load artifacts từ các block trước =====\n",
    "# (1) A3C signals\n",
    "signals = pd.read_csv(os.path.join(SIG_DIR, \"a3c_signals_infer.csv\"))\n",
    "signals[\"date\"] = pd.to_datetime(signals[\"date\"])\n",
    "\n",
    "# (2) Giá để tính return\n",
    "df_px = df_backtest.rename(columns={\"timestamp\": \"date\"}).copy()\n",
    "df_px[\"date\"] = pd.to_datetime(df_px[\"date\"])\n",
    "px_wide = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "ret_wide = px_wide.pct_change().fillna(0.0)\n",
    "\n",
    "# (3) Map ticker -> cluster (đảm bảo tương thích)\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    for tk in meta[\"tickers\"]:\n",
    "        # nếu 1 mã gặp nhiều cụm theo thời gian, giữ cụm đầu tiên để đơn giản\n",
    "        ticker2cluster.setdefault(tk, meta[\"cluster\"])\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "# ===== Train/Test split =====\n",
    "TRAIN_START = pd.Timestamp(\"2023-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")\n",
    "TEST_END    = ret_wide.index.max()\n",
    "\n",
    "# ===== Chuẩn bị signal với execution lag =====\n",
    "sig_wide_raw = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\").sort_index()\n",
    "idx_all = ret_wide.index.union(sig_wide_raw.index)\n",
    "ret_wide = ret_wide.reindex(idx_all).fillna(0.0)\n",
    "sig_wide = sig_wide_raw.reindex(idx_all).fillna(0.0)\n",
    "sig_wide_lag = sig_wide.shift(EXECUTION_LAG)  # <- execution lag\n",
    "\n",
    "# chỉ giữ tickers có trong map cụm & có return\n",
    "tickers = [t for t in ret_wide.columns if t in ticker2cluster.index]\n",
    "ret_wide = ret_wide[tickers].astype(\"float32\")\n",
    "sig_wide_lag = sig_wide_lag[tickers].astype(\"float32\")\n",
    "cluster_of = ticker2cluster.loc[tickers]\n",
    "\n",
    "# ===== Danh sách cụm và thành viên =====\n",
    "clusters = sorted(cluster_of.unique().tolist())\n",
    "cluster_members = {c: cluster_of[cluster_of == c].index.tolist() for c in clusters}\n",
    "C = len(clusters)\n",
    "\n",
    "# ===== Helpers =====\n",
    "def build_state_arrays(ret_w, sig_lag, start, end, K=STATE_LKBK):\n",
    "    \"\"\"\n",
    "    Xây state ở cấp cụm (long-only):\n",
    "      - activity[c,t] = tỷ lệ mã trong cụm c có signal>0 tại ngày t\n",
    "      - cluster_ret[c,t] = mean return của các mã active (signal>0) trong cụm c tại ngày t\n",
    "    Trả ra:\n",
    "      S_mat: (T, 2*C*K)  -> [activity(K ngày), cluster_ret(K ngày)]\n",
    "      R_mat: (T, C)      -> return cụm tại ngày t (dùng để tính reward)\n",
    "      dates: index tương ứng\n",
    "      ACTIVE_masks: dict[c] -> DataFrame mask active của cụm c trên khoảng thời gian này\n",
    "    \"\"\"\n",
    "    R = ret_w.loc[start:end]\n",
    "    S = sig_lag.loc[start:end]\n",
    "    dates = R.index\n",
    "\n",
    "    # tính activity và return cụm từng ngày, theo cụm\n",
    "    act_cols, ret_cols = [], []\n",
    "    ACTIVE_masks = {}\n",
    "\n",
    "    for c in clusters:\n",
    "        tks = cluster_members[c]\n",
    "        if not tks:\n",
    "            # tạo cột 0 nếu cụm rỗng (hiếm)\n",
    "            act_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ACTIVE_masks[c] = pd.DataFrame(0.0, index=dates, columns=tks)\n",
    "        else:\n",
    "            S_c = S[tks]\n",
    "            R_c = R[tks]\n",
    "\n",
    "            active_mask = (S_c > 0).astype(\"float32\")  # long-only\n",
    "            ACTIVE_masks[c] = active_mask\n",
    "\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0)  # chia đều trong các mã active\n",
    "            w = w.fillna(0.0)\n",
    "\n",
    "            act_c = (active_mask.mean(axis=1)).astype(\"float32\")  # % mã active\n",
    "            ret_c = ((R_c * w).sum(axis=1)).astype(\"float32\")     # mean ret của mã active\n",
    "\n",
    "        act_cols.append(act_c.rename(c))\n",
    "        ret_cols.append(ret_c.rename(c))\n",
    "\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "\n",
    "    # dựng state = concat K ngày gần nhất cho activity & cluster_ret\n",
    "    def stack_lookback(df, K):\n",
    "        mats = []\n",
    "        for k in range(K):\n",
    "            mats.append(df.shift(k).fillna(0.0))\n",
    "        return np.concatenate([m.values[:, :, None] for m in mats], axis=2)  # (T, C, K)\n",
    "\n",
    "    A3 = stack_lookback(act_df, K)    # (T,C,K)\n",
    "    R3 = stack_lookback(cret_df, K)   # (T,C,K)\n",
    "\n",
    "    # Loại bỏ những ngày đầu chưa đủ lookback\n",
    "    valid = np.arange(A3.shape[0]) >= (K - 1)\n",
    "    A3 = A3[valid]   # (T',C,K)\n",
    "    R3_look = R3[valid]\n",
    "    dates2 = dates[valid]\n",
    "\n",
    "    # Flatten state: (T', 2*C*K)\n",
    "    S_mat = np.concatenate([A3.reshape(len(dates2), -1), R3_look.reshape(len(dates2), -1)], axis=1).astype(\"float32\")\n",
    "    # Reward dùng return cụm (không lookback): lấy cret_df tại ngày tương ứng\n",
    "    R_mat = cret_df.loc[dates2].values.astype(\"float32\")\n",
    "\n",
    "    # đồng bộ ACTIVE_masks về dates2\n",
    "    ACTIVE_masks = {c: ACTIVE_masks[c].loc[dates2] for c in clusters}\n",
    "    return S_mat, R_mat, dates2, ACTIVE_masks\n",
    "\n",
    "# ===== Chuẩn bị Train & Test =====\n",
    "S_train, R_train, d_train, ACTIVE_train = build_state_arrays(ret_wide, sig_wide_lag, TRAIN_START, TRAIN_END, K=STATE_LKBK)\n",
    "S_test,  R_test,  d_test,  ACTIVE_test  = build_state_arrays(ret_wide, sig_wide_lag, TEST_START,  TEST_END,  K=STATE_LKBK)\n",
    "\n",
    "# ===== DDPG (long-only simplex) =====\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, a_dim)  # logits\n",
    "        )\n",
    "    def forward(self, s):\n",
    "        return torch.softmax(self.net(s), dim=-1)  # long-only, sum=1\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s, a], dim=-1))\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, maxlen=20000):\n",
    "        self.maxlen = maxlen; self.buf=[]\n",
    "    def push(self, s,a,r,s2):\n",
    "        if len(self.buf)>=self.maxlen: self.buf.pop(0)\n",
    "        self.buf.append((s,a,r,s2))\n",
    "    def sample(self, bs):\n",
    "        n=min(bs,len(self.buf))\n",
    "        idx=np.random.choice(len(self.buf), n, replace=False)\n",
    "        s,a,r,s2 = zip(*[self.buf[i] for i in idx])\n",
    "        return (np.array(s, np.float32), np.array(a, np.float32),\n",
    "                np.array(r, np.float32).reshape(-1,1), np.array(s2, np.float32))\n",
    "\n",
    "def step_soft_update(src, tgt, tau):\n",
    "    with torch.no_grad():\n",
    "        for p, tp in zip(src.parameters(), tgt.parameters()):\n",
    "            tp.data.mul_(1-tau); tp.data.add_(tau*p.data)\n",
    "\n",
    "def port_reward_longonly(w_c, r_c, prev_w_c=None):\n",
    "    gross = float(np.dot(w_c, r_c))\n",
    "    fee = 0.0\n",
    "    # phí ở cấp cụm đã được tính ở cấp mã trong mô phỏng cuối, nên ở training giữ đơn giản để ổn định\n",
    "    return gross - fee\n",
    "\n",
    "s_dim = S_train.shape[1]; a_dim = C\n",
    "actor = Actor(s_dim, a_dim).to(device)\n",
    "critic = Critic(s_dim, a_dim).to(device)\n",
    "t_actor = Actor(s_dim, a_dim).to(device); t_actor.load_state_dict(actor.state_dict())\n",
    "t_critic= Critic(s_dim, a_dim).to(device); t_critic.load_state_dict(critic.state_dict())\n",
    "optA = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "optC = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "mse = nn.MSELoss()\n",
    "buf = Buffer()\n",
    "\n",
    "# ---- Train (RAM-friendly) ----\n",
    "S_tr = S_train; R_tr = R_train\n",
    "prev_w = None\n",
    "for ep in range(EPOCHS):\n",
    "    c_loss=a_loss=0.0\n",
    "    prev_w = None\n",
    "    for t in range(len(S_tr)-1):\n",
    "        s  = torch.from_numpy(S_tr[t]).to(device).unsqueeze(0)\n",
    "        s2 = torch.from_numpy(S_tr[t+1]).to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            w = actor(s).cpu().numpy()[0]\n",
    "        # exploration trong simplex\n",
    "        logits = np.log(w + 1e-9) + np.random.normal(0, NOISE_STD, size=a_dim)\n",
    "        w_e = np.exp(logits); w_e = (w_e / w_e.sum()).astype(\"float32\")\n",
    "\n",
    "        r = port_reward_longonly(w_e, R_tr[t], prev_w)\n",
    "        prev_w = w_e.copy()\n",
    "        buf.push(S_tr[t], w_e, r, S_tr[t+1])\n",
    "\n",
    "        if len(buf.buf) >= BATCH_SIZE:\n",
    "            sb, ab, rb, s2b = buf.sample(BATCH_SIZE)\n",
    "            sb  = torch.from_numpy(sb).to(device)\n",
    "            ab  = torch.from_numpy(ab).to(device)\n",
    "            rb  = torch.from_numpy(rb).to(device)\n",
    "            s2b = torch.from_numpy(s2b).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                a2 = t_actor(s2b)\n",
    "                q2 = t_critic(s2b, a2)\n",
    "                y  = rb + GAMMA * q2\n",
    "\n",
    "            q  = critic(sb, ab)\n",
    "            lc = mse(q, y)\n",
    "            optC.zero_grad(); lc.backward(); optC.step()\n",
    "\n",
    "            ap = actor(sb)\n",
    "            la = -critic(sb, ap).mean()\n",
    "            optA.zero_grad(); la.backward(); optA.step()\n",
    "\n",
    "            step_soft_update(actor, t_actor, TAU)\n",
    "            step_soft_update(critic, t_critic, TAU)\n",
    "\n",
    "            c_loss += float(lc.item()); a_loss += float(la.item())\n",
    "    print(f\"[DDPG] Epoch {ep+1}/{EPOCHS} | Critic {c_loss:.4f} | Actor {a_loss:.4f}\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Backtest (Test) — long-only với phí theo turnover ở cấp mã =====\n",
    "dates = d_test\n",
    "capital = INIT_CAPITAL\n",
    "portfolio_value = pd.Series(index=dates, dtype=\"float64\")\n",
    "portfolio_value.iloc[0] = capital\n",
    "\n",
    "# stream save cluster weights để không tốn RAM\n",
    "cw_path = os.path.join(OUTPUT_DIR, \"cluster_weights_test.csv\")\n",
    "with open(cw_path, \"w\", newline=\"\") as f:\n",
    "    cw = csv.writer(f); cw.writerow([\"date\"] + [f\"cluster_{c}\" for c in clusters])\n",
    "\n",
    "prev_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "\n",
    "for i, dt in enumerate(dates):\n",
    "    # dùng state của ngày trước (action lag 1 step để chắc chắn không leak)\n",
    "    s_dt = dates[i-1] if i>0 else dates[i]\n",
    "    s = torch.from_numpy(S_test[i-1].astype(\"float32\") if i>0 else S_test[i].astype(\"float32\")).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        w_c = actor(s).cpu().numpy()[0].astype(\"float32\")\n",
    "    w_c = np.clip(w_c, 0, 1); ssum = w_c.sum(); w_c = w_c/ssum if ssum>0 else np.ones_like(w_c)/len(w_c)\n",
    "\n",
    "    # phân bổ xuống mã: trong mỗi cụm, chia đều cho mã active (signal>0)\n",
    "    w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "    for j, c in enumerate(clusters):\n",
    "        members = cluster_members[c]\n",
    "        if not members: continue\n",
    "        act_row = ACTIVE_test[c].iloc[i]  # mask của ngày dt\n",
    "        valid = [tk for tk in members if (tk in act_row.index and act_row[tk] > 0)]\n",
    "        if len(valid) >= MIN_NAMES_PER_CLUSTER:\n",
    "            share = w_c[j] / len(valid)\n",
    "            w_ticker.loc[valid] += share\n",
    "\n",
    "    # nếu không cụm nào active -> phân bổ đều toàn thị trường (đảm bảo sum=1)\n",
    "    ssum = float(w_ticker.sum())\n",
    "    if ssum <= 1e-12:\n",
    "        w_ticker[:] = 1.0 / len(w_ticker)\n",
    "    else:\n",
    "        w_ticker /= ssum\n",
    "\n",
    "    # lưu cluster weights (stream)\n",
    "    with open(cw_path, \"a\", newline=\"\") as f:\n",
    "        cw = csv.writer(f); cw.writerow([dt.strftime(\"%Y-%m-%d\")] + [float(x) for x in w_c])\n",
    "\n",
    "    # lợi nhuận ngày dt & phí turnover\n",
    "    r_vec = ret_wide.loc[dt, w_ticker.index].values.astype(\"float32\")\n",
    "    turnover = float(np.sum(np.abs(w_ticker.values - prev_w_ticker.values)))\n",
    "    fee = (COST_BPS/1e4) * turnover\n",
    "\n",
    "    r_net = float(np.dot(w_ticker.values, r_vec)) - fee\n",
    "    capital = capital * (1.0 + r_net)\n",
    "    portfolio_value.iloc[i] = capital\n",
    "\n",
    "    prev_w_ticker = w_ticker\n",
    "    if (i % 50) == 0:\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Benchmark (buy & hold VNINDEX) =====\n",
    "print(\"Fetching VNINDEX for benchmark (buy & hold)...\")\n",
    "client = FiinSession(username=\"DSTC_18@fiinquant.vn\", password=\"Fiinquant0606\").login()\n",
    "bench = client.Fetch_Trading_Data(\n",
    "    realtime=False, tickers=BENCHMARK_TKR, fields=['close'],\n",
    "    adjusted=True, by=\"1d\", from_date=str(dates.min().date())\n",
    ").get_data()\n",
    "bench[\"date\"] = pd.to_datetime(bench[\"timestamp\"])\n",
    "bench = bench.set_index(\"date\")[\"close\"].sort_index().reindex(dates).ffill().bfill()\n",
    "bench_ret = bench.pct_change().fillna(0.0)\n",
    "benchmark_value = (1 + bench_ret).cumprod() * INIT_CAPITAL\n",
    "\n",
    "# ===== Save outputs (gọn) =====\n",
    "portfolio_value.to_frame(\"portfolio_value\").to_csv(os.path.join(OUTPUT_DIR, \"portfolio_value_test.csv\"))\n",
    "benchmark_value.to_frame(\"benchmark_value\").to_csv(os.path.join(OUTPUT_DIR, \"benchmark_value_test.csv\"))\n",
    "\n",
    "print(\"✅ Done Block 10 (long-only, lag, turnover, cluster-DDPG, RAM-optimized).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5b470",
   "metadata": {},
   "source": [
    "**Block 11: Thống kê kết quả và vẽ biểu đồ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14652a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11 — Performance Stats & Visualization (Test + Special Period)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # vẽ không cần GUI, tiết kiệm tài nguyên\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "STATS_FILE = os.path.join(OUTPUT_DIR, \"stats_test.csv\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    # trả về số âm (tỷ lệ sụt tối đa), ví dụ -0.25 = -25%\n",
    "    peak = series.cummax()\n",
    "    dd = (series / peak) - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def compute_stats(port_val: pd.Series, bench_val: pd.Series | None = None) -> dict:\n",
    "    # daily return từ equity để đảm bảo đồng nhất\n",
    "    port_ret = port_val.pct_change().fillna(0.0)\n",
    "\n",
    "    stats = {\n",
    "        \"Start\": port_val.index.min().strftime(\"%Y-%m-%d\"),\n",
    "        \"End\": port_val.index.max().strftime(\"%Y-%m-%d\"),\n",
    "        \"Final Value\": float(port_val.iloc[-1]),\n",
    "        \"ROI (%)\": float((port_val.iloc[-1] / port_val.iloc[0] - 1.0) * 100.0),\n",
    "        \"Volatility (ann %)\": float(port_ret.std() * np.sqrt(252) * 100.0) if port_ret.std() > 0 else 0.0,\n",
    "        \"Sharpe\": float((port_ret.mean() / port_ret.std()) * np.sqrt(252)) if port_ret.std() > 0 else 0.0,\n",
    "        \"Sortino\": float((port_ret.mean() / port_ret[port_ret < 0].std()) * np.sqrt(252)) if port_ret[port_ret < 0].std() > 0 else 0.0,\n",
    "        \"Max Drawdown (%)\": float(max_drawdown(port_val) * 100.0),\n",
    "        \"Hit Rate (%)\": float((port_ret > 0).mean() * 100.0),\n",
    "        \"Days\": int(len(port_ret))\n",
    "    }\n",
    "    if bench_val is not None and len(bench_val) > 1:\n",
    "        bench_ret = bench_val.pct_change().fillna(0.0)\n",
    "        stats.update({\n",
    "            \"Benchmark Final\": float(bench_val.iloc[-1]),\n",
    "            \"Benchmark ROI (%)\": float((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100.0),\n",
    "            \"Excess Return (pp)\": float(stats[\"ROI (%)\"] - ((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100.0))\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "def plot_equity(port_val: pd.Series, bench_val: pd.Series | None, title: str, path: str):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(port_val.index, port_val.values, label=\"Portfolio\", linewidth=1.6)\n",
    "    if bench_val is not None:\n",
    "        plt.plot(bench_val.index, bench_val.values, label=\"VNINDEX (Buy&Hold)\", linewidth=1.2)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_hist(port_ret: pd.Series, title: str, path: str):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.hist(port_ret.dropna(), bins=50, alpha=0.8, color=\"#1f77b4\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Daily Return\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- Load results từ Block 10 ----------\n",
    "# Test equity (DDPG)\n",
    "port_val = pd.read_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"portfolio_value_test.csv\"),\n",
    "    index_col=0, parse_dates=True\n",
    ").iloc[:, 0]\n",
    "port_val = port_val.sort_index()\n",
    "\n",
    "# Benchmark equity (Buy & Hold VNINDEX)\n",
    "bench_val = pd.read_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"benchmark_value_test.csv\"),\n",
    "    index_col=0, parse_dates=True\n",
    ").iloc[:, 0]\n",
    "bench_val = bench_val.sort_index().reindex(port_val.index).ffill().bfill()\n",
    "\n",
    "# ---------- Stats (Test) ----------\n",
    "stats_test = compute_stats(port_val, bench_val)\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plot_equity(\n",
    "    port_val, bench_val,\n",
    "    \"Equity Curve (Test) — Portfolio vs VNINDEX\",\n",
    "    os.path.join(OUTPUT_DIR, \"equity_test.png\")\n",
    ")\n",
    "plot_hist(\n",
    "    port_val.pct_change().fillna(0.0),\n",
    "    \"Daily Return Histogram (Test)\",\n",
    "    os.path.join(OUTPUT_DIR, \"hist_test.png\")\n",
    ")\n",
    "\n",
    "# ---------- Special Period Analysis ----------\n",
    "special_start, special_end = pd.Timestamp(\"2025-03-26\"), pd.Timestamp(\"2025-04-15\")\n",
    "sub_port = port_val.loc[special_start:special_end]\n",
    "sub_bench = bench_val.loc[special_start:special_end]\n",
    "\n",
    "stats_special = {}\n",
    "if len(sub_port) > 1:\n",
    "    stats_special = compute_stats(sub_port, sub_bench)\n",
    "    plot_equity(\n",
    "        sub_port, sub_bench,\n",
    "        f\"Equity Curve — Special Period ({special_start.date()} → {special_end.date()})\",\n",
    "        os.path.join(OUTPUT_DIR, \"equity_special.png\")\n",
    "    )\n",
    "    plot_hist(\n",
    "        sub_port.pct_change().fillna(0.0),\n",
    "        f\"Daily Return Histogram — Special Period ({special_start.date()} → {special_end.date()})\",\n",
    "        os.path.join(OUTPUT_DIR, \"hist_special.png\")\n",
    "    )\n",
    "\n",
    "# ---------- Save all stats ----------\n",
    "all_stats = {\"test\": stats_test}\n",
    "if stats_special:\n",
    "    all_stats[\"special\"] = stats_special\n",
    "\n",
    "pd.DataFrame(all_stats).T.to_csv(STATS_FILE)\n",
    "print(\"📊 Test Stats:\")\n",
    "print(pd.Series(stats_test))\n",
    "if stats_special:\n",
    "    print(\"\\n📊 Special Period Stats:\")\n",
    "    print(pd.Series(stats_special))\n",
    "\n",
    "print(f\"\\n✅ Block 12 done. Stats saved to: {STATS_FILE}\")\n",
    "print(f\"   Plots saved to: {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
