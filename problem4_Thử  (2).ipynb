{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4d6cad",
   "metadata": {},
   "source": [
    "**HƯỚNG DẪN CHẠY**\n",
    "\n",
    "*Nhóm chạy code theo thứ tự từng cell từ trên xuống xuống dưới*\n",
    "\n",
    "**Một số điểm lưu ý:**\n",
    "\n",
    "- *Thời gian chạy các block đa số lâu (Khoảng 10 phút riêng block 6 khoảng 25 phút)*\n",
    "\n",
    "- *Các block 8,9,10,11 có lưu kết quả file csv* \n",
    "\n",
    "- **Các file csv kết quả nhóm có upload lên github**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bbe97",
   "metadata": {},
   "source": [
    "**Tải các thư viện cần thiết** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch scikit-learn matplotlib\n",
    "!pip install --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx\n",
    "!pip install --upgrade --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565db80b",
   "metadata": {},
   "source": [
    "**Block 1: tải dữ liệu lịch sử và realtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a2bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mã HOSE: 413\n",
      "Fetching data, it may take a while. Please wait...\n",
      "History ban đầu:   ticker         timestamp      open      high       low     close     volume  \\\n",
      "0    AAA  2023-01-03 00:00  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA  2023-01-04 00:00  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA  2023-01-05 00:00  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA  2023-01-06 00:00  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA  2023-01-09 00:00  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs           fn  \n",
      "0  938600.0  504700.0   40579000.0  899404000.0  \n",
      "1  462900.0  780600.0  151639000.0   36850000.0  \n",
      "2  487200.0  473700.0  343911000.0  -59103000.0  \n",
      "3  564300.0  828300.0  345999000.0 -294312000.0  \n",
      "4  414000.0  631800.0  514557000.0 -483197000.0  \n"
     ]
    }
   ],
   "source": [
    "# Block 1 — Login & Lấy dữ liệu tất cả HOSE/HNX/UPCOM\n",
    "import pandas as pd\n",
    "from FiinQuantX import FiinSession, BarDataUpdate\n",
    "# --- Login ---\n",
    "username = \"DSTC_18@fiinquant.vn\"\n",
    "password = \"Fiinquant0606\"\n",
    "\n",
    "client = FiinSession(\n",
    "    username=username,\n",
    "    password=password\n",
    ").login()\n",
    "\n",
    "# --- Lấy danh sách cổ phiếu từng sàn ---\n",
    "tickers_hose  = list(client.TickerList(ticker=\"VNINDEX\"))     # HOSE\n",
    "print(f\"Số mã HOSE: {len(tickers_hose)}\")\n",
    "\n",
    "# --- Lấy dữ liệu lịch sử toàn bộ ---\n",
    "event_history = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2023-01-01\"   # Lấy dữ liệu từ 2023 tới nay\n",
    ")\n",
    "\n",
    "df_all = event_history.get_data()\n",
    "print(\"History ban đầu:\", df_all.head())\n",
    "\n",
    "# --- Callback realtime ---\n",
    "def onDataUpdate(data: BarDataUpdate):\n",
    "    global df_all\n",
    "    df_update = data.to_dataFrame()\n",
    "    df_all = pd.concat([df_all, df_update])\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    print(\"Realtime update:\")\n",
    "    print(df_update.head())\n",
    "\n",
    "# --- Bật realtime nối tiếp dữ liệu ---\n",
    "event_realtime = client.Fetch_Trading_Data(\n",
    "    realtime=True,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    period=1,\n",
    "    callback=onDataUpdate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1122224",
   "metadata": {},
   "source": [
    "**Block 2: lấy dữ liệu FA, lọc các mã không hợp lệ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c93b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mã VNINDEX tổng: 413\n",
      "Resume: đã phát hiện 3 ticker đã xử lý trong 'vnindex_fa_quarterly_vnstock.csv'.\n",
      "Skip CCC (đã có trong processed/master/invalid).\n",
      "Skip SBG (đã có trong processed/master/invalid).\n",
      "[FUCTVGF3] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEIP100] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[GMH] ✓ Lấy xong và lưu (rows=18).\n",
      "[FUEKIV30] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[NO1] ✓ Lấy xong và lưu (rows=17).\n",
      "[FUCTVGF4] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[RYG] ✓ Lấy xong và lưu (rows=7).\n",
      "[FUEDCMID] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEKIVFS] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEMAVND] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEFCV50] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEBFVND] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUCTVGF5] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEKIVND] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEABVND] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUETCC50] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUETPVND] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[AAA] ✓ Lấy xong và lưu (rows=50).\n",
      "[AAM] ✓ Lấy xong và lưu (rows=50).\n",
      "[AAT] ✓ Lấy xong và lưu (rows=28).\n",
      "[ABR] ✓ Lấy xong và lưu (rows=23).\n",
      "[ABS] ✓ Lấy xong và lưu (rows=26).\n",
      "[ABT] ✓ Lấy xong và lưu (rows=50).\n",
      "[ACB] ✓ Lấy xong và lưu (rows=51).\n",
      "[ACC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ACL] ✓ Lấy xong và lưu (rows=50).\n",
      "[ADP] ✓ Lấy xong và lưu (rows=35).\n",
      "[AGR] ✓ Lấy xong và lưu (rows=50).\n",
      "[AGG] ✓ Lấy xong và lưu (rows=25).\n",
      "[ACG] ✓ Lấy xong và lưu (rows=29).\n",
      "[ANV] ✓ Lấy xong và lưu (rows=52).\n",
      "[APG] ✓ Lấy xong và lưu (rows=50).\n",
      "[APH] ✓ Lấy xong và lưu (rows=22).\n",
      "[HII] ✓ Lấy xong và lưu (rows=34).\n",
      "[ASG] ✓ Lấy xong và lưu (rows=28).\n",
      "[ASM] ✓ Lấy xong và lưu (rows=50).\n",
      "[ASP] ✓ Lấy xong và lưu (rows=50).\n",
      "[BAF] ✓ Lấy xong và lưu (rows=17).\n",
      "[BBC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BCE] ✓ Lấy xong và lưu (rows=50).\n",
      "[BCG] ✓ Lấy xong và lưu (rows=39).\n",
      "[BFC] ✓ Lấy xong và lưu (rows=43).\n",
      "[BHN] ✓ Lấy xong và lưu (rows=37).\n",
      "[BIC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BID] ✓ Lấy xong và lưu (rows=50).\n",
      "[BCM] ✓ Lấy xong và lưu (rows=30).\n",
      "[DBD] ✓ Lấy xong và lưu (rows=42).\n",
      "[BWE] ✓ Lấy xong và lưu (rows=35).\n",
      "[BKG] ✓ Lấy xong và lưu (rows=21).\n",
      "[BMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BMI] ✓ Lấy xong và lưu (rows=50).\n",
      "[BMP] ✓ Lấy xong và lưu (rows=50).\n",
      "[BRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BSI] ✓ Lấy xong và lưu (rows=50).\n",
      "[BSR] ✓ Lấy xong và lưu (rows=31).\n",
      "[BTP] ✓ Lấy xong và lưu (rows=50).\n",
      "[BTT] ✓ Lấy xong và lưu (rows=50).\n",
      "[BVH] ✓ Lấy xong và lưu (rows=50).\n",
      "[TNH] ✓ Lấy xong và lưu (rows=22).\n",
      "[C32] ✓ Lấy xong và lưu (rows=50).\n",
      "[C47] ✓ Lấy xong và lưu (rows=50).\n",
      "[CCI] ✓ Lấy xong và lưu (rows=50).\n",
      "[CCL] ✓ Lấy xong và lưu (rows=50).\n",
      "[CDC] ✓ Lấy xong và lưu (rows=52).\n",
      "[CRE] ✓ Lấy xong và lưu (rows=30).\n",
      "[STK] ✓ Lấy xong và lưu (rows=43).\n",
      "[CHP] ✓ Lấy xong và lưu (rows=50).\n",
      "[CIG] ✓ Lấy xong và lưu (rows=50).\n",
      "[CII] ✓ Lấy xong và lưu (rows=50).\n",
      "[CKG] ⚠️ Rate limit detected (attempt 1/6). Đợi 502.2s rồi thử lại. Message: Failed to fetch data: 502 - Bad Gateway\n",
      "[CKG] ✓ Lấy xong và lưu (rows=38).\n",
      "[CLC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ADG] ✓ Lấy xong và lưu (rows=20).\n",
      "[CLL] ✓ Lấy xong và lưu (rows=50).\n",
      "[CLW] ✓ Lấy xong và lưu (rows=50).\n",
      "[CMG] ✓ Lấy xong và lưu (rows=50).\n",
      "[CMV] ✓ Lấy xong và lưu (rows=50).\n",
      "[CMX] ✓ Lấy xong và lưu (rows=50).\n",
      "[CNG] ✓ Lấy xong và lưu (rows=50).\n",
      "[COM] ✓ Lấy xong và lưu (rows=50).\n",
      "[CRC] ✓ Lấy xong và lưu (rows=34).\n",
      "[CSM] ✓ Lấy xong và lưu (rows=50).\n",
      "[CSV] ✓ Lấy xong và lưu (rows=44).\n",
      "[CTD] ✓ Lấy xong và lưu (rows=50).\n",
      "[CTF] ✓ Lấy xong và lưu (rows=35).\n",
      "[CTG] ✓ Lấy xong và lưu (rows=53).\n",
      "[CTI] ✓ Lấy xong và lưu (rows=50).\n",
      "[ICT] ✓ Lấy xong và lưu (rows=31).\n",
      "[CTR] ✓ Lấy xong và lưu (rows=50).\n",
      "[CTS] ✓ Lấy xong và lưu (rows=50).\n",
      "[CVT] ✓ Lấy xong và lưu (rows=50).\n",
      "[D2D] ✓ Lấy xong và lưu (rows=50).\n",
      "[DAH] ✓ Lấy xong và lưu (rows=37).\n",
      "[ADS] ✓ Lấy xong và lưu (rows=38).\n",
      "[DPG] ✓ Lấy xong và lưu (rows=31).\n",
      "[DBC] ✓ Lấy xong và lưu (rows=51).\n",
      "[DBT] ✓ Lấy xong và lưu (rows=50).\n",
      "[DC4] ✓ Lấy xong và lưu (rows=50).\n",
      "[DCL] ✓ Lấy xong và lưu (rows=50).\n",
      "[DCM] ✓ Lấy xong và lưu (rows=42).\n",
      "[DGC] ✓ Lấy xong và lưu (rows=47).\n",
      "[DGW] ✓ Lấy xong và lưu (rows=42).\n",
      "[DHA] ✓ Lấy xong và lưu (rows=50).\n",
      "[DHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[DHG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DHM] ✓ Lấy xong và lưu (rows=50).\n",
      "[TTE] ✓ Lấy xong và lưu (rows=30).\n",
      "[DIG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DLG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DMC] ✓ Lấy xong và lưu (rows=52).\n",
      "[DSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[DSE] ✓ Lấy xong và lưu (rows=52).\n",
      "[DPM] ✓ Lấy xong và lưu (rows=50).\n",
      "[DPR] ✓ Lấy xong và lưu (rows=50).\n",
      "[DQC] ✓ Lấy xong và lưu (rows=50).\n",
      "[DRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[DRH] ✓ Lấy xong và lưu (rows=50).\n",
      "[DRL] ✓ Lấy xong và lưu (rows=50).\n",
      "[DSN] ✓ Lấy xong và lưu (rows=50).\n",
      "[DTA] ✓ Lấy xong và lưu (rows=50).\n",
      "[DTL] ✓ Lấy xong và lưu (rows=50).\n",
      "[DTT] ✓ Lấy xong và lưu (rows=50).\n",
      "[DVP] ✓ Lấy xong và lưu (rows=50).\n",
      "[DXG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DXS] ✓ Lấy xong và lưu (rows=18).\n",
      "[DXV] ✓ Lấy xong và lưu (rows=50).\n",
      "[FUESSV50] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[E1VFVN30] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[EIB] ✓ Lấy xong và lưu (rows=50).\n",
      "[ELC] ✓ Lấy xong và lưu (rows=50).\n",
      "[FUESSVFL] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[EVE] ✓ Lấy xong và lưu (rows=50).\n",
      "[EVG] ✓ Lấy xong và lưu (rows=35).\n",
      "[EVF] ✓ Lấy xong và lưu (rows=46).\n",
      "[FCM] ✓ Lấy xong và lưu (rows=50).\n",
      "[FCN] ✓ Lấy xong và lưu (rows=50).\n",
      "[FDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[FIR] ✓ Lấy xong và lưu (rows=31).\n",
      "[FIT] ✓ Lấy xong và lưu (rows=50).\n",
      "[FMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[FPT] ✓ Lấy xong và lưu (rows=52).\n",
      "[FRT] ✓ Lấy xong và lưu (rows=33).\n",
      "[FTS] ✓ Lấy xong và lưu (rows=50).\n",
      "[FUEMAV30] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUESSV30] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEVFVND] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[FUEVN100] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[GAS] ✓ Lấy xong và lưu (rows=50).\n",
      "[GDT] ✓ Lấy xong và lưu (rows=50).\n",
      "[GEX] ✓ Lấy xong và lưu (rows=50).\n",
      "[GEE] ✓ Lấy xong và lưu (rows=15).\n",
      "[PGV] ✓ Lấy xong và lưu (rows=34).\n",
      "[GIL] ✓ Lấy xong và lưu (rows=50).\n",
      "[GEG] ✓ Lấy xong và lưu (rows=42).\n",
      "[GMD] ✓ Lấy xong và lưu (rows=50).\n",
      "[GSP] ✓ Lấy xong và lưu (rows=50).\n",
      "[GTA] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAG] ✓ Lấy xong và lưu (rows=54).\n",
      "[HAH] ✓ Lấy xong và lưu (rows=44).\n",
      "[HPX] ✓ Lấy xong và lưu (rows=30).\n",
      "[HID] ✓ Lấy xong và lưu (rows=39).\n",
      "[HHV] ✓ Lấy xong và lưu (rows=23).\n",
      "[HAP] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAR] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAS] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAX] ✓ Lấy xong và lưu (rows=51).\n",
      "[HCD] ✓ Lấy xong và lưu (rows=38).\n",
      "[HCM] ✓ Lấy xong và lưu (rows=52).\n",
      "[HDB] ✓ Lấy xong và lưu (rows=45).\n",
      "[HDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HDG] ✓ Lấy xong và lưu (rows=52).\n",
      "[HHP] ✓ Lấy xong và lưu (rows=28).\n",
      "[HHS] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCH] ✓ Lấy xong và lưu (rows=39).\n",
      "[HSL] ✓ Lấy xong và lưu (rows=31).\n",
      "[HMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HNA] ✓ Lấy xong và lưu (rows=39).\n",
      "[HPG] ✓ Lấy xong và lưu (rows=50).\n",
      "[HQC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HSG] ✓ Lấy xong và lưu (rows=50).\n",
      "[HT1] ✓ Lấy xong và lưu (rows=50).\n",
      "[HTG] ✓ Lấy xong và lưu (rows=42).\n",
      "[HTI] ✓ Lấy xong và lưu (rows=50).\n",
      "[HTN] ✓ Lấy xong và lưu (rows=30).\n",
      "[HTL] ✓ Lấy xong và lưu (rows=50).\n",
      "[HTV] ✓ Lấy xong và lưu (rows=49).\n",
      "[HU1] ✓ Lấy xong và lưu (rows=50).\n",
      "[HVH] ✓ Lấy xong và lưu (rows=30).\n",
      "[HVX] ✓ Lấy xong và lưu (rows=50).\n",
      "[ILB] ✓ Lấy xong và lưu (rows=42).\n",
      "[IDI] ✓ Lấy xong và lưu (rows=50).\n",
      "[IJC] ✓ Lấy xong và lưu (rows=50).\n",
      "[IMP] ✓ Lấy xong và lưu (rows=50).\n",
      "[ITC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ITD] ✓ Lấy xong và lưu (rows=50).\n",
      "[JVC] ✓ Lấy xong và lưu (rows=50).\n",
      "[KBC] ✓ Lấy xong và lưu (rows=50).\n",
      "[KDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[KDH] ✓ Lấy xong và lưu (rows=50).\n",
      "[KHG] ✓ Lấy xong và lưu (rows=18).\n",
      "[KHP] ✓ Lấy xong và lưu (rows=50).\n",
      "[KMR] ✓ Lấy xong và lưu (rows=50).\n",
      "[KOS] ✓ Lấy xong và lưu (rows=32).\n",
      "[KSB] ✓ Lấy xong và lưu (rows=50).\n",
      "[L10] ✓ Lấy xong và lưu (rows=50).\n",
      "[LAF] ✓ Lấy xong và lưu (rows=50).\n",
      "[LBM] ✓ Lấy xong và lưu (rows=51).\n",
      "[LCG] ✓ Lấy xong và lưu (rows=51).\n",
      "[LDG] ✓ Lấy xong và lưu (rows=42).\n",
      "[LGC] ✓ Lấy xong và lưu (rows=50).\n",
      "[LGL] ✓ Lấy xong và lưu (rows=50).\n",
      "[LHG] ✓ Lấy xong và lưu (rows=50).\n",
      "[LIX] ✓ Lấy xong và lưu (rows=50).\n",
      "[LM8] ✓ Lấy xong và lưu (rows=50).\n",
      "[LSS] ✓ Lấy xong và lưu (rows=50).\n",
      "[LPB] ✓ Lấy xong và lưu (rows=47).\n",
      "[MBB] ✓ Lấy xong và lưu (rows=56).\n",
      "[MCM] ✓ Lấy xong và lưu (rows=22).\n",
      "[MCP] ✓ Lấy xong và lưu (rows=50).\n",
      "[MDG] ✓ Lấy xong và lưu (rows=50).\n",
      "[MHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[MIG] ✓ Lấy xong và lưu (rows=42).\n",
      "[MSB] ✓ Lấy xong và lưu (rows=39).\n",
      "[MSN] ✓ Lấy xong và lưu (rows=51).\n",
      "[MWG] ✓ Lấy xong và lưu (rows=46).\n",
      "[NAB] ✓ Lấy xong và lưu (rows=50).\n",
      "[NAF] ✓ Lấy xong và lưu (rows=41).\n",
      "[NAV] ✓ Lấy xong và lưu (rows=50).\n",
      "[NBB] ✓ Lấy xong và lưu (rows=50).\n",
      "[NCT] ✓ Lấy xong và lưu (rows=45).\n",
      "[NHA] ✓ Lấy xong và lưu (rows=50).\n",
      "[NHH] ✓ Lấy xong và lưu (rows=26).\n",
      "[VHM] ✓ Lấy xong và lưu (rows=36).\n",
      "[NHT] ✓ Lấy xong và lưu (rows=27).\n",
      "[NKG] ✓ Lấy xong và lưu (rows=50).\n",
      "[NLG] ✓ Lấy xong và lưu (rows=50).\n",
      "[NNC] ✓ Lấy xong và lưu (rows=50).\n",
      "[NVL] ✓ Lấy xong và lưu (rows=38).\n",
      "[NSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[NT2] ✓ Lấy xong và lưu (rows=50).\n",
      "[NTL] ✓ Lấy xong và lưu (rows=50).\n",
      "[NVT] ✓ Lấy xong và lưu (rows=50).\n",
      "[OCB] ✓ Lấy xong và lưu (rows=46).\n",
      "[OGC] ✓ Lấy xong và lưu (rows=50).\n",
      "[OPC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ORS] ✓ Lấy xong và lưu (rows=50).\n",
      "[PAC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PAN] ✓ Lấy xong và lưu (rows=50).\n",
      "[PC1] ✓ Lấy xong và lưu (rows=49).\n",
      "[PDN] ✓ Lấy xong và lưu (rows=50).\n",
      "[PDR] ✓ Lấy xong và lưu (rows=50).\n",
      "[PET] ✓ Lấy xong và lưu (rows=50).\n",
      "[PLX] ✓ Lấy xong và lưu (rows=47).\n",
      "[PGC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PGD] ✓ Lấy xong và lưu (rows=50).\n",
      "[PGI] ✓ Lấy xong và lưu (rows=50).\n",
      "[PLP] ✓ Lấy xong và lưu (rows=33).\n",
      "[PHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PHR] ✓ Lấy xong và lưu (rows=50).\n",
      "[PIT] ✓ Lấy xong và lưu (rows=50).\n",
      "[PJT] ✓ Lấy xong và lưu (rows=50).\n",
      "[PMG] ✓ Lấy xong và lưu (rows=32).\n",
      "[PNC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PNJ] ✓ Lấy xong và lưu (rows=50).\n",
      "[PPC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PTB] ✓ Lấy xong và lưu (rows=50).\n",
      "[PTC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PTL] ✓ Lấy xong và lưu (rows=50).\n",
      "[DAT] ✓ Lấy xong và lưu (rows=40).\n",
      "[PVD] ✓ Lấy xong và lưu (rows=50).\n",
      "[POW] ✓ Lấy xong và lưu (rows=34).\n",
      "[PVT] ✓ Lấy xong và lưu (rows=51).\n",
      "[PVP] ✓ Lấy xong và lưu (rows=49).\n",
      "[QCG] ✓ Lấy xong và lưu (rows=50).\n",
      "[QNP] ✓ Lấy xong và lưu (rows=34).\n",
      "[RAL] ✓ Lấy xong và lưu (rows=50).\n",
      "[REE] ✓ Lấy xong và lưu (rows=50).\n",
      "[SGN] ✓ Lấy xong và lưu (rows=42).\n",
      "[SAM] ✓ Lấy xong và lưu (rows=50).\n",
      "[SAV] ✓ Lấy xong và lưu (rows=50).\n",
      "[SBA] ✓ Lấy xong và lưu (rows=50).\n",
      "[SAB] ✓ Lấy xong và lưu (rows=48).\n",
      "[SBT] ✓ Lấy xong và lưu (rows=50).\n",
      "[SC5] ✓ Lấy xong và lưu (rows=50).\n",
      "[SCR] ✓ Lấy xong và lưu (rows=50).\n",
      "[SCS] ✓ Lấy xong và lưu (rows=43).\n",
      "[SSB] ✓ Lấy xong và lưu (rows=39).\n",
      "[SFC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SFG] ✓ Lấy xong và lưu (rows=49).\n",
      "[SFI] ✓ Lấy xong và lưu (rows=50).\n",
      "[SGR] ✓ Lấy xong và lưu (rows=38).\n",
      "[SGT] ✓ Lấy xong và lưu (rows=47).\n",
      "[SIP] ✓ Lấy xong và lưu (rows=26).\n",
      "[SHA] ✓ Lấy xong và lưu (rows=50).\n",
      "[SHB] ✓ Lấy xong và lưu (rows=50).\n",
      "[MSH] ✓ Lấy xong và lưu (rows=38).\n",
      "[SHI] ✓ Lấy xong và lưu (rows=50).\n",
      "[SHP] ✓ Lấy xong và lưu (rows=50).\n",
      "[SBV] ✓ Lấy xong và lưu (rows=35).\n",
      "[SZC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SJD] ✓ Lấy xong và lưu (rows=50).\n",
      "[SJS] ✓ Lấy xong và lưu (rows=51).\n",
      "[SKG] ✓ Lấy xong và lưu (rows=48).\n",
      "[SMA] ✓ Lấy xong và lưu (rows=50).\n",
      "[SMB] ✓ Lấy xong và lưu (rows=49).\n",
      "[SMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SPM] ✓ Lấy xong và lưu (rows=50).\n",
      "[SRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SRF] ✓ Lấy xong và lưu (rows=50).\n",
      "[S4A] ✓ Lấy xong và lưu (rows=42).\n",
      "[SSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SSI] ✓ Lấy xong và lưu (rows=50).\n",
      "[ST8] ✓ Lấy xong và lưu (rows=50).\n",
      "[STB] ✓ Lấy xong và lưu (rows=50).\n",
      "[STG] ✓ Lấy xong và lưu (rows=50).\n",
      "[SVC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SVD] ✓ Lấy xong và lưu (rows=21).\n",
      "[SVI] ✓ Lấy xong và lưu (rows=50).\n",
      "[SVT] ✓ Lấy xong và lưu (rows=50).\n",
      "[SZL] ✓ Lấy xong và lưu (rows=50).\n",
      "[AST] ✓ Lấy xong và lưu (rows=33).\n",
      "[TAL] ✓ Lấy xong và lưu (rows=8).\n",
      "[TBC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCB] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCL] ✓ Lấy xong và lưu (rows=49).\n",
      "[TCM] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCO] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCR] ✓ Lấy xong và lưu (rows=50).\n",
      "[FUCVREIT] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: Mã chứng khoán không hợp lệ. Chỉ cổ phiếu mới có thông tin.\n",
      "[TCI] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCT] ✓ Lấy xong và lưu (rows=50).\n",
      "[TDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TDG] ✓ Lấy xong và lưu (rows=35).\n",
      "[TDH] ✓ Lấy xong và lưu (rows=50).\n",
      "[TDM] ✓ Lấy xong và lưu (rows=38).\n",
      "[TDP] ✓ Lấy xong và lưu (rows=30).\n",
      "[TDW] ✓ Lấy xong và lưu (rows=50).\n",
      "[TEG] ✓ Lấy xong và lưu (rows=39).\n",
      "[THG] ✓ Lấy xong và lưu (rows=50).\n",
      "[TIP] ✓ Lấy xong và lưu (rows=50).\n",
      "[TIX] ✓ Lấy xong và lưu (rows=50).\n",
      "[TLD] ✓ Lấy xong và lưu (rows=32).\n",
      "[TLG] ✓ Lấy xong và lưu (rows=50).\n",
      "[TLH] ✓ Lấy xong và lưu (rows=50).\n",
      "[TMP] ✓ Lấy xong và lưu (rows=50).\n",
      "[TMS] ✓ Lấy xong và lưu (rows=50).\n",
      "[TMT] ✓ Lấy xong và lưu (rows=50).\n",
      "[TN1] ✓ Lấy xong và lưu (rows=29).\n",
      "[TNC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TNI] ✓ Lấy xong và lưu (rows=38).\n",
      "[TNT] ✓ Lấy xong và lưu (rows=50).\n",
      "[TPB] ✓ Lấy xong và lưu (rows=42).\n",
      "[TPC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TRA] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCD] ✓ Lấy xong và lưu (rows=35).\n",
      "[TRC] ✓ Lấy xong và lưu (rows=51).\n",
      "[TVB] ✓ Lấy xong và lưu (rows=50).\n",
      "[TSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TTA] ✓ Lấy xong và lưu (rows=21).\n",
      "[TTF] ✓ Lấy xong và lưu (rows=50).\n",
      "[HUB] ✓ Lấy xong và lưu (rows=37).\n",
      "[TV2] ✓ Lấy xong và lưu (rows=50).\n",
      "[TVS] ✓ Lấy xong và lưu (rows=50).\n",
      "[TYA] ✓ Lấy xong và lưu (rows=50).\n",
      "[UIC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VAB] ✓ Lấy xong và lưu (rows=41).\n",
      "[VAF] ✓ Lấy xong và lưu (rows=48).\n",
      "[VCA] ✓ Lấy xong và lưu (rows=50).\n",
      "[VCB] ✓ Lấy xong và lưu (rows=50).\n",
      "[VCF] ✓ Lấy xong và lưu (rows=50).\n",
      "[VCG] ✓ Lấy xong và lưu (rows=52).\n",
      "[VCI] ✓ Lấy xong và lưu (rows=50).\n",
      "[VDS] ✓ Lấy xong và lưu (rows=50).\n",
      "[VFG] ✓ Lấy xong và lưu (rows=50).\n",
      "[VGC] ✓ Lấy xong và lưu (rows=42).\n",
      "[VHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VIB] ✓ Lấy xong và lưu (rows=50).\n",
      "[VIC] ✓ Lấy xong và lưu (rows=51).\n",
      "[TVT] ✓ Lấy xong và lưu (rows=45).\n",
      "[VID] ✓ Lấy xong và lưu (rows=50).\n",
      "[VDP] ✓ Lấy xong và lưu (rows=35).\n",
      "[VJC] ✓ Lấy xong và lưu (rows=35).\n",
      "[VIP] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPS] ✓ Lấy xong và lưu (rows=42).\n",
      "[VIX] ✓ Lấy xong và lưu (rows=58).\n",
      "[VMD] ✓ Lấy xong và lưu (rows=49).\n",
      "[HVN] ✓ Lấy xong và lưu (rows=42).\n",
      "[VND] ✓ Lấy xong và lưu (rows=51).\n",
      "[VNE] ✓ Lấy xong và lưu (rows=50).\n",
      "[VNG] ✓ Lấy xong và lưu (rows=50).\n",
      "[VNL] ✓ Lấy xong và lưu (rows=50).\n",
      "[VNM] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPD] ✓ Lấy xong và lưu (rows=43).\n",
      "[GVR] ✓ Lấy xong và lưu (rows=28).\n",
      "[VNS] ✓ Lấy xong và lưu (rows=50).\n",
      "[VOS] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPB] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPG] ✓ Lấy xong và lưu (rows=33).\n",
      "[VPH] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPL] ✓ Lấy xong và lưu (rows=22).\n",
      "[VPI] ✓ Lấy xong và lưu (rows=32).\n",
      "[VRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VRE] ✓ Lấy xong và lưu (rows=33).\n",
      "[VSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VSH] ✓ Lấy xong và lưu (rows=50).\n",
      "[VSI] ✓ Lấy xong và lưu (rows=50).\n",
      "[VTB] ✓ Lấy xong và lưu (rows=51).\n",
      "[VTO] ✓ Lấy xong và lưu (rows=50).\n",
      "[VTP] ✓ Lấy xong và lưu (rows=37).\n",
      "[YBM] ✓ Lấy xong và lưu (rows=30).\n",
      "[YEG] ✓ Lấy xong và lưu (rows=31).\n",
      "=== Hoàn tất Block 2 (vnstock) ===\n",
      "Tổng tickers xử lý thành công: 389\n",
      "Tổng tickers bị đánh dấu INVALID và bỏ qua: 22\n",
      "Tổng tickers thất bại (retry hết nhưng không invalid): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Meta</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Chỉ tiêu cơ cấu nguồn vốn</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Chỉ tiêu hiệu quả hoạt động</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"9\" halign=\"left\">Chỉ tiêu định giá</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>yearReport</th>\n",
       "      <th>lengthReport</th>\n",
       "      <th>(ST+LT borrowings)/Equity</th>\n",
       "      <th>Debt/Equity</th>\n",
       "      <th>Fixed Asset-To-Equity</th>\n",
       "      <th>Owners' Equity/Charter Capital</th>\n",
       "      <th>Asset Turnover</th>\n",
       "      <th>Fixed Asset Turnover</th>\n",
       "      <th>Days Sales Outstanding</th>\n",
       "      <th>...</th>\n",
       "      <th>Market Capital (Bn. VND)</th>\n",
       "      <th>Outstanding Share (Mil. Shares)</th>\n",
       "      <th>P/E</th>\n",
       "      <th>P/B</th>\n",
       "      <th>P/S</th>\n",
       "      <th>P/Cash Flow</th>\n",
       "      <th>EPS (VND)</th>\n",
       "      <th>BVPS (VND)</th>\n",
       "      <th>EV/EBITDA</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051588</td>\n",
       "      <td>0.124650</td>\n",
       "      <td>1.090538</td>\n",
       "      <td>0.562538</td>\n",
       "      <td>4.422492</td>\n",
       "      <td>63.351078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452000e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>11.480752</td>\n",
       "      <td>0.806941</td>\n",
       "      <td>1.362896</td>\n",
       "      <td>3.795689</td>\n",
       "      <td>356.816813</td>\n",
       "      <td>10905.381178</td>\n",
       "      <td>11.204889</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022473</td>\n",
       "      <td>0.051838</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>1.089856</td>\n",
       "      <td>0.495824</td>\n",
       "      <td>3.693597</td>\n",
       "      <td>71.229634</td>\n",
       "      <td>...</td>\n",
       "      <td>1.311750e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>16.681350</td>\n",
       "      <td>0.729454</td>\n",
       "      <td>1.406508</td>\n",
       "      <td>3.940808</td>\n",
       "      <td>121.619694</td>\n",
       "      <td>10898.564366</td>\n",
       "      <td>20.561323</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038005</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>1.077694</td>\n",
       "      <td>0.461186</td>\n",
       "      <td>3.248087</td>\n",
       "      <td>69.708499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.229250e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>20.639945</td>\n",
       "      <td>0.691291</td>\n",
       "      <td>1.395969</td>\n",
       "      <td>10.732768</td>\n",
       "      <td>120.501797</td>\n",
       "      <td>10776.944672</td>\n",
       "      <td>19.378872</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.106987</td>\n",
       "      <td>0.146877</td>\n",
       "      <td>1.065434</td>\n",
       "      <td>0.456444</td>\n",
       "      <td>3.178684</td>\n",
       "      <td>66.693418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313400e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>21.487381</td>\n",
       "      <td>0.747113</td>\n",
       "      <td>1.491061</td>\n",
       "      <td>7.664694</td>\n",
       "      <td>167.561987</td>\n",
       "      <td>10654.338632</td>\n",
       "      <td>31.526449</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.063503</td>\n",
       "      <td>0.156407</td>\n",
       "      <td>1.048888</td>\n",
       "      <td>0.438959</td>\n",
       "      <td>2.972644</td>\n",
       "      <td>64.985075</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366200e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>19.196756</td>\n",
       "      <td>0.789407</td>\n",
       "      <td>1.585955</td>\n",
       "      <td>11.360902</td>\n",
       "      <td>66.896642</td>\n",
       "      <td>10488.880887</td>\n",
       "      <td>18.004344</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.032987</td>\n",
       "      <td>0.150374</td>\n",
       "      <td>1.142198</td>\n",
       "      <td>0.473768</td>\n",
       "      <td>3.133843</td>\n",
       "      <td>60.882722</td>\n",
       "      <td>...</td>\n",
       "      <td>1.485000e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>16.702805</td>\n",
       "      <td>0.787954</td>\n",
       "      <td>1.568495</td>\n",
       "      <td>14.300980</td>\n",
       "      <td>5.990141</td>\n",
       "      <td>11421.984245</td>\n",
       "      <td>15.236970</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047413</td>\n",
       "      <td>0.157043</td>\n",
       "      <td>1.142922</td>\n",
       "      <td>0.572311</td>\n",
       "      <td>3.624814</td>\n",
       "      <td>53.195105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658250e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>12.102004</td>\n",
       "      <td>0.880344</td>\n",
       "      <td>1.460188</td>\n",
       "      <td>-104.265001</td>\n",
       "      <td>130.001223</td>\n",
       "      <td>11415.994104</td>\n",
       "      <td>12.309960</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>0.115587</td>\n",
       "      <td>0.165952</td>\n",
       "      <td>1.128155</td>\n",
       "      <td>0.637397</td>\n",
       "      <td>4.058792</td>\n",
       "      <td>50.114334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600500e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>9.127335</td>\n",
       "      <td>0.859811</td>\n",
       "      <td>1.234861</td>\n",
       "      <td>59.962717</td>\n",
       "      <td>228.434875</td>\n",
       "      <td>11281.546389</td>\n",
       "      <td>9.337824</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.175449</td>\n",
       "      <td>1.105311</td>\n",
       "      <td>0.682272</td>\n",
       "      <td>4.248544</td>\n",
       "      <td>47.441126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592250e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>8.101948</td>\n",
       "      <td>0.873057</td>\n",
       "      <td>1.143669</td>\n",
       "      <td>24.127825</td>\n",
       "      <td>174.405400</td>\n",
       "      <td>11053.111514</td>\n",
       "      <td>9.445047</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043288</td>\n",
       "      <td>0.101775</td>\n",
       "      <td>0.169373</td>\n",
       "      <td>1.137871</td>\n",
       "      <td>0.724659</td>\n",
       "      <td>4.420403</td>\n",
       "      <td>40.665490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.666500e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>6.932984</td>\n",
       "      <td>0.887623</td>\n",
       "      <td>1.121519</td>\n",
       "      <td>12.311260</td>\n",
       "      <td>293.153010</td>\n",
       "      <td>11378.706114</td>\n",
       "      <td>7.042915</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Meta                         Chỉ tiêu cơ cấu nguồn vốn              \\\n",
       "  ticker yearReport lengthReport (ST+LT borrowings)/Equity Debt/Equity   \n",
       "0    GMH       2025            2                  0.000000    0.051588   \n",
       "1    GMH       2025            1                  0.022473    0.051838   \n",
       "2    GMH       2024            4                  0.000000    0.038005   \n",
       "3    GMH       2024            3                  0.038418    0.106987   \n",
       "4    GMH       2024            2                  0.015499    0.063503   \n",
       "5    GMH       2024            1                  0.009567    0.032987   \n",
       "6    GMH       2023            4                  0.000000    0.047413   \n",
       "7    GMH       2023            3                  0.040551    0.115587   \n",
       "8    GMH       2023            2                  0.000000    0.088011   \n",
       "9    GMH       2023            1                  0.043288    0.101775   \n",
       "\n",
       "                                                        \\\n",
       "  Fixed Asset-To-Equity Owners' Equity/Charter Capital   \n",
       "0              0.124650                       1.090538   \n",
       "1              0.130708                       1.089856   \n",
       "2              0.138373                       1.077694   \n",
       "3              0.146877                       1.065434   \n",
       "4              0.156407                       1.048888   \n",
       "5              0.150374                       1.142198   \n",
       "6              0.157043                       1.142922   \n",
       "7              0.165952                       1.128155   \n",
       "8              0.175449                       1.105311   \n",
       "9              0.169373                       1.137871   \n",
       "\n",
       "  Chỉ tiêu hiệu quả hoạt động                                              \\\n",
       "               Asset Turnover Fixed Asset Turnover Days Sales Outstanding   \n",
       "0                    0.562538             4.422492              63.351078   \n",
       "1                    0.495824             3.693597              71.229634   \n",
       "2                    0.461186             3.248087              69.708499   \n",
       "3                    0.456444             3.178684              66.693418   \n",
       "4                    0.438959             2.972644              64.985075   \n",
       "5                    0.473768             3.133843              60.882722   \n",
       "6                    0.572311             3.624814              53.195105   \n",
       "7                    0.637397             4.058792              50.114334   \n",
       "8                    0.682272             4.248544              47.441126   \n",
       "9                    0.724659             4.420403              40.665490   \n",
       "\n",
       "   ...        Chỉ tiêu định giá                                             \\\n",
       "   ... Market Capital (Bn. VND) Outstanding Share (Mil. Shares)        P/E   \n",
       "0  ...             1.452000e+11                      16500000.0  11.480752   \n",
       "1  ...             1.311750e+11                      16500000.0  16.681350   \n",
       "2  ...             1.229250e+11                      16500000.0  20.639945   \n",
       "3  ...             1.313400e+11                      16500000.0  21.487381   \n",
       "4  ...             1.366200e+11                      16500000.0  19.196756   \n",
       "5  ...             1.485000e+11                      16500000.0  16.702805   \n",
       "6  ...             1.658250e+11                      16500000.0  12.102004   \n",
       "7  ...             1.600500e+11                      16500000.0   9.127335   \n",
       "8  ...             1.592250e+11                      16500000.0   8.101948   \n",
       "9  ...             1.666500e+11                      16500000.0   6.932984   \n",
       "\n",
       "                                                                       ticker  \n",
       "        P/B       P/S P/Cash Flow   EPS (VND)    BVPS (VND)  EV/EBITDA         \n",
       "0  0.806941  1.362896    3.795689  356.816813  10905.381178  11.204889    GMH  \n",
       "1  0.729454  1.406508    3.940808  121.619694  10898.564366  20.561323    GMH  \n",
       "2  0.691291  1.395969   10.732768  120.501797  10776.944672  19.378872    GMH  \n",
       "3  0.747113  1.491061    7.664694  167.561987  10654.338632  31.526449    GMH  \n",
       "4  0.789407  1.585955   11.360902   66.896642  10488.880887  18.004344    GMH  \n",
       "5  0.787954  1.568495   14.300980    5.990141  11421.984245  15.236970    GMH  \n",
       "6  0.880344  1.460188 -104.265001  130.001223  11415.994104  12.309960    GMH  \n",
       "7  0.859811  1.234861   59.962717  228.434875  11281.546389   9.337824    GMH  \n",
       "8  0.873057  1.143669   24.127825  174.405400  11053.111514   9.445047    GMH  \n",
       "9  0.887623  1.121519   12.311260  293.153010  11378.706114   7.042915    GMH  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 2 — Lấy dữ liệu FA theo quý (VNINDEX) với retry + backoff, skip ticker không hợp lệ và lưu incremental\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from vnstock import Finance\n",
    "\n",
    "# --- Dùng tickers_hose từ Block 1 nếu có, nếu không lấy lại từ client ---\n",
    "try:\n",
    "    tickers_vnindex = tickers_hose\n",
    "except NameError:\n",
    "    tickers_vnindex = list(client.TickerList(ticker=\"VNINDEX\"))\n",
    "\n",
    "print(f\"Số mã VNINDEX tổng: {len(tickers_vnindex)}\")\n",
    "\n",
    "# --- Output files ---\n",
    "master_file = \"vnindex_fa_quarterly_vnstock.csv\"\n",
    "per_ticker_dir = \"vnindex_fa_by_ticker\"\n",
    "invalid_file = \"vnindex_invalid_tickers.txt\"\n",
    "os.makedirs(per_ticker_dir, exist_ok=True)\n",
    "\n",
    "# --- Retry / backoff parameters ---\n",
    "MAX_RETRIES = 6\n",
    "BASE_DELAY = 5\n",
    "BACKOFF_BASE = 2.0\n",
    "JITTER = 1.0\n",
    "RATE_LIMIT_PATTERNS = [\n",
    "    r\"rate limit exceeded\",\n",
    "    r\"you have sent too many requests\",\n",
    "    r\"too many requests\",\n",
    "    r\"rate limit\",\n",
    "    r\"vci.*rate\",\n",
    "]\n",
    "INVALID_TICKER_PATTERNS = [\n",
    "    r\"mã chứng khoán không hợp lệ\",\n",
    "    r\"chỉ cổ phiếu mới có thông tin\",\n",
    "    r\"không phải mã cổ phiếu\",\n",
    "    r\"invalid symbol\",\n",
    "    r\"symbol is invalid\",\n",
    "]\n",
    "\n",
    "def parse_wait_seconds_from_msg(msg):\n",
    "    if not msg:\n",
    "        return None\n",
    "    m = re.search(r\"after\\s+(\\d+)\\s*seconds?\", msg, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m2 = re.search(r\"after\\s+(\\d+)\\s*s\\b\", msg, flags=re.IGNORECASE)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "    m3 = re.search(r\"(\\d+)\", msg)\n",
    "    if m3:\n",
    "        return int(m3.group(1))\n",
    "    return None\n",
    "\n",
    "# --- Resume: đọc danh sách tickers đã xử lý trong master file nếu có ---\n",
    "processed = set()\n",
    "if os.path.exists(master_file):\n",
    "    try:\n",
    "        df_exist = pd.read_csv(master_file, usecols=['ticker'])\n",
    "        processed.update(df_exist['ticker'].astype(str).unique().tolist())\n",
    "        print(f\"Resume: đã phát hiện {len(processed)} ticker đã xử lý trong '{master_file}'.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Resume: đọc danh sách ticker invalid đã lưu trước đó ---\n",
    "invalid_set = set()\n",
    "if os.path.exists(invalid_file):\n",
    "    try:\n",
    "        with open(invalid_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    invalid_set.add(s)\n",
    "        processed.update(invalid_set)  # coi invalid như đã xử lý để skip lần sau\n",
    "        print(f\"Resume: {len(invalid_set)} ticker đã được đánh dấu INVALID và sẽ bị bỏ qua.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Thu thập dữ liệu ---\n",
    "fa_list = []\n",
    "successful = []\n",
    "skipped_invalid = []\n",
    "failed = []\n",
    "\n",
    "for t in tickers_vnindex:\n",
    "    if t in processed:\n",
    "        print(f\"Skip {t} (đã có trong processed/master/invalid).\")\n",
    "        continue\n",
    "\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    last_exception_msg = None\n",
    "\n",
    "    while attempt < MAX_RETRIES and not success:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            finance = Finance(symbol=t, source='VCI')\n",
    "            df = finance.ratio(period='quarterly')\n",
    "            df = pd.DataFrame(df) if df is not None else pd.DataFrame()\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"[{t}] ⚠️ Không có dữ liệu FA (vnstock). Bỏ qua (không lưu).\")\n",
    "                # không ghi master nếu trống; đánh dấu là đã thử (không add vào processed để có thể thử lại sau nếu muốn)\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            # Đảm bảo có cột ticker\n",
    "            if 'ticker' not in df.columns:\n",
    "                df['ticker'] = t\n",
    "            else:\n",
    "                df['ticker'] = df['ticker'].fillna(t)\n",
    "\n",
    "            # Nếu có ReportDate (meta) thì drop đi\n",
    "            if 'ReportDate' in df.columns:\n",
    "                df = df.drop(columns=['ReportDate'])\n",
    "\n",
    "            # Coerce numeric cho các cột (nếu có thể)\n",
    "            for c in df.columns:\n",
    "                if c != 'ticker':\n",
    "                    df[c] = pd.to_numeric(df[c], errors='ignore')\n",
    "\n",
    "            # --- Lưu per-ticker CSV ---\n",
    "            per_file = os.path.join(per_ticker_dir, f\"{t}_fa.csv\")\n",
    "            df.to_csv(per_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "            # --- Append vào master CSV ---\n",
    "            write_header = not os.path.exists(master_file)\n",
    "            df.to_csv(master_file, mode='a', header=write_header, index=False, encoding='utf-8-sig')\n",
    "\n",
    "            print(f\"[{t}] ✓ Lấy xong và lưu (rows={len(df)}).\")\n",
    "            fa_list.append(df)\n",
    "            successful.append(t)\n",
    "            processed.add(t)  # mark processed so resume will skip next time\n",
    "            success = True\n",
    "\n",
    "            # nhẹ nhàng sleep giữa requests\n",
    "            time.sleep(BASE_DELAY + random.uniform(0, JITTER))\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            last_exception_msg = msg\n",
    "            lower_msg = msg.lower()\n",
    "\n",
    "            # --- Nếu lỗi do ticker không hợp lệ -> bỏ luôn, không retry ---\n",
    "            is_invalid = any(re.search(pat, lower_msg) for pat in INVALID_TICKER_PATTERNS)\n",
    "            if is_invalid:\n",
    "                print(f\"[{t}] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: {msg}\")\n",
    "                skipped_invalid.append(t)\n",
    "                processed.add(t)\n",
    "                # lưu vào file invalid để lần sau không thử lại\n",
    "                try:\n",
    "                    with open(invalid_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(t + \"\\n\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                break\n",
    "\n",
    "            # --- Nếu nghi ngờ rate-limit hoặc server báo wait time ---\n",
    "            is_rate_limit = any(pat in lower_msg for pat in RATE_LIMIT_PATTERNS)\n",
    "            parsed_wait = parse_wait_seconds_from_msg(msg)\n",
    "\n",
    "            if is_rate_limit or parsed_wait:\n",
    "                wait_sec = parsed_wait if parsed_wait and parsed_wait > 0 else (BACKOFF_BASE ** attempt)\n",
    "                wait_sec = wait_sec + random.uniform(0, JITTER)\n",
    "                print(f\"[{t}] ⚠️ Rate limit detected (attempt {attempt}/{MAX_RETRIES}). \"\n",
    "                      f\"Đợi {wait_sec:.1f}s rồi thử lại. Message: {msg}\")\n",
    "                time.sleep(wait_sec)\n",
    "                continue\n",
    "            else:\n",
    "                backoff = (BACKOFF_BASE ** attempt) + random.uniform(0, JITTER)\n",
    "                print(f\"[{t}] ⚠️ Lỗi khi fetch (attempt {attempt}/{MAX_RETRIES}): {msg}. \"\n",
    "                      f\"Đợi {backoff:.1f}s rồi thử lại.\")\n",
    "                time.sleep(backoff)\n",
    "                continue\n",
    "\n",
    "    # nếu vòng retry kết thúc mà không success và không thuộc invalid -> mark failed\n",
    "    if not success and t not in skipped_invalid:\n",
    "        print(f\"[{t}] ❌ Không lấy được dữ liệu sau {MAX_RETRIES} lần. Bỏ qua ticker này. Lỗi cuối: {last_exception_msg}\")\n",
    "        failed.append(t)\n",
    "\n",
    "# --- Kết quả tổng kết ---\n",
    "print(\"=== Hoàn tất Block 2 (vnstock) ===\")\n",
    "print(f\"Tổng tickers xử lý thành công: {len(successful)}\")\n",
    "print(f\"Tổng tickers bị đánh dấu INVALID và bỏ qua: {len(skipped_invalid)}\")\n",
    "print(f\"Tổng tickers thất bại (retry hết nhưng không invalid): {len(failed)}\")\n",
    "\n",
    "if fa_list:\n",
    "    sample_df = pd.concat(fa_list, ignore_index=True, sort=False).head(10)\n",
    "    display(sample_df)\n",
    "else:\n",
    "    print(\"❗ Không có dữ liệu FA thu được cho các ticker đã chạy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9ef83",
   "metadata": {},
   "source": [
    "**Block 3: Chuẩn hóa FA và gộp dữ liệu với giá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831a3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc dữ liệu FA từ 391 ticker, tổng 17586 dòng\n",
      "✅ Đã lưu dữ liệu merged giá + FA vào: vnindex_price_fa_merged.csv\n",
      "Sample merged:\n",
      "  ticker  timestamp      open      high       low     close     volume  \\\n",
      "0    AAA 2023-01-03  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA 2023-01-04  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA 2023-01-05  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA 2023-01-06  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA 2023-01-09  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs           fn  fa_year  fa_quarter  \\\n",
      "0  938600.0  504700.0   40579000.0  899404000.0     2022           4   \n",
      "1  462900.0  780600.0  151639000.0   36850000.0     2022           4   \n",
      "2  487200.0  473700.0  343911000.0  -59103000.0     2022           4   \n",
      "3  564300.0  828300.0  345999000.0 -294312000.0     2022           4   \n",
      "4  414000.0  631800.0  514557000.0 -483197000.0     2022           4   \n",
      "\n",
      "   Debt/Equity  Net Profit Margin (%)        P/E       P/B  \n",
      "0     0.749394              -0.028688  21.391591  0.637137  \n",
      "1     0.749394              -0.028688  21.391591  0.637137  \n",
      "2     0.749394              -0.028688  21.391591  0.637137  \n",
      "3     0.749394              -0.028688  21.391591  0.637137  \n",
      "4     0.749394              -0.028688  21.391591  0.637137  \n",
      "Số mã merge thành công: 391\n"
     ]
    }
   ],
   "source": [
    "# Block 3 — Chuẩn hoá FA (từ file per-ticker có 2 dòng header) + Merge với giá và lưu CSV\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- Các cột FA cần lấy ---\n",
    "fa_fields = [\"Debt/Equity\", \"Net Profit Margin (%)\", \"P/E\", \"P/B\"]\n",
    "need_cols = [\"ticker\",\"yearReport\",\"lengthReport\"] + fa_fields\n",
    "\n",
    "# --- Đọc toàn bộ FA từ thư mục ---\n",
    "folder = \"vnindex_fa_by_ticker\"\n",
    "fa_list = []\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\"_fa.csv\"):\n",
    "        path = os.path.join(folder, file)\n",
    "        try:\n",
    "            # ⚡ Bỏ dòng header đầu (Meta...), lấy dòng thứ 2 làm header\n",
    "            df = pd.read_csv(path, header=1)\n",
    "\n",
    "            # Đảm bảo đủ cột cần thiết\n",
    "            for col in need_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "\n",
    "            df = df[need_cols]\n",
    "            fa_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Lỗi đọc {file}: {e}\")\n",
    "\n",
    "# --- Gộp toàn bộ FA ---\n",
    "fa_data = pd.concat(fa_list, ignore_index=True)\n",
    "print(f\"Đọc dữ liệu FA từ {len(fa_list)} ticker, tổng {len(fa_data)} dòng\")\n",
    "\n",
    "# --- Chuẩn hoá giá (df_all từ Block 1) ---\n",
    "df_price = df_all[df_all[\"ticker\"].isin(fa_data[\"ticker\"].unique())].copy()\n",
    "df_price[\"timestamp\"] = pd.to_datetime(df_price[\"timestamp\"])\n",
    "df_price = df_price.sort_values([\"ticker\",\"timestamp\"])\n",
    "\n",
    "# tạo key (fa_year, fa_quarter) = quý trước để tránh data leak\n",
    "pi = df_price[\"timestamp\"].dt.to_period(\"Q\")\n",
    "prev_pi = pi - 1\n",
    "df_price[\"fa_year\"] = prev_pi.dt.year.astype(int)\n",
    "df_price[\"fa_quarter\"] = prev_pi.dt.quarter.astype(int)\n",
    "\n",
    "# --- Chuẩn hoá FA ---\n",
    "fa_clean = fa_data.rename(columns={\n",
    "    \"yearReport\": \"fa_year\",\n",
    "    \"lengthReport\": \"fa_quarter\"\n",
    "})\n",
    "# ép kiểu int cho chắc\n",
    "fa_clean[\"fa_year\"] = fa_clean[\"fa_year\"].astype(\"Int64\")\n",
    "fa_clean[\"fa_quarter\"] = fa_clean[\"fa_quarter\"].astype(\"Int64\")\n",
    "\n",
    "fa_clean = (\n",
    "    fa_clean.sort_values([\"ticker\",\"fa_year\",\"fa_quarter\"])\n",
    "            .drop_duplicates(subset=[\"ticker\",\"fa_year\",\"fa_quarter\"], keep=\"last\")\n",
    ")\n",
    "\n",
    "# --- Merge giá + FA ---\n",
    "df_merged = df_price.merge(\n",
    "    fa_clean,\n",
    "    on=[\"ticker\",\"fa_year\",\"fa_quarter\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# FFill để lấp chỗ trống\n",
    "df_merged = df_merged.sort_values([\"ticker\",\"timestamp\"])\n",
    "df_merged[fa_fields] = df_merged.groupby(\"ticker\")[fa_fields].ffill()\n",
    "\n",
    "# --- Lưu ra CSV ---\n",
    "output_file = \"vnindex_price_fa_merged.csv\"\n",
    "df_merged.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Đã lưu dữ liệu merged giá + FA vào: {output_file}\")\n",
    "\n",
    "# --- Preview ---\n",
    "print(\"Sample merged:\")\n",
    "print(df_merged.head())\n",
    "print(\"Số mã merge thành công:\", df_merged['ticker'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08faae38",
   "metadata": {},
   "source": [
    "**Xóa biến df_all không cần thiết nữa để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331523dd",
   "metadata": {},
   "source": [
    "**Block 4: Tính các chỉ số TA dựa vào thư viện FiinQuant và ghép dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d43b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã lưu dữ liệu có TA (từng mã, KHÔNG regime) vào: vnindex_price_fa_ta.csv\n",
      "Sample with TA:\n",
      "  ticker  timestamp     close  ema_20  ema_50  macd  rsi\n",
      "0    AAA 2023-01-03  6866.145     NaN     NaN   NaN  NaN\n",
      "1    AAA 2023-01-04  6827.733     NaN     NaN   NaN  NaN\n",
      "2    AAA 2023-01-05  6885.351     NaN     NaN   NaN  NaN\n",
      "3    AAA 2023-01-06  6856.542     NaN     NaN   NaN  NaN\n",
      "4    AAA 2023-01-09  6789.321     NaN     NaN   NaN  NaN\n",
      "Shape sau khi thêm TA: (264721, 29)\n"
     ]
    }
   ],
   "source": [
    "# Block 4 — Tính các chỉ số TA cho từng mã (KHÔNG có regime)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load dữ liệu merge từ Block 3 ---\n",
    "df_merged = pd.read_csv(\"vnindex_price_fa_merged.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# --- Khởi tạo Indicator ---\n",
    "fi = client.FiinIndicator()\n",
    "\n",
    "# --- Hàm tính TA cho từng ticker ---\n",
    "def add_ta_indicators(df):\n",
    "    df = df.sort_values(\"timestamp\").copy().reset_index(drop=True)\n",
    "\n",
    "    # EMA\n",
    "    df['ema_5']  = fi.ema(df['close'], window=5)\n",
    "    df['ema_20'] = fi.ema(df['close'], window=20)\n",
    "    df['ema_50'] = fi.ema(df['close'], window=50)\n",
    "\n",
    "    # MACD\n",
    "    df['macd']        = fi.macd(df['close'], window_fast=12, window_slow=26)\n",
    "    df['macd_signal'] = fi.macd_signal(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "    df['macd_diff']   = fi.macd_diff(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "\n",
    "    # RSI\n",
    "    df['rsi'] = fi.rsi(df['close'], window=14)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df['bollinger_hband'] = fi.bollinger_hband(df['close'], window=20, window_dev=2)\n",
    "    df['bollinger_lband'] = fi.bollinger_lband(df['close'], window=20, window_dev=2)\n",
    "\n",
    "    # ATR\n",
    "    df['atr'] = fi.atr(df['high'], df['low'], df['close'], window=14)\n",
    "\n",
    "    # OBV\n",
    "    df['obv'] = fi.obv(df['close'], df['volume'])\n",
    "\n",
    "    # VWAP\n",
    "    df['vwap'] = fi.vwap(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Áp dụng TA cho toàn bộ df_merged ---\n",
    "df_with_ta = df_merged.groupby(\"ticker\", group_keys=False).apply(add_ta_indicators)\n",
    "\n",
    "# --- Lưu ra CSV (không còn regime nữa) ---\n",
    "output_file = \"vnindex_price_fa_ta.csv\"\n",
    "df_with_ta.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Đã lưu dữ liệu có TA (từng mã, KHÔNG regime) vào: {output_file}\")\n",
    "\n",
    "# --- Preview ---\n",
    "print(\"Sample with TA:\")\n",
    "print(df_with_ta[['ticker','timestamp','close','ema_20','ema_50','macd','rsi']].head())\n",
    "print(\"Shape sau khi thêm TA:\", df_with_ta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235303e",
   "metadata": {},
   "source": [
    "**Xóa bớt biến df_merged không còn cần thiết để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26204a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_merged\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614073e",
   "metadata": {},
   "source": [
    "**Block 5: Chuẩn hóa dữ liệu FA và TA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59579dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features:\n",
      "  ticker  timestamp  Debt/Equity  Net Profit Margin (%)       P/E       P/B  \\\n",
      "0    AAA 2023-06-14     0.302753               0.976223  0.581524  0.119378   \n",
      "1    AAM 2023-06-14     0.274571               0.976478  0.567593  0.081723   \n",
      "2    AAT 2023-06-14     0.289872               0.976190  0.566385  0.088712   \n",
      "3    ABR 2023-06-14     0.281477               0.979266  0.568159  0.136659   \n",
      "4    ABS 2023-06-14     0.302868               0.977524  0.572447  0.092493   \n",
      "\n",
      "    ema_5_z  ema_20_z  ema_50_z    macd_z  macd_signal_z  macd_diff_z  \\\n",
      "0  1.335124  1.662545  1.799046 -0.008320       0.591579    -1.164222   \n",
      "1 -0.860284 -0.664630 -0.242035 -0.764137      -0.817184    -0.109792   \n",
      "2  3.147082  3.376253  3.548276  2.967265       3.142393     2.231773   \n",
      "3  0.348835  1.002783  1.511919 -0.803966      -0.276326    -1.417017   \n",
      "4  2.772696  3.133127  3.354561  2.500178       2.788283     1.191728   \n",
      "\n",
      "      rsi_z  bollinger_hband_z  bollinger_lband_z     atr_z     obv_z  \\\n",
      "0 -1.530688           1.363061           1.839777  0.589827  1.405288   \n",
      "1 -0.549930          -1.111354           0.214963 -1.672962 -1.156616   \n",
      "2  1.285717           3.255245          -2.341308  3.090152  2.465147   \n",
      "3 -1.363102           0.856571           1.201947  0.576320 -0.556661   \n",
      "4  0.818367           2.953700           1.355816  2.352162  2.390705   \n",
      "\n",
      "     vwap_z  \n",
      "0  1.603355  \n",
      "1 -0.550497  \n",
      "2  3.222970  \n",
      "3  0.554186  \n",
      "4  2.919060  \n",
      "Shape sau khi scaling & dropna: (221135, 18)\n"
     ]
    }
   ],
   "source": [
    "# Block 5 — Feature engineering & scaling (NO regime, short version)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Danh sách cột FA & TA ---\n",
    "fa_features = [\"Debt/Equity\", \"Net Profit Margin (%)\", \"P/E\", \"P/B\"]\n",
    "\n",
    "ta_features = [\n",
    "    \"ema_5\",\"ema_20\",\"ema_50\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"rsi\",\"bollinger_hband\",\"bollinger_lband\",\"atr\",\"obv\",\"vwap\"\n",
    "]\n",
    "\n",
    "# --- Chuẩn hoá FA: cross-section min-max scaling theo ngày ---\n",
    "def scale_fa_minmax(df):\n",
    "    df_scaled = df.copy()\n",
    "    for f in fa_features:\n",
    "        vals = pd.to_numeric(df[f], errors=\"coerce\")\n",
    "        vmin, vmax = vals.min(), vals.max()\n",
    "        if np.isfinite(vmin) and np.isfinite(vmax) and vmax > vmin:\n",
    "            df_scaled[f] = (vals - vmin) / (vmax - vmin)\n",
    "        else:\n",
    "            df_scaled[f] = np.nan\n",
    "    return df_scaled\n",
    "\n",
    "df_scaled_fa = df_with_ta.groupby(\"timestamp\", group_keys=False).apply(scale_fa_minmax)\n",
    "\n",
    "# --- Chuẩn hoá TA: rolling z-score theo từng ticker ---\n",
    "def zscore_rolling(series, window=60):\n",
    "    return (series - series.rolling(window).mean()) / series.rolling(window).std()\n",
    "\n",
    "df_scaled = df_scaled_fa.groupby(\"ticker\", group_keys=False).apply(\n",
    "    lambda g: g.assign(**{f\"{col}_z\": zscore_rolling(g[col], 60) for col in ta_features})\n",
    ")\n",
    "\n",
    "# --- Drop các cột gốc TA, giữ bản z-score ---\n",
    "keep_cols = [\"ticker\",\"timestamp\"] + fa_features + [f\"{col}_z\" for col in ta_features]\n",
    "df_features = df_scaled[keep_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(df_features.head())\n",
    "print(\"Shape sau khi scaling & dropna:\", df_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f5d94",
   "metadata": {},
   "source": [
    "**Xóa các biến df_with_ta, df_scaled, df_scaled_fa không cần thiết nữa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee3d19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_with_ta, df_scaled, df_scaled_fa\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245a6ca",
   "metadata": {},
   "source": [
    "**Block 6: Giảm chiều dữ liệu bằng t-SNE và phân cụm bằng DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 255, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sample:\n",
      "  ticker  timestamp  cluster     tsne_x     tsne_y    month\n",
      "0    AAA 2023-06-14       -1  17.444849 -43.739433  2023-06\n",
      "1    AAA 2023-06-15       -1  17.398275 -43.821537  2023-06\n",
      "2    AAA 2023-06-16       -1   5.181763 -58.317707  2023-06\n",
      "3    AAA 2023-06-19       -1   4.829109 -57.954765  2023-06\n",
      "4    AAA 2023-06-20       -1   4.363069 -57.532513  2023-06\n",
      "Số cụm mỗi tháng:\n",
      "month\n",
      "2023-06     28\n",
      "2023-07    127\n",
      "2023-08    118\n",
      "2023-09     61\n",
      "2023-10    115\n",
      "2023-11     74\n",
      "2023-12    110\n",
      "2024-01    126\n",
      "2024-02     47\n",
      "2024-03     96\n",
      "2024-04     68\n",
      "2024-05     75\n",
      "2024-06    109\n",
      "2024-07    110\n",
      "2024-08    100\n",
      "2024-09     97\n",
      "2024-10    151\n",
      "2024-11    108\n",
      "2024-12    107\n",
      "2025-01     62\n",
      "2025-02     97\n",
      "2025-03    113\n",
      "2025-04     62\n",
      "2025-05     99\n",
      "2025-06    135\n",
      "2025-07    141\n",
      "2025-08    110\n",
      "2025-09     77\n",
      "2025-10      1\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Block 6 — Giảm chiều dữ liệu & phân cụm (t-SNE + DBSCAN)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# --- Chọn các cột features để phân cụm ---\n",
    "# FA features: khớp với Block 3 & Block 5\n",
    "fa_features = [\"Debt/Equity\", \"Net Profit Margin (%)\", \"P/E\", \"P/B\"]\n",
    "\n",
    "# TA features đã được chuẩn hoá z-score ở Block 5\n",
    "ta_features_z = [c for c in df_features.columns if c.endswith(\"_z\")]\n",
    "\n",
    "feature_cols = fa_features + ta_features_z\n",
    "\n",
    "# --- Thêm cột tháng để snapshot ---\n",
    "df_features[\"month\"] = df_features[\"timestamp\"].dt.to_period(\"M\")\n",
    "\n",
    "cluster_results = []\n",
    "\n",
    "for (month, g) in df_features.groupby(\"month\"):\n",
    "    if len(g) < 10:   # quá ít cổ phiếu thì bỏ\n",
    "        continue\n",
    "\n",
    "    X = g[feature_cols].values\n",
    "\n",
    "    # --- t-SNE giảm chiều còn 2D ---\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"random\", random_state=42)\n",
    "    X_emb = tsne.fit_transform(X)\n",
    "\n",
    "    # --- DBSCAN phân cụm ---\n",
    "    db = DBSCAN(eps=0.5, min_samples=5).fit(X_emb)\n",
    "    labels = db.labels_\n",
    "\n",
    "    temp = g[[\"ticker\",\"timestamp\"]].copy()\n",
    "    temp[\"cluster\"] = labels\n",
    "    temp[\"tsne_x\"] = X_emb[:,0]\n",
    "    temp[\"tsne_y\"] = X_emb[:,1]\n",
    "    temp[\"month\"]  = str(month)\n",
    "\n",
    "    cluster_results.append(temp)\n",
    "\n",
    "df_clusters = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(\"Cluster sample:\")\n",
    "print(df_clusters.head())\n",
    "print(\"Số cụm mỗi tháng:\")\n",
    "print(df_clusters.groupby(\"month\")[\"cluster\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde0916",
   "metadata": {},
   "source": [
    "**Block 7: Xây tensors (clusters mapping) và masks (active stocks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d88b8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Tensor data directory: C:/tensors_out\n",
      "✅ Cluster 0: tensor (512, 64, 15, 16), mask (512, 64, 15, 16) saved.\n",
      "✅ Cluster 1: tensor (512, 64, 14, 16), mask (512, 64, 14, 16) saved.\n",
      "✅ Cluster 2: tensor (512, 64, 20, 16), mask (512, 64, 20, 16) saved.\n",
      "✅ Cluster 3: tensor (512, 64, 21, 16), mask (512, 64, 21, 16) saved.\n",
      "✅ Cluster 4: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 5: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 6: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 7: tensor (512, 64, 22, 16), mask (512, 64, 22, 16) saved.\n",
      "✅ Cluster 8: tensor (512, 64, 22, 16), mask (512, 64, 22, 16) saved.\n",
      "✅ Cluster 9: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 10: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 11: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 12: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 13: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 14: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 15: tensor (512, 64, 32, 16), mask (512, 64, 32, 16) saved.\n",
      "✅ Cluster 16: tensor (512, 64, 29, 16), mask (512, 64, 29, 16) saved.\n",
      "✅ Cluster 17: tensor (512, 64, 31, 16), mask (512, 64, 31, 16) saved.\n",
      "✅ Cluster 18: tensor (512, 64, 29, 16), mask (512, 64, 29, 16) saved.\n",
      "✅ Cluster 19: tensor (512, 64, 31, 16), mask (512, 64, 31, 16) saved.\n",
      "✅ Cluster 20: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 21: tensor (512, 64, 33, 16), mask (512, 64, 33, 16) saved.\n",
      "✅ Cluster 22: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 23: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 24: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 25: tensor (512, 64, 33, 16), mask (512, 64, 33, 16) saved.\n",
      "✅ Cluster 26: tensor (512, 64, 29, 16), mask (512, 64, 29, 16) saved.\n",
      "✅ Cluster 27: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 28: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 29: tensor (512, 64, 32, 16), mask (512, 64, 32, 16) saved.\n",
      "✅ Cluster 30: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 31: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 32: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 33: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 34: tensor (512, 64, 29, 16), mask (512, 64, 29, 16) saved.\n",
      "✅ Cluster 35: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 36: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 37: tensor (512, 64, 32, 16), mask (512, 64, 32, 16) saved.\n",
      "✅ Cluster 38: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 39: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 40: tensor (512, 64, 32, 16), mask (512, 64, 32, 16) saved.\n",
      "✅ Cluster 41: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 42: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 43: tensor (512, 64, 32, 16), mask (512, 64, 32, 16) saved.\n",
      "✅ Cluster 44: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 45: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 46: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 47: tensor (512, 64, 29, 16), mask (512, 64, 29, 16) saved.\n",
      "✅ Cluster 48: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 49: tensor (512, 64, 31, 16), mask (512, 64, 31, 16) saved.\n",
      "✅ Cluster 50: tensor (512, 64, 29, 16), mask (512, 64, 29, 16) saved.\n",
      "✅ Cluster 51: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 52: tensor (512, 64, 31, 16), mask (512, 64, 31, 16) saved.\n",
      "✅ Cluster 53: tensor (512, 64, 30, 16), mask (512, 64, 30, 16) saved.\n",
      "✅ Cluster 54: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 55: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 56: tensor (512, 64, 24, 16), mask (512, 64, 24, 16) saved.\n",
      "✅ Cluster 57: tensor (512, 64, 32, 16), mask (512, 64, 32, 16) saved.\n",
      "✅ Cluster 58: tensor (512, 64, 28, 16), mask (512, 64, 28, 16) saved.\n",
      "✅ Cluster 59: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 60: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 61: tensor (512, 64, 24, 16), mask (512, 64, 24, 16) saved.\n",
      "✅ Cluster 62: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 63: tensor (512, 64, 23, 16), mask (512, 64, 23, 16) saved.\n",
      "✅ Cluster 64: tensor (512, 64, 23, 16), mask (512, 64, 23, 16) saved.\n",
      "✅ Cluster 65: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 66: tensor (512, 64, 27, 16), mask (512, 64, 27, 16) saved.\n",
      "✅ Cluster 67: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 68: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 69: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 70: tensor (512, 64, 23, 16), mask (512, 64, 23, 16) saved.\n",
      "✅ Cluster 71: tensor (512, 64, 24, 16), mask (512, 64, 24, 16) saved.\n",
      "✅ Cluster 72: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 73: tensor (512, 64, 20, 16), mask (512, 64, 20, 16) saved.\n",
      "✅ Cluster 74: tensor (512, 64, 22, 16), mask (512, 64, 22, 16) saved.\n",
      "✅ Cluster 75: tensor (512, 64, 23, 16), mask (512, 64, 23, 16) saved.\n",
      "✅ Cluster 76: tensor (512, 64, 19, 16), mask (512, 64, 19, 16) saved.\n",
      "✅ Cluster 77: tensor (512, 64, 19, 16), mask (512, 64, 19, 16) saved.\n",
      "✅ Cluster 78: tensor (512, 64, 21, 16), mask (512, 64, 21, 16) saved.\n",
      "✅ Cluster 79: tensor (512, 64, 21, 16), mask (512, 64, 21, 16) saved.\n",
      "✅ Cluster 80: tensor (512, 64, 26, 16), mask (512, 64, 26, 16) saved.\n",
      "✅ Cluster 81: tensor (512, 64, 19, 16), mask (512, 64, 19, 16) saved.\n",
      "✅ Cluster 82: tensor (512, 64, 20, 16), mask (512, 64, 20, 16) saved.\n",
      "✅ Cluster 83: tensor (512, 64, 25, 16), mask (512, 64, 25, 16) saved.\n",
      "✅ Cluster 84: tensor (512, 64, 23, 16), mask (512, 64, 23, 16) saved.\n",
      "✅ Cluster 85: tensor (512, 64, 19, 16), mask (512, 64, 19, 16) saved.\n",
      "✅ Cluster 86: tensor (512, 64, 21, 16), mask (512, 64, 21, 16) saved.\n",
      "✅ Cluster 87: tensor (512, 64, 18, 16), mask (512, 64, 18, 16) saved.\n",
      "✅ Cluster 88: tensor (512, 64, 19, 16), mask (512, 64, 19, 16) saved.\n",
      "✅ Cluster 89: tensor (512, 64, 19, 16), mask (512, 64, 19, 16) saved.\n",
      "✅ Cluster 90: tensor (512, 64, 22, 16), mask (512, 64, 22, 16) saved.\n",
      "✅ Cluster 91: tensor (512, 64, 18, 16), mask (512, 64, 18, 16) saved.\n",
      "✅ Cluster 92: tensor (512, 64, 16, 16), mask (512, 64, 16, 16) saved.\n",
      "✅ Cluster 93: tensor (512, 64, 18, 16), mask (512, 64, 18, 16) saved.\n",
      "✅ Cluster 94: tensor (512, 64, 24, 16), mask (512, 64, 24, 16) saved.\n",
      "✅ Cluster 95: tensor (512, 64, 22, 16), mask (512, 64, 22, 16) saved.\n",
      "✅ Cluster 96: tensor (512, 64, 16, 16), mask (512, 64, 16, 16) saved.\n",
      "✅ Cluster 97: tensor (512, 64, 18, 16), mask (512, 64, 18, 16) saved.\n",
      "✅ Cluster 98: tensor (512, 64, 16, 16), mask (512, 64, 16, 16) saved.\n",
      "✅ Cluster 99: tensor (512, 64, 15, 16), mask (512, 64, 15, 16) saved.\n",
      "✅ Cluster 100: tensor (512, 64, 13, 16), mask (512, 64, 13, 16) saved.\n",
      "✅ Cluster 101: tensor (512, 64, 14, 16), mask (512, 64, 14, 16) saved.\n",
      "✅ Cluster 102: tensor (512, 64, 12, 16), mask (512, 64, 12, 16) saved.\n",
      "✅ Cluster 103: tensor (512, 64, 22, 16), mask (512, 64, 22, 16) saved.\n",
      "✅ Cluster 104: tensor (512, 64, 13, 16), mask (512, 64, 13, 16) saved.\n",
      "✅ Cluster 105: tensor (512, 64, 13, 16), mask (512, 64, 13, 16) saved.\n",
      "✅ Cluster 106: tensor (512, 64, 14, 16), mask (512, 64, 14, 16) saved.\n",
      "✅ Cluster 107: tensor (512, 64, 12, 16), mask (512, 64, 12, 16) saved.\n",
      "✅ Cluster 108: tensor (512, 64, 13, 16), mask (512, 64, 13, 16) saved.\n",
      "✅ Cluster 109: tensor (512, 64, 9, 16), mask (512, 64, 9, 16) saved.\n",
      "✅ Cluster 110: tensor (512, 64, 9, 16), mask (512, 64, 9, 16) saved.\n",
      "✅ Cluster 111: tensor (512, 64, 11, 16), mask (512, 64, 11, 16) saved.\n",
      "✅ Cluster 112: tensor (512, 64, 6, 16), mask (512, 64, 6, 16) saved.\n",
      "✅ Cluster 113: tensor (512, 64, 6, 16), mask (512, 64, 6, 16) saved.\n",
      "✅ Cluster 114: tensor (512, 64, 6, 16), mask (512, 64, 6, 16) saved.\n",
      "✅ Cluster 115: tensor (512, 64, 6, 16), mask (512, 64, 6, 16) saved.\n",
      "✅ Cluster 116: tensor (512, 64, 8, 16), mask (512, 64, 8, 16) saved.\n",
      "✅ Cluster 117: tensor (512, 64, 5, 16), mask (512, 64, 5, 16) saved.\n",
      "✅ Cluster 118: tensor (512, 64, 5, 16), mask (512, 64, 5, 16) saved.\n",
      "✅ Cluster 119: tensor (512, 64, 5, 16), mask (512, 64, 5, 16) saved.\n",
      "✅ Cluster 120: tensor (512, 64, 5, 16), mask (512, 64, 5, 16) saved.\n",
      "✅ Cluster 121: tensor (512, 64, 8, 16), mask (512, 64, 8, 16) saved.\n",
      "✅ Cluster 122: tensor (512, 64, 6, 16), mask (512, 64, 6, 16) saved.\n",
      "✅ Cluster 123: tensor (512, 64, 4, 16), mask (512, 64, 4, 16) saved.\n",
      "✅ Cluster 124: tensor (512, 64, 5, 16), mask (512, 64, 5, 16) saved.\n",
      "✅ Cluster 125: tensor (512, 64, 4, 16), mask (512, 64, 4, 16) saved.\n",
      "✅ Cluster 126: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 127: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 128: tensor (512, 64, 4, 16), mask (512, 64, 4, 16) saved.\n",
      "✅ Cluster 129: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 130: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 131: tensor (512, 64, 5, 16), mask (512, 64, 5, 16) saved.\n",
      "✅ Cluster 132: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 133: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 134: tensor (512, 64, 2, 16), mask (512, 64, 2, 16) saved.\n",
      "✅ Cluster 135: tensor (512, 64, 2, 16), mask (512, 64, 2, 16) saved.\n",
      "✅ Cluster 136: tensor (512, 64, 2, 16), mask (512, 64, 2, 16) saved.\n",
      "✅ Cluster 137: tensor (512, 64, 3, 16), mask (512, 64, 3, 16) saved.\n",
      "✅ Cluster 138: tensor (512, 64, 2, 16), mask (512, 64, 2, 16) saved.\n",
      "✅ Cluster 139: tensor (512, 64, 2, 16), mask (512, 64, 2, 16) saved.\n",
      "✅ Cluster 140: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 141: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 142: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 143: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 144: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 145: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 146: tensor (511, 64, 1, 16), mask (511, 64, 1, 16) saved.\n",
      "✅ Cluster 147: tensor (511, 64, 1, 16), mask (511, 64, 1, 16) saved.\n",
      "✅ Cluster 148: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "✅ Cluster 149: tensor (512, 64, 1, 16), mask (512, 64, 1, 16) saved.\n",
      "🎯 Done Block 7.2: tensors + masks saved for all clusters.\n"
     ]
    }
   ],
   "source": [
    "# Block 7.2 — Tensors & Masks (fix: lưu ra folder khác)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, json\n",
    "\n",
    "LOOKBACK = 64   # window size\n",
    "DATA_DIR = \"C:/tensors_out\"   # đổi sang C ổ cho đơn giản\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(\"📂 Tensor data directory:\", DATA_DIR)\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in [\"ticker\",\"timestamp\",\"cluster\",\"month\"]]\n",
    "\n",
    "tensor_index = []\n",
    "\n",
    "for c_id, g in df_clusters.groupby(\"cluster\"):\n",
    "    if c_id == -1:   # noise bỏ qua\n",
    "        continue\n",
    "\n",
    "    tickers = sorted(g[\"ticker\"].unique())\n",
    "    g_feat = df_features[df_features[\"ticker\"].isin(tickers)].copy()\n",
    "\n",
    "    # Pivot: index = timestamp, columns = (ticker, feature)\n",
    "    pivoted = g_feat.pivot(index=\"timestamp\", columns=\"ticker\", values=feature_cols)\n",
    "    pivoted.columns = pd.MultiIndex.from_product([tickers, feature_cols])\n",
    "\n",
    "    # Mask\n",
    "    mask_df = ~pivoted.isna()\n",
    "    pivoted_filled = pivoted.ffill().bfill()\n",
    "\n",
    "    T, N, F = len(pivoted_filled.index), len(tickers), len(feature_cols)\n",
    "    X = pivoted_filled.values.reshape(T, N, F)\n",
    "    M = mask_df.values.reshape(T, N, F).astype(np.int8)\n",
    "\n",
    "    cluster_tensors, cluster_masks, cluster_dates = [], [], []\n",
    "    for i in range(LOOKBACK, T):\n",
    "        cluster_tensors.append(X[i-LOOKBACK:i])\n",
    "        cluster_masks.append(M[i-LOOKBACK:i])\n",
    "        cluster_dates.append(pivoted_filled.index[i])  # ngày cuối của window\n",
    "\n",
    "    if cluster_tensors:\n",
    "        X_arr = np.array(cluster_tensors, dtype=np.float16)  # tiết kiệm RAM\n",
    "        M_arr = np.array(cluster_masks, dtype=np.int8)\n",
    "\n",
    "        tensor_file = f\"cluster_{c_id}_tensor.npy\"\n",
    "        mask_file   = f\"cluster_{c_id}_mask.npy\"\n",
    "\n",
    "        np.save(os.path.join(DATA_DIR, tensor_file), X_arr)\n",
    "        np.save(os.path.join(DATA_DIR, mask_file), M_arr)\n",
    "\n",
    "        tensor_index.append({\n",
    "            \"cluster\": int(c_id),\n",
    "            \"tickers\": tickers,\n",
    "            \"dates\": [str(d) for d in cluster_dates],\n",
    "            \"dates_shifted\": [str(d+pd.Timedelta(days=1)) for d in cluster_dates],\n",
    "            \"tensor_file\": tensor_file,\n",
    "            \"mask_file\": mask_file\n",
    "        })\n",
    "\n",
    "        print(f\"✅ Cluster {c_id}: tensor {X_arr.shape}, mask {M_arr.shape} saved.\")\n",
    "\n",
    "    del g_feat, pivoted, pivoted_filled, mask_df, X, M, cluster_tensors, cluster_masks\n",
    "    gc.collect()\n",
    "\n",
    "# Save metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"w\") as f:\n",
    "    json.dump(tensor_index, f, indent=2)\n",
    "\n",
    "print(\"🎯 Done Block 7.2: tensors + masks saved for all clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95417aff",
   "metadata": {},
   "source": [
    "**Block 7.5: Chuẩn bị dữ liệu backtest(loại bỏ các cột dữ liệu không cần thiết nữa)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e9bfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done Block 7.5: df_backtest sẵn sàng cho reward.\n",
      "Kích thước df_backtest: (264721, 6)\n",
      "Số tickers unique: 391\n",
      "Khoảng thời gian: 2023-01-03 00:00:00 → 2025-10-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Block 7.5 — Chuẩn bị dữ liệu backtest cho reward thật (từ CSV Block 3)\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "# Load lại từ file đã lưu ở Block 3\n",
    "df_price = pd.read_csv(\"vnindex_price_fa_merged.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# Chỉ giữ OHLC cần thiết\n",
    "df_backtest = df_price[[\"ticker\",\"timestamp\",\"open\",\"high\",\"low\",\"close\"]].copy()\n",
    "df_backtest = df_backtest.sort_values([\"timestamp\",\"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Done Block 7.5: df_backtest sẵn sàng cho reward.\")\n",
    "print(\"Kích thước df_backtest:\", df_backtest.shape)\n",
    "print(\"Số tickers unique:\", df_backtest['ticker'].nunique())\n",
    "print(\"Khoảng thời gian:\", df_backtest['timestamp'].min(), \"→\", df_backtest['timestamp'].max())\n",
    "\n",
    "# Lưu lại để Block 10x dùng cho SL/TP backtest\n",
    "df_backtest.to_csv(\"./backtest_ddpg/df_backtest.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7aa4e",
   "metadata": {},
   "source": [
    "**Block 8: Huấn luyện A3C theo từng cụm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e993a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 | X=(512, 64, 15, 19)\n",
      "  Epoch 1/3, Loss=0.5582\n",
      "  Epoch 2/3, Loss=-0.0633\n",
      "  Epoch 3/3, Loss=-0.3345\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_0.pt\n",
      "Cluster 1 | X=(512, 64, 14, 19)\n",
      "  Epoch 1/3, Loss=-0.3479\n",
      "  Epoch 2/3, Loss=-0.1559\n",
      "  Epoch 3/3, Loss=-0.3015\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_1.pt\n",
      "Cluster 2 | X=(512, 64, 20, 19)\n",
      "  Epoch 1/3, Loss=-0.5230\n",
      "  Epoch 2/3, Loss=-0.3853\n",
      "  Epoch 3/3, Loss=-0.4694\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_2.pt\n",
      "Cluster 3 | X=(512, 64, 21, 19)\n",
      "  Epoch 1/3, Loss=-0.4557\n",
      "  Epoch 2/3, Loss=-0.4514\n",
      "  Epoch 3/3, Loss=-0.4496\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_3.pt\n",
      "Cluster 4 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-1.1942\n",
      "  Epoch 2/3, Loss=-0.5445\n",
      "  Epoch 3/3, Loss=-0.6573\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_4.pt\n",
      "Cluster 5 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=0.3239\n",
      "  Epoch 2/3, Loss=-0.4962\n",
      "  Epoch 3/3, Loss=-0.5282\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_5.pt\n",
      "Cluster 6 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.4335\n",
      "  Epoch 2/3, Loss=-0.5409\n",
      "  Epoch 3/3, Loss=-0.5386\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_6.pt\n",
      "Cluster 7 | X=(512, 64, 22, 19)\n",
      "  Epoch 1/3, Loss=-0.9845\n",
      "  Epoch 2/3, Loss=-0.5491\n",
      "  Epoch 3/3, Loss=-0.4934\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_7.pt\n",
      "Cluster 8 | X=(512, 64, 22, 19)\n",
      "  Epoch 1/3, Loss=-0.4024\n",
      "  Epoch 2/3, Loss=-0.4782\n",
      "  Epoch 3/3, Loss=-0.4702\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_8.pt\n",
      "Cluster 9 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.6432\n",
      "  Epoch 2/3, Loss=-0.5165\n",
      "  Epoch 3/3, Loss=-0.5443\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_9.pt\n",
      "Cluster 10 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=-0.3029\n",
      "  Epoch 2/3, Loss=-0.6139\n",
      "  Epoch 3/3, Loss=-0.6449\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_10.pt\n",
      "Cluster 11 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=-0.5145\n",
      "  Epoch 2/3, Loss=-0.5636\n",
      "  Epoch 3/3, Loss=-0.5615\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_11.pt\n",
      "Cluster 12 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=0.5481\n",
      "  Epoch 2/3, Loss=-0.5373\n",
      "  Epoch 3/3, Loss=-0.5595\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_12.pt\n",
      "Cluster 13 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.7032\n",
      "  Epoch 2/3, Loss=-0.4939\n",
      "  Epoch 3/3, Loss=-0.5515\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_13.pt\n",
      "Cluster 14 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-0.2425\n",
      "  Epoch 2/3, Loss=-0.5659\n",
      "  Epoch 3/3, Loss=-0.5618\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_14.pt\n",
      "Cluster 15 | X=(512, 64, 32, 19)\n",
      "  Epoch 1/3, Loss=-0.2747\n",
      "  Epoch 2/3, Loss=-0.6492\n",
      "  Epoch 3/3, Loss=-0.6779\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_15.pt\n",
      "Cluster 16 | X=(512, 64, 29, 19)\n",
      "  Epoch 1/3, Loss=-0.7736\n",
      "  Epoch 2/3, Loss=-0.6496\n",
      "  Epoch 3/3, Loss=-0.6374\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_16.pt\n",
      "Cluster 17 | X=(512, 64, 31, 19)\n",
      "  Epoch 1/3, Loss=-1.0665\n",
      "  Epoch 2/3, Loss=-0.7277\n",
      "  Epoch 3/3, Loss=-0.6796\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_17.pt\n",
      "Cluster 18 | X=(512, 64, 29, 19)\n",
      "  Epoch 1/3, Loss=-0.5634\n",
      "  Epoch 2/3, Loss=-0.6225\n",
      "  Epoch 3/3, Loss=-0.6191\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_18.pt\n",
      "Cluster 19 | X=(512, 64, 31, 19)\n",
      "  Epoch 1/3, Loss=-0.3552\n",
      "  Epoch 2/3, Loss=-0.6359\n",
      "  Epoch 3/3, Loss=-0.6643\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_19.pt\n",
      "Cluster 20 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=0.0165\n",
      "  Epoch 2/3, Loss=-0.6326\n",
      "  Epoch 3/3, Loss=-0.5143\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_20.pt\n",
      "Cluster 21 | X=(512, 64, 33, 19)\n",
      "  Epoch 1/3, Loss=-0.9341\n",
      "  Epoch 2/3, Loss=-0.7226\n",
      "  Epoch 3/3, Loss=-0.7126\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_21.pt\n",
      "Cluster 22 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-0.5063\n",
      "  Epoch 2/3, Loss=-0.5642\n",
      "  Epoch 3/3, Loss=-0.5919\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_22.pt\n",
      "Cluster 23 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=-0.6184\n",
      "  Epoch 2/3, Loss=-0.6074\n",
      "  Epoch 3/3, Loss=-0.6394\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_23.pt\n",
      "Cluster 24 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=-0.4364\n",
      "  Epoch 2/3, Loss=-0.6551\n",
      "  Epoch 3/3, Loss=-0.6243\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_24.pt\n",
      "Cluster 25 | X=(512, 64, 33, 19)\n",
      "  Epoch 1/3, Loss=-0.5929\n",
      "  Epoch 2/3, Loss=-0.7038\n",
      "  Epoch 3/3, Loss=-0.7135\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_25.pt\n",
      "Cluster 26 | X=(512, 64, 29, 19)\n",
      "  Epoch 1/3, Loss=-1.7090\n",
      "  Epoch 2/3, Loss=-0.6740\n",
      "  Epoch 3/3, Loss=-0.6250\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_26.pt\n",
      "Cluster 27 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-1.2411\n",
      "  Epoch 2/3, Loss=-0.5643\n",
      "  Epoch 3/3, Loss=-0.5446\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_27.pt\n",
      "Cluster 28 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=-0.3974\n",
      "  Epoch 2/3, Loss=-0.5706\n",
      "  Epoch 3/3, Loss=-0.5686\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_28.pt\n",
      "Cluster 29 | X=(512, 64, 32, 19)\n",
      "  Epoch 1/3, Loss=-0.6149\n",
      "  Epoch 2/3, Loss=-0.6932\n",
      "  Epoch 3/3, Loss=-0.6760\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_29.pt\n",
      "Cluster 30 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-0.6443\n",
      "  Epoch 2/3, Loss=-0.6113\n",
      "  Epoch 3/3, Loss=-0.6065\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_30.pt\n",
      "Cluster 31 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-1.0250\n",
      "  Epoch 2/3, Loss=-0.4655\n",
      "  Epoch 3/3, Loss=-0.6428\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_31.pt\n",
      "Cluster 32 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-1.0769\n",
      "  Epoch 2/3, Loss=-0.5461\n",
      "  Epoch 3/3, Loss=-0.5598\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_32.pt\n",
      "Cluster 33 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-0.5713\n",
      "  Epoch 2/3, Loss=-0.6105\n",
      "  Epoch 3/3, Loss=-0.6067\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_33.pt\n",
      "Cluster 34 | X=(512, 64, 29, 19)\n",
      "  Epoch 1/3, Loss=-0.6555\n",
      "  Epoch 2/3, Loss=-0.6207\n",
      "  Epoch 3/3, Loss=-0.6214\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_34.pt\n",
      "Cluster 35 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=-0.9553\n",
      "  Epoch 2/3, Loss=-0.6309\n",
      "  Epoch 3/3, Loss=-0.6406\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_35.pt\n",
      "Cluster 36 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=0.0665\n",
      "  Epoch 2/3, Loss=-0.6320\n",
      "  Epoch 3/3, Loss=-0.6358\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_36.pt\n",
      "Cluster 37 | X=(512, 64, 32, 19)\n",
      "  Epoch 1/3, Loss=-0.3783\n",
      "  Epoch 2/3, Loss=-0.6718\n",
      "  Epoch 3/3, Loss=-0.6800\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_37.pt\n",
      "Cluster 38 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=-0.6578\n",
      "  Epoch 2/3, Loss=-0.6354\n",
      "  Epoch 3/3, Loss=-0.6428\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_38.pt\n",
      "Cluster 39 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-0.5593\n",
      "  Epoch 2/3, Loss=-0.5831\n",
      "  Epoch 3/3, Loss=-0.5818\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_39.pt\n",
      "Cluster 40 | X=(512, 64, 32, 19)\n",
      "  Epoch 1/3, Loss=-0.6748\n",
      "  Epoch 2/3, Loss=-0.6897\n",
      "  Epoch 3/3, Loss=-0.6989\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_40.pt\n",
      "Cluster 41 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=-0.4468\n",
      "  Epoch 2/3, Loss=-0.6141\n",
      "  Epoch 3/3, Loss=-0.5584\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_41.pt\n",
      "Cluster 42 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=-0.9033\n",
      "  Epoch 2/3, Loss=-0.5137\n",
      "  Epoch 3/3, Loss=-0.5792\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_42.pt\n",
      "Cluster 43 | X=(512, 64, 32, 19)\n",
      "  Epoch 1/3, Loss=-0.7674\n",
      "  Epoch 2/3, Loss=-0.6952\n",
      "  Epoch 3/3, Loss=-0.6923\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_43.pt\n",
      "Cluster 44 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-0.6038\n",
      "  Epoch 2/3, Loss=-0.5894\n",
      "  Epoch 3/3, Loss=-0.6087\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_44.pt\n",
      "Cluster 45 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-0.6411\n",
      "  Epoch 2/3, Loss=-0.6097\n",
      "  Epoch 3/3, Loss=-0.5945\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_45.pt\n",
      "Cluster 46 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-0.5107\n",
      "  Epoch 2/3, Loss=-0.5644\n",
      "  Epoch 3/3, Loss=-0.6010\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_46.pt\n",
      "Cluster 47 | X=(512, 64, 29, 19)\n",
      "  Epoch 1/3, Loss=-0.0991\n",
      "  Epoch 2/3, Loss=-0.5959\n",
      "  Epoch 3/3, Loss=-0.5828\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_47.pt\n",
      "Cluster 48 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-0.5451\n",
      "  Epoch 2/3, Loss=-0.5975\n",
      "  Epoch 3/3, Loss=-0.5880\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_48.pt\n",
      "Cluster 49 | X=(512, 64, 31, 19)\n",
      "  Epoch 1/3, Loss=-1.0121\n",
      "  Epoch 2/3, Loss=-0.6964\n",
      "  Epoch 3/3, Loss=-0.6630\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_49.pt\n",
      "Cluster 50 | X=(512, 64, 29, 19)\n",
      "  Epoch 1/3, Loss=-0.3921\n",
      "  Epoch 2/3, Loss=-0.5810\n",
      "  Epoch 3/3, Loss=-0.6085\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_50.pt\n",
      "Cluster 51 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.3510\n",
      "  Epoch 2/3, Loss=-0.5216\n",
      "  Epoch 3/3, Loss=-0.5301\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_51.pt\n",
      "Cluster 52 | X=(512, 64, 31, 19)\n",
      "  Epoch 1/3, Loss=-0.6226\n",
      "  Epoch 2/3, Loss=-0.6760\n",
      "  Epoch 3/3, Loss=-0.6559\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_52.pt\n",
      "Cluster 53 | X=(512, 64, 30, 19)\n",
      "  Epoch 1/3, Loss=-1.0019\n",
      "  Epoch 2/3, Loss=-0.6680\n",
      "  Epoch 3/3, Loss=-0.6544\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_53.pt\n",
      "Cluster 54 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=0.1730\n",
      "  Epoch 2/3, Loss=-0.6335\n",
      "  Epoch 3/3, Loss=-0.5750\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_54.pt\n",
      "Cluster 55 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-0.6616\n",
      "  Epoch 2/3, Loss=-0.6126\n",
      "  Epoch 3/3, Loss=-0.5924\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_55.pt\n",
      "Cluster 56 | X=(512, 64, 24, 19)\n",
      "  Epoch 1/3, Loss=-0.3877\n",
      "  Epoch 2/3, Loss=-0.4902\n",
      "  Epoch 3/3, Loss=-0.5148\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_56.pt\n",
      "Cluster 57 | X=(512, 64, 32, 19)\n",
      "  Epoch 1/3, Loss=-0.4254\n",
      "  Epoch 2/3, Loss=-0.6701\n",
      "  Epoch 3/3, Loss=-0.6939\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_57.pt\n",
      "Cluster 58 | X=(512, 64, 28, 19)\n",
      "  Epoch 1/3, Loss=-1.5535\n",
      "  Epoch 2/3, Loss=-0.6368\n",
      "  Epoch 3/3, Loss=-0.6133\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_58.pt\n",
      "Cluster 59 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-1.3874\n",
      "  Epoch 2/3, Loss=-0.5675\n",
      "  Epoch 3/3, Loss=-0.6178\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_59.pt\n",
      "Cluster 60 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=0.0647\n",
      "  Epoch 2/3, Loss=-0.5609\n",
      "  Epoch 3/3, Loss=-0.5522\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_60.pt\n",
      "Cluster 61 | X=(512, 64, 24, 19)\n",
      "  Epoch 1/3, Loss=-0.6405\n",
      "  Epoch 2/3, Loss=-0.4591\n",
      "  Epoch 3/3, Loss=-0.5345\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_61.pt\n",
      "Cluster 62 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.6686\n",
      "  Epoch 2/3, Loss=-0.4707\n",
      "  Epoch 3/3, Loss=-0.5798\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_62.pt\n",
      "Cluster 63 | X=(512, 64, 23, 19)\n",
      "  Epoch 1/3, Loss=-0.4048\n",
      "  Epoch 2/3, Loss=-0.5393\n",
      "  Epoch 3/3, Loss=-0.4782\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_63.pt\n",
      "Cluster 64 | X=(512, 64, 23, 19)\n",
      "  Epoch 1/3, Loss=-0.4737\n",
      "  Epoch 2/3, Loss=-0.4657\n",
      "  Epoch 3/3, Loss=-0.4844\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_64.pt\n",
      "Cluster 65 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-0.1270\n",
      "  Epoch 2/3, Loss=-0.6514\n",
      "  Epoch 3/3, Loss=-0.5354\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_65.pt\n",
      "Cluster 66 | X=(512, 64, 27, 19)\n",
      "  Epoch 1/3, Loss=-1.2823\n",
      "  Epoch 2/3, Loss=-0.6042\n",
      "  Epoch 3/3, Loss=-0.6166\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_66.pt\n",
      "Cluster 67 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.4903\n",
      "  Epoch 2/3, Loss=-0.5786\n",
      "  Epoch 3/3, Loss=-0.5100\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_67.pt\n",
      "Cluster 68 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=-0.8498\n",
      "  Epoch 2/3, Loss=-0.5221\n",
      "  Epoch 3/3, Loss=-0.5661\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_68.pt\n",
      "Cluster 69 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.1835\n",
      "  Epoch 2/3, Loss=-0.6102\n",
      "  Epoch 3/3, Loss=-0.5258\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_69.pt\n",
      "Cluster 70 | X=(512, 64, 23, 19)\n",
      "  Epoch 1/3, Loss=-0.3281\n",
      "  Epoch 2/3, Loss=-0.4795\n",
      "  Epoch 3/3, Loss=-0.4988\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_70.pt\n",
      "Cluster 71 | X=(512, 64, 24, 19)\n",
      "  Epoch 1/3, Loss=-0.4584\n",
      "  Epoch 2/3, Loss=-0.5225\n",
      "  Epoch 3/3, Loss=-0.5025\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_71.pt\n",
      "Cluster 72 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.7304\n",
      "  Epoch 2/3, Loss=-0.5274\n",
      "  Epoch 3/3, Loss=-0.5460\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_72.pt\n",
      "Cluster 73 | X=(512, 64, 20, 19)\n",
      "  Epoch 1/3, Loss=-0.9691\n",
      "  Epoch 2/3, Loss=-0.4552\n",
      "  Epoch 3/3, Loss=-0.4147\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_73.pt\n",
      "Cluster 74 | X=(512, 64, 22, 19)\n",
      "  Epoch 1/3, Loss=0.9843\n",
      "  Epoch 2/3, Loss=-0.4030\n",
      "  Epoch 3/3, Loss=-0.4593\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_74.pt\n",
      "Cluster 75 | X=(512, 64, 23, 19)\n",
      "  Epoch 1/3, Loss=0.4647\n",
      "  Epoch 2/3, Loss=-0.4584\n",
      "  Epoch 3/3, Loss=-0.4685\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_75.pt\n",
      "Cluster 76 | X=(512, 64, 19, 19)\n",
      "  Epoch 1/3, Loss=-0.3757\n",
      "  Epoch 2/3, Loss=-0.0143\n",
      "  Epoch 3/3, Loss=-0.4284\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_76.pt\n",
      "Cluster 77 | X=(512, 64, 19, 19)\n",
      "  Epoch 1/3, Loss=-0.2752\n",
      "  Epoch 2/3, Loss=-0.5146\n",
      "  Epoch 3/3, Loss=-0.2830\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_77.pt\n",
      "Cluster 78 | X=(512, 64, 21, 19)\n",
      "  Epoch 1/3, Loss=0.0682\n",
      "  Epoch 2/3, Loss=-0.4128\n",
      "  Epoch 3/3, Loss=-0.4062\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_78.pt\n",
      "Cluster 79 | X=(512, 64, 21, 19)\n",
      "  Epoch 1/3, Loss=-0.6123\n",
      "  Epoch 2/3, Loss=-0.4405\n",
      "  Epoch 3/3, Loss=-0.5071\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_79.pt\n",
      "Cluster 80 | X=(512, 64, 26, 19)\n",
      "  Epoch 1/3, Loss=-0.5093\n",
      "  Epoch 2/3, Loss=-0.4865\n",
      "  Epoch 3/3, Loss=-0.5839\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_80.pt\n",
      "Cluster 81 | X=(512, 64, 19, 19)\n",
      "  Epoch 1/3, Loss=0.2528\n",
      "  Epoch 2/3, Loss=-0.5223\n",
      "  Epoch 3/3, Loss=-0.2311\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_81.pt\n",
      "Cluster 82 | X=(512, 64, 20, 19)\n",
      "  Epoch 1/3, Loss=-0.2139\n",
      "  Epoch 2/3, Loss=-0.4147\n",
      "  Epoch 3/3, Loss=-0.4257\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_82.pt\n",
      "Cluster 83 | X=(512, 64, 25, 19)\n",
      "  Epoch 1/3, Loss=-0.1658\n",
      "  Epoch 2/3, Loss=-0.5738\n",
      "  Epoch 3/3, Loss=-0.5185\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_83.pt\n",
      "Cluster 84 | X=(512, 64, 23, 19)\n",
      "  Epoch 1/3, Loss=0.1668\n",
      "  Epoch 2/3, Loss=-0.3145\n",
      "  Epoch 3/3, Loss=-0.4735\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_84.pt\n",
      "Cluster 85 | X=(512, 64, 19, 19)\n",
      "  Epoch 1/3, Loss=-0.3730\n",
      "  Epoch 2/3, Loss=-0.0293\n",
      "  Epoch 3/3, Loss=-0.6091\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_85.pt\n",
      "Cluster 86 | X=(512, 64, 21, 19)\n",
      "  Epoch 1/3, Loss=-0.1642\n",
      "  Epoch 2/3, Loss=-0.4291\n",
      "  Epoch 3/3, Loss=-0.4316\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_86.pt\n",
      "Cluster 87 | X=(512, 64, 18, 19)\n",
      "  Epoch 1/3, Loss=0.1419\n",
      "  Epoch 2/3, Loss=-0.3176\n",
      "  Epoch 3/3, Loss=-0.3628\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_87.pt\n",
      "Cluster 88 | X=(512, 64, 19, 19)\n",
      "  Epoch 1/3, Loss=-0.2240\n",
      "  Epoch 2/3, Loss=-0.7140\n",
      "  Epoch 3/3, Loss=-0.0946\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_88.pt\n",
      "Cluster 89 | X=(512, 64, 19, 19)\n",
      "  Epoch 1/3, Loss=-0.5589\n",
      "  Epoch 2/3, Loss=-0.2764\n",
      "  Epoch 3/3, Loss=-0.4801\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_89.pt\n",
      "Cluster 90 | X=(512, 64, 22, 19)\n",
      "  Epoch 1/3, Loss=-0.3435\n",
      "  Epoch 2/3, Loss=-0.4402\n",
      "  Epoch 3/3, Loss=-0.4718\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_90.pt\n",
      "Cluster 91 | X=(512, 64, 18, 19)\n",
      "  Epoch 1/3, Loss=-0.3022\n",
      "  Epoch 2/3, Loss=-0.3791\n",
      "  Epoch 3/3, Loss=-0.4207\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_91.pt\n",
      "Cluster 92 | X=(512, 64, 16, 19)\n",
      "  Epoch 1/3, Loss=-0.2989\n",
      "  Epoch 2/3, Loss=-0.2928\n",
      "  Epoch 3/3, Loss=-0.3534\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_92.pt\n",
      "Cluster 93 | X=(512, 64, 18, 19)\n",
      "  Epoch 1/3, Loss=0.3772\n",
      "  Epoch 2/3, Loss=-0.3868\n",
      "  Epoch 3/3, Loss=-0.3592\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_93.pt\n",
      "Cluster 94 | X=(512, 64, 24, 19)\n",
      "  Epoch 1/3, Loss=-0.1446\n",
      "  Epoch 2/3, Loss=-0.5464\n",
      "  Epoch 3/3, Loss=-0.5075\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_94.pt\n",
      "Cluster 95 | X=(512, 64, 22, 19)\n",
      "  Epoch 1/3, Loss=-0.4695\n",
      "  Epoch 2/3, Loss=-0.4226\n",
      "  Epoch 3/3, Loss=-0.4900\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_95.pt\n",
      "Cluster 96 | X=(512, 64, 16, 19)\n",
      "  Epoch 1/3, Loss=-0.3350\n",
      "  Epoch 2/3, Loss=-0.3316\n",
      "  Epoch 3/3, Loss=-0.3384\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_96.pt\n",
      "Cluster 97 | X=(512, 64, 18, 19)\n",
      "  Epoch 1/3, Loss=-0.9776\n",
      "  Epoch 2/3, Loss=-0.4606\n",
      "  Epoch 3/3, Loss=-0.3922\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_97.pt\n",
      "Cluster 98 | X=(512, 64, 16, 19)\n",
      "  Epoch 1/3, Loss=-0.3678\n",
      "  Epoch 2/3, Loss=-0.3102\n",
      "  Epoch 3/3, Loss=-0.3426\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_98.pt\n",
      "Cluster 99 | X=(512, 64, 15, 19)\n",
      "  Epoch 1/3, Loss=-0.1916\n",
      "  Epoch 2/3, Loss=-0.3929\n",
      "  Epoch 3/3, Loss=-0.3282\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_99.pt\n",
      "Cluster 100 | X=(512, 64, 13, 19)\n",
      "  Epoch 1/3, Loss=-0.5142\n",
      "  Epoch 2/3, Loss=-0.4437\n",
      "  Epoch 3/3, Loss=-0.2434\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_100.pt\n",
      "Cluster 101 | X=(512, 64, 14, 19)\n",
      "  Epoch 1/3, Loss=-0.7956\n",
      "  Epoch 2/3, Loss=-0.5824\n",
      "  Epoch 3/3, Loss=-0.2303\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_101.pt\n",
      "Cluster 102 | X=(512, 64, 12, 19)\n",
      "  Epoch 1/3, Loss=-0.0301\n",
      "  Epoch 2/3, Loss=-0.2037\n",
      "  Epoch 3/3, Loss=-0.2809\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_102.pt\n",
      "Cluster 103 | X=(512, 64, 22, 19)\n",
      "  Epoch 1/3, Loss=-0.2892\n",
      "  Epoch 2/3, Loss=-0.4851\n",
      "  Epoch 3/3, Loss=-0.4663\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_103.pt\n",
      "Cluster 104 | X=(512, 64, 13, 19)\n",
      "  Epoch 1/3, Loss=0.1424\n",
      "  Epoch 2/3, Loss=-0.2099\n",
      "  Epoch 3/3, Loss=-0.2465\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_104.pt\n",
      "Cluster 105 | X=(512, 64, 13, 19)\n",
      "  Epoch 1/3, Loss=-0.4720\n",
      "  Epoch 2/3, Loss=-0.3574\n",
      "  Epoch 3/3, Loss=-0.2921\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_105.pt\n",
      "Cluster 106 | X=(512, 64, 14, 19)\n",
      "  Epoch 1/3, Loss=-0.2352\n",
      "  Epoch 2/3, Loss=-0.3171\n",
      "  Epoch 3/3, Loss=-0.2680\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_106.pt\n",
      "Cluster 107 | X=(512, 64, 12, 19)\n",
      "  Epoch 1/3, Loss=-0.3141\n",
      "  Epoch 2/3, Loss=-0.2019\n",
      "  Epoch 3/3, Loss=-0.2446\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_107.pt\n",
      "Cluster 108 | X=(512, 64, 13, 19)\n",
      "  Epoch 1/3, Loss=-0.2392\n",
      "  Epoch 2/3, Loss=-0.4495\n",
      "  Epoch 3/3, Loss=-0.2110\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_108.pt\n",
      "Cluster 109 | X=(512, 64, 9, 19)\n",
      "  Epoch 1/3, Loss=-0.6104\n",
      "  Epoch 2/3, Loss=-0.1940\n",
      "  Epoch 3/3, Loss=-0.1972\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_109.pt\n",
      "Cluster 110 | X=(512, 64, 9, 19)\n",
      "  Epoch 1/3, Loss=-0.3339\n",
      "  Epoch 2/3, Loss=-0.1327\n",
      "  Epoch 3/3, Loss=-0.1981\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_110.pt\n",
      "Cluster 111 | X=(512, 64, 11, 19)\n",
      "  Epoch 1/3, Loss=-0.3905\n",
      "  Epoch 2/3, Loss=-0.2762\n",
      "  Epoch 3/3, Loss=-0.2258\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_111.pt\n",
      "Cluster 112 | X=(512, 64, 6, 19)\n",
      "  Epoch 1/3, Loss=0.4291\n",
      "  Epoch 2/3, Loss=-0.1949\n",
      "  Epoch 3/3, Loss=-0.1462\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_112.pt\n",
      "Cluster 113 | X=(512, 64, 6, 19)\n",
      "  Epoch 1/3, Loss=-0.0743\n",
      "  Epoch 2/3, Loss=-0.0126\n",
      "  Epoch 3/3, Loss=-0.1880\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_113.pt\n",
      "Cluster 114 | X=(512, 64, 6, 19)\n",
      "  Epoch 1/3, Loss=-0.4774\n",
      "  Epoch 2/3, Loss=0.0843\n",
      "  Epoch 3/3, Loss=-0.2337\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_114.pt\n",
      "Cluster 115 | X=(512, 64, 6, 19)\n",
      "  Epoch 1/3, Loss=-0.2998\n",
      "  Epoch 2/3, Loss=-0.0070\n",
      "  Epoch 3/3, Loss=-0.1864\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_115.pt\n",
      "Cluster 116 | X=(512, 64, 8, 19)\n",
      "  Epoch 1/3, Loss=-0.2676\n",
      "  Epoch 2/3, Loss=-0.0968\n",
      "  Epoch 3/3, Loss=-0.2078\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_116.pt\n",
      "Cluster 117 | X=(512, 64, 5, 19)\n",
      "  Epoch 1/3, Loss=1.0025\n",
      "  Epoch 2/3, Loss=-0.2416\n",
      "  Epoch 3/3, Loss=-0.2608\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_117.pt\n",
      "Cluster 118 | X=(512, 64, 5, 19)\n",
      "  Epoch 1/3, Loss=0.1202\n",
      "  Epoch 2/3, Loss=-0.0775\n",
      "  Epoch 3/3, Loss=-0.1483\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_118.pt\n",
      "Cluster 119 | X=(512, 64, 5, 19)\n",
      "  Epoch 1/3, Loss=0.1959\n",
      "  Epoch 2/3, Loss=-0.0940\n",
      "  Epoch 3/3, Loss=-0.1591\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_119.pt\n",
      "Cluster 120 | X=(512, 64, 5, 19)\n",
      "  Epoch 1/3, Loss=0.5773\n",
      "  Epoch 2/3, Loss=-0.1968\n",
      "  Epoch 3/3, Loss=-0.1273\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_120.pt\n",
      "Cluster 121 | X=(512, 64, 8, 19)\n",
      "  Epoch 1/3, Loss=-0.1357\n",
      "  Epoch 2/3, Loss=-0.1354\n",
      "  Epoch 3/3, Loss=-0.1881\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_121.pt\n",
      "Cluster 122 | X=(512, 64, 6, 19)\n",
      "  Epoch 1/3, Loss=0.7546\n",
      "  Epoch 2/3, Loss=-0.1507\n",
      "  Epoch 3/3, Loss=-0.1279\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_122.pt\n",
      "Cluster 123 | X=(512, 64, 4, 19)\n",
      "  Epoch 1/3, Loss=-0.0078\n",
      "  Epoch 2/3, Loss=-0.1092\n",
      "  Epoch 3/3, Loss=-0.1093\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_123.pt\n",
      "Cluster 124 | X=(512, 64, 5, 19)\n",
      "  Epoch 1/3, Loss=0.7967\n",
      "  Epoch 2/3, Loss=-0.0394\n",
      "  Epoch 3/3, Loss=-0.0865\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_124.pt\n",
      "Cluster 125 | X=(512, 64, 4, 19)\n",
      "  Epoch 1/3, Loss=-0.0368\n",
      "  Epoch 2/3, Loss=-0.0887\n",
      "  Epoch 3/3, Loss=-0.1660\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_125.pt\n",
      "Cluster 126 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=-0.3935\n",
      "  Epoch 2/3, Loss=-0.0500\n",
      "  Epoch 3/3, Loss=0.0225\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_126.pt\n",
      "Cluster 127 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=0.6859\n",
      "  Epoch 2/3, Loss=-0.0847\n",
      "  Epoch 3/3, Loss=-0.2500\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_127.pt\n",
      "Cluster 128 | X=(512, 64, 4, 19)\n",
      "  Epoch 1/3, Loss=-0.1281\n",
      "  Epoch 2/3, Loss=0.0511\n",
      "  Epoch 3/3, Loss=-0.0862\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_128.pt\n",
      "Cluster 129 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=-0.0126\n",
      "  Epoch 2/3, Loss=-0.1328\n",
      "  Epoch 3/3, Loss=-0.0177\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_129.pt\n",
      "Cluster 130 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=0.2724\n",
      "  Epoch 2/3, Loss=-0.0170\n",
      "  Epoch 3/3, Loss=-0.0627\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_130.pt\n",
      "Cluster 131 | X=(512, 64, 5, 19)\n",
      "  Epoch 1/3, Loss=-0.2460\n",
      "  Epoch 2/3, Loss=0.0162\n",
      "  Epoch 3/3, Loss=-0.2378\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_131.pt\n",
      "Cluster 132 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=0.6014\n",
      "  Epoch 2/3, Loss=-0.1678\n",
      "  Epoch 3/3, Loss=-0.2973\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_132.pt\n",
      "Cluster 133 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=-0.5889\n",
      "  Epoch 2/3, Loss=0.0307\n",
      "  Epoch 3/3, Loss=0.0874\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_133.pt\n",
      "Cluster 134 | X=(512, 64, 2, 19)\n",
      "  Epoch 1/3, Loss=0.2909\n",
      "  Epoch 2/3, Loss=-0.0146\n",
      "  Epoch 3/3, Loss=-0.1218\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_134.pt\n",
      "Cluster 135 | X=(512, 64, 2, 19)\n",
      "  Epoch 1/3, Loss=-0.2410\n",
      "  Epoch 2/3, Loss=-0.0207\n",
      "  Epoch 3/3, Loss=0.0105\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_135.pt\n",
      "Cluster 136 | X=(512, 64, 2, 19)\n",
      "  Epoch 1/3, Loss=0.2103\n",
      "  Epoch 2/3, Loss=-0.0243\n",
      "  Epoch 3/3, Loss=-0.0845\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_136.pt\n",
      "Cluster 137 | X=(512, 64, 3, 19)\n",
      "  Epoch 1/3, Loss=-0.2543\n",
      "  Epoch 2/3, Loss=-0.0817\n",
      "  Epoch 3/3, Loss=-0.0210\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_137.pt\n",
      "Cluster 138 | X=(512, 64, 2, 19)\n",
      "  Epoch 1/3, Loss=-0.2255\n",
      "  Epoch 2/3, Loss=-0.0626\n",
      "  Epoch 3/3, Loss=-0.0312\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_138.pt\n",
      "Cluster 139 | X=(512, 64, 2, 19)\n",
      "  Epoch 1/3, Loss=-0.3222\n",
      "  Epoch 2/3, Loss=-0.1677\n",
      "  Epoch 3/3, Loss=-0.0283\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_139.pt\n",
      "Cluster 140 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=0.1418\n",
      "  Epoch 2/3, Loss=0.0041\n",
      "  Epoch 3/3, Loss=-0.0704\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_140.pt\n",
      "Cluster 141 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=0.1560\n",
      "  Epoch 2/3, Loss=0.0607\n",
      "  Epoch 3/3, Loss=-0.0168\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_141.pt\n",
      "Cluster 142 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=0.2465\n",
      "  Epoch 2/3, Loss=0.1405\n",
      "  Epoch 3/3, Loss=0.0513\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_142.pt\n",
      "Cluster 143 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=0.1929\n",
      "  Epoch 2/3, Loss=0.0497\n",
      "  Epoch 3/3, Loss=-0.0227\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_143.pt\n",
      "Cluster 144 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=-0.0496\n",
      "  Epoch 2/3, Loss=-0.0454\n",
      "  Epoch 3/3, Loss=-0.0074\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_144.pt\n",
      "Cluster 145 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=0.0627\n",
      "  Epoch 2/3, Loss=-0.0436\n",
      "  Epoch 3/3, Loss=-0.0750\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_145.pt\n",
      "Cluster 146 | X=(511, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=-0.1636\n",
      "  Epoch 2/3, Loss=-0.0772\n",
      "  Epoch 3/3, Loss=0.0076\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_146.pt\n",
      "Cluster 147 | X=(511, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=-0.0449\n",
      "  Epoch 2/3, Loss=0.0284\n",
      "  Epoch 3/3, Loss=-0.0196\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_147.pt\n",
      "Cluster 148 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=0.0305\n",
      "  Epoch 2/3, Loss=-0.0530\n",
      "  Epoch 3/3, Loss=-0.0388\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_148.pt\n",
      "Cluster 149 | X=(512, 64, 1, 19)\n",
      "  Epoch 1/3, Loss=-0.0056\n",
      "  Epoch 2/3, Loss=0.0320\n",
      "  Epoch 3/3, Loss=-0.0237\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_149.pt\n",
      "✅ Done Block 8: signals saved to ./signals/a3c_signals.csv, models in ./models/\n"
     ]
    }
   ],
   "source": [
    "# Block 8 — A3C multi-stock per-cluster (clean version)\n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIR   = \"./tensors/\"\n",
    "SIG_DIR    = \"./signals/\"\n",
    "MODEL_DIR  = \"./models/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reset signals file\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Loss ---\n",
    "def a3c_loss(logits, values, actions, rewards, beta=0.01):\n",
    "    adv = rewards - values.squeeze(-1)\n",
    "    critic = adv.pow(2).mean()\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    actor = -(logp.gather(1, actions.unsqueeze(1)).squeeze(1) * adv.detach()).mean()\n",
    "    entropy = -(torch.softmax(logits, dim=-1) * logp).sum(-1).mean()\n",
    "    return actor + 0.5*critic - beta*entropy\n",
    "\n",
    "# --- Training & inference ---\n",
    "def process_cluster(meta, epochs=3, lr=1e-3, batch_size=256):\n",
    "    c_id, tickers = meta[\"cluster\"], meta[\"tickers\"]\n",
    "    dates = pd.to_datetime(meta[\"dates\"])\n",
    "    dates_shifted = pd.to_datetime(meta[\"dates_shifted\"])\n",
    "\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # --- Lấy giá để tính reward ---\n",
    "    px = []\n",
    "    for tk in tickers:\n",
    "        s = df_backtest[df_backtest[\"ticker\"]==tk].set_index(\"timestamp\")[\"close\"]\n",
    "        s = s.reindex(dates_shifted).ffill().bfill().values\n",
    "        px.append(s)\n",
    "    px = np.stack(px, axis=1)  # (B,N)\n",
    "    r = np.zeros_like(px, dtype=np.float32)\n",
    "    r[1:] = np.log(px[1:] / np.maximum(px[:-1], 1e-9))  # daily log-return\n",
    "\n",
    "    # --- Model + optimizer ---\n",
    "    model = A3CNet(F).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Mini-batch generator\n",
    "    total = B*N\n",
    "    def iterator():\n",
    "        for start in range(0, total, batch_size):\n",
    "            end = min(total, start+batch_size)\n",
    "            xb, mb, rb, idx = [], [], [], []\n",
    "            for s in range(start,end):\n",
    "                b, n = divmod(s, N)\n",
    "                xb.append(X[b,:,n,:])\n",
    "                mb.append(M[b,:,n,:])\n",
    "                rb.append(r[b,n])\n",
    "                idx.append((b,n))\n",
    "            yield np.stack(xb), np.stack(mb), np.array(rb), idx\n",
    "\n",
    "    # --- Train ---\n",
    "    for ep in range(epochs):\n",
    "        loss_ep = 0\n",
    "        for xb, mb, rb, _ in iterator():\n",
    "            xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "            rb = torch.tensor(rb, dtype=torch.float32).to(device)\n",
    "            xb = xb * torch.tensor(mb, dtype=torch.float32).to(device)\n",
    "\n",
    "            logits, vals = model(xb)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            act = dist.sample()\n",
    "\n",
    "            # Mapping: 0=short, 1=flat, 2=long\n",
    "            reward = torch.where(act==2, rb, torch.where(act==0, -rb, torch.zeros_like(rb)))\n",
    "            loss = a3c_loss(logits, vals, act, reward)\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            loss_ep += loss.item()\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Loss={loss_ep:.4f}\")\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Save model ---\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"  ✅ Saved model checkpoint: {model_path}\")\n",
    "\n",
    "    # --- Inference & save signals ---\n",
    "    with open(SIG_FILE,\"a\",newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for xb, mb, _, idx in iterator():\n",
    "                xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "                xb = xb * torch.tensor(mb, dtype=torch.float32).to(device)\n",
    "                raw_acts = torch.argmax(model(xb)[0], dim=-1).cpu().numpy()  # 0,1,2\n",
    "                acts = np.where(raw_acts==2, 1, np.where(raw_acts==0, -1, 0))  # map -> -1,0,1\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    w.writerow([dates_shifted[b], tickers[n], int(acts[k])])\n",
    "                del xb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, M, px, r, model, opt\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run all clusters ---\n",
    "for meta in tensor_index:\n",
    "    process_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 8: signals saved to {SIG_FILE}, models in {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418f9a1",
   "metadata": {},
   "source": [
    "**Block 9: Suy luận từ mô hình A3C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5e2629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inference] Cluster 0 | X=(512, 64, 15, 19)\n",
      "[Inference] Cluster 1 | X=(512, 64, 14, 19)\n",
      "[Inference] Cluster 2 | X=(512, 64, 20, 19)\n",
      "[Inference] Cluster 3 | X=(512, 64, 21, 19)\n",
      "[Inference] Cluster 4 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 5 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 6 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 7 | X=(512, 64, 22, 19)\n",
      "[Inference] Cluster 8 | X=(512, 64, 22, 19)\n",
      "[Inference] Cluster 9 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 10 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 11 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 12 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 13 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 14 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 15 | X=(512, 64, 32, 19)\n",
      "[Inference] Cluster 16 | X=(512, 64, 29, 19)\n",
      "[Inference] Cluster 17 | X=(512, 64, 31, 19)\n",
      "[Inference] Cluster 18 | X=(512, 64, 29, 19)\n",
      "[Inference] Cluster 19 | X=(512, 64, 31, 19)\n",
      "[Inference] Cluster 20 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 21 | X=(512, 64, 33, 19)\n",
      "[Inference] Cluster 22 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 23 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 24 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 25 | X=(512, 64, 33, 19)\n",
      "[Inference] Cluster 26 | X=(512, 64, 29, 19)\n",
      "[Inference] Cluster 27 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 28 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 29 | X=(512, 64, 32, 19)\n",
      "[Inference] Cluster 30 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 31 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 32 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 33 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 34 | X=(512, 64, 29, 19)\n",
      "[Inference] Cluster 35 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 36 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 37 | X=(512, 64, 32, 19)\n",
      "[Inference] Cluster 38 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 39 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 40 | X=(512, 64, 32, 19)\n",
      "[Inference] Cluster 41 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 42 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 43 | X=(512, 64, 32, 19)\n",
      "[Inference] Cluster 44 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 45 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 46 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 47 | X=(512, 64, 29, 19)\n",
      "[Inference] Cluster 48 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 49 | X=(512, 64, 31, 19)\n",
      "[Inference] Cluster 50 | X=(512, 64, 29, 19)\n",
      "[Inference] Cluster 51 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 52 | X=(512, 64, 31, 19)\n",
      "[Inference] Cluster 53 | X=(512, 64, 30, 19)\n",
      "[Inference] Cluster 54 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 55 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 56 | X=(512, 64, 24, 19)\n",
      "[Inference] Cluster 57 | X=(512, 64, 32, 19)\n",
      "[Inference] Cluster 58 | X=(512, 64, 28, 19)\n",
      "[Inference] Cluster 59 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 60 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 61 | X=(512, 64, 24, 19)\n",
      "[Inference] Cluster 62 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 63 | X=(512, 64, 23, 19)\n",
      "[Inference] Cluster 64 | X=(512, 64, 23, 19)\n",
      "[Inference] Cluster 65 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 66 | X=(512, 64, 27, 19)\n",
      "[Inference] Cluster 67 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 68 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 69 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 70 | X=(512, 64, 23, 19)\n",
      "[Inference] Cluster 71 | X=(512, 64, 24, 19)\n",
      "[Inference] Cluster 72 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 73 | X=(512, 64, 20, 19)\n",
      "[Inference] Cluster 74 | X=(512, 64, 22, 19)\n",
      "[Inference] Cluster 75 | X=(512, 64, 23, 19)\n",
      "[Inference] Cluster 76 | X=(512, 64, 19, 19)\n",
      "[Inference] Cluster 77 | X=(512, 64, 19, 19)\n",
      "[Inference] Cluster 78 | X=(512, 64, 21, 19)\n",
      "[Inference] Cluster 79 | X=(512, 64, 21, 19)\n",
      "[Inference] Cluster 80 | X=(512, 64, 26, 19)\n",
      "[Inference] Cluster 81 | X=(512, 64, 19, 19)\n",
      "[Inference] Cluster 82 | X=(512, 64, 20, 19)\n",
      "[Inference] Cluster 83 | X=(512, 64, 25, 19)\n",
      "[Inference] Cluster 84 | X=(512, 64, 23, 19)\n",
      "[Inference] Cluster 85 | X=(512, 64, 19, 19)\n",
      "[Inference] Cluster 86 | X=(512, 64, 21, 19)\n",
      "[Inference] Cluster 87 | X=(512, 64, 18, 19)\n",
      "[Inference] Cluster 88 | X=(512, 64, 19, 19)\n",
      "[Inference] Cluster 89 | X=(512, 64, 19, 19)\n",
      "[Inference] Cluster 90 | X=(512, 64, 22, 19)\n",
      "[Inference] Cluster 91 | X=(512, 64, 18, 19)\n",
      "[Inference] Cluster 92 | X=(512, 64, 16, 19)\n",
      "[Inference] Cluster 93 | X=(512, 64, 18, 19)\n",
      "[Inference] Cluster 94 | X=(512, 64, 24, 19)\n",
      "[Inference] Cluster 95 | X=(512, 64, 22, 19)\n",
      "[Inference] Cluster 96 | X=(512, 64, 16, 19)\n",
      "[Inference] Cluster 97 | X=(512, 64, 18, 19)\n",
      "[Inference] Cluster 98 | X=(512, 64, 16, 19)\n",
      "[Inference] Cluster 99 | X=(512, 64, 15, 19)\n",
      "[Inference] Cluster 100 | X=(512, 64, 13, 19)\n",
      "[Inference] Cluster 101 | X=(512, 64, 14, 19)\n",
      "[Inference] Cluster 102 | X=(512, 64, 12, 19)\n",
      "[Inference] Cluster 103 | X=(512, 64, 22, 19)\n",
      "[Inference] Cluster 104 | X=(512, 64, 13, 19)\n",
      "[Inference] Cluster 105 | X=(512, 64, 13, 19)\n",
      "[Inference] Cluster 106 | X=(512, 64, 14, 19)\n",
      "[Inference] Cluster 107 | X=(512, 64, 12, 19)\n",
      "[Inference] Cluster 108 | X=(512, 64, 13, 19)\n",
      "[Inference] Cluster 109 | X=(512, 64, 9, 19)\n",
      "[Inference] Cluster 110 | X=(512, 64, 9, 19)\n",
      "[Inference] Cluster 111 | X=(512, 64, 11, 19)\n",
      "[Inference] Cluster 112 | X=(512, 64, 6, 19)\n",
      "[Inference] Cluster 113 | X=(512, 64, 6, 19)\n",
      "[Inference] Cluster 114 | X=(512, 64, 6, 19)\n",
      "[Inference] Cluster 115 | X=(512, 64, 6, 19)\n",
      "[Inference] Cluster 116 | X=(512, 64, 8, 19)\n",
      "[Inference] Cluster 117 | X=(512, 64, 5, 19)\n",
      "[Inference] Cluster 118 | X=(512, 64, 5, 19)\n",
      "[Inference] Cluster 119 | X=(512, 64, 5, 19)\n",
      "[Inference] Cluster 120 | X=(512, 64, 5, 19)\n",
      "[Inference] Cluster 121 | X=(512, 64, 8, 19)\n",
      "[Inference] Cluster 122 | X=(512, 64, 6, 19)\n",
      "[Inference] Cluster 123 | X=(512, 64, 4, 19)\n",
      "[Inference] Cluster 124 | X=(512, 64, 5, 19)\n",
      "[Inference] Cluster 125 | X=(512, 64, 4, 19)\n",
      "[Inference] Cluster 126 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 127 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 128 | X=(512, 64, 4, 19)\n",
      "[Inference] Cluster 129 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 130 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 131 | X=(512, 64, 5, 19)\n",
      "[Inference] Cluster 132 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 133 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 134 | X=(512, 64, 2, 19)\n",
      "[Inference] Cluster 135 | X=(512, 64, 2, 19)\n",
      "[Inference] Cluster 136 | X=(512, 64, 2, 19)\n",
      "[Inference] Cluster 137 | X=(512, 64, 3, 19)\n",
      "[Inference] Cluster 138 | X=(512, 64, 2, 19)\n",
      "[Inference] Cluster 139 | X=(512, 64, 2, 19)\n",
      "[Inference] Cluster 140 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 141 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 142 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 143 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 144 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 145 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 146 | X=(511, 64, 1, 19)\n",
      "[Inference] Cluster 147 | X=(511, 64, 1, 19)\n",
      "[Inference] Cluster 148 | X=(512, 64, 1, 19)\n",
      "[Inference] Cluster 149 | X=(512, 64, 1, 19)\n",
      "✅ Done Block 9: inference signals saved to ./signals/a3c_signals_infer.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 9 — Inference từ checkpoint A3C\n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "SIG_DIR   = \"./signals/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reset signals file\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model định nghĩa lại (giống Block 8) ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Inference function ---\n",
    "def infer_cluster(meta, batch_size=256):\n",
    "    c_id, tickers, dates, dates_shifted = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"], meta[\"dates_shifted\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"[Inference] Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # Load model checkpoint\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️ Model checkpoint not found: {model_path}, skip\")\n",
    "        return\n",
    "    model = A3CNet(F).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Inference & save signals\n",
    "    total = B * N\n",
    "    with open(SIG_FILE, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, total, batch_size):\n",
    "                end = min(total, start+batch_size)\n",
    "                xb, mb, idx = [], [], []\n",
    "                for s in range(start, end):\n",
    "                    b, n = divmod(s, N)\n",
    "                    xb.append(X[b, :, n, :])\n",
    "                    mb.append(M[b, :, n, :])\n",
    "                    idx.append((b, n))\n",
    "                xb = torch.tensor(np.stack(xb), dtype=torch.float32).to(device)\n",
    "                mb = torch.tensor(np.stack(mb), dtype=torch.float32).to(device)\n",
    "\n",
    "                # Áp dụng mask\n",
    "                xb = xb * mb\n",
    "\n",
    "                acts = torch.argmax(model(xb)[0], dim=-1).cpu().numpy() - 1  # (-1,0,1)\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    # dùng dates[b] (ngày cuối window), nhưng reward tính T+1 (dates_shifted)\n",
    "                    w.writerow([dates[b], tickers[n], int(acts[k])])\n",
    "                del xb, mb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, M, model\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run inference all clusters ---\n",
    "for meta in tensor_index:\n",
    "    infer_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 9: inference signals saved to {SIG_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f8af",
   "metadata": {},
   "source": [
    "**Block 10 : Huấn luyện Cluster DDPG (chỉ với trường hợp vị thế long)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94a09d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Block 10-prep done.\n",
      "Train: (490, 1560) (490, 78) from 2023-01-16 00:00:00 to 2024-12-31 00:00:00\n",
      "Test: (176, 1560) (176, 78) from 2025-01-15 00:00:00 to 2025-10-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Block 10-prep — Chuẩn bị dữ liệu DDPG (state & reward arrays)\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR   = \"./tensors/\"\n",
    "SIG_DIR    = \"./signals/\"\n",
    "OUTPUT_DIR = \"./ddpg_prep/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== Hyper-params =====\n",
    "EXECUTION_LAG  = 2\n",
    "STATE_LKBK     = 10\n",
    "MIN_NAMES_PER_CLUSTER = 2\n",
    "\n",
    "# ===== Load artifacts =====\n",
    "# 1. A3C signals (Block 9 output)\n",
    "signals = pd.read_csv(os.path.join(SIG_DIR, \"a3c_signals_infer.csv\"))\n",
    "signals[\"date\"] = pd.to_datetime(signals[\"date\"])\n",
    "\n",
    "# 2. Giá (Block 7.5 output)\n",
    "df_px = df_backtest.rename(columns={\"timestamp\": \"date\"}).copy()\n",
    "df_px[\"date\"] = pd.to_datetime(df_px[\"date\"])\n",
    "px_wide = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "ret_wide = px_wide.pct_change().fillna(0.0)\n",
    "\n",
    "# 3. Map ticker → cluster\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    for tk in meta[\"tickers\"]:\n",
    "        ticker2cluster.setdefault(tk, meta[\"cluster\"])\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "# ===== Align signals & returns =====\n",
    "sig_wide_raw = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\").sort_index()\n",
    "idx_all = ret_wide.index.union(sig_wide_raw.index)\n",
    "ret_wide = ret_wide.reindex(idx_all).fillna(0.0)\n",
    "sig_wide = sig_wide_raw.reindex(idx_all).fillna(0.0)\n",
    "sig_wide_lag = sig_wide.shift(EXECUTION_LAG)\n",
    "\n",
    "tickers = [t for t in ret_wide.columns if t in ticker2cluster.index]\n",
    "ret_wide = ret_wide[tickers].astype(\"float32\")\n",
    "sig_wide_lag = sig_wide_lag[tickers].astype(\"float32\")\n",
    "cluster_of = ticker2cluster.loc[tickers]\n",
    "\n",
    "clusters = sorted(cluster_of.unique().tolist())\n",
    "cluster_members = {c: cluster_of[cluster_of == c].index.tolist() for c in clusters}\n",
    "C = len(clusters)\n",
    "\n",
    "# ===== Helper: build state arrays =====\n",
    "def build_state_arrays(ret_w, sig_lag, start, end, K=STATE_LKBK):\n",
    "    R = ret_w.loc[start:end]\n",
    "    S = sig_lag.loc[start:end]\n",
    "    dates = R.index\n",
    "\n",
    "    act_cols, ret_cols = [], []\n",
    "    ACTIVE_masks = {}\n",
    "    for c in clusters:\n",
    "        tks = cluster_members[c]\n",
    "        if not tks:\n",
    "            act_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ACTIVE_masks[c] = pd.DataFrame(0.0, index=dates, columns=tks)\n",
    "        else:\n",
    "            S_c, R_c = S[tks], R[tks]\n",
    "            active_mask = (S_c > 0).astype(\"float32\")\n",
    "            ACTIVE_masks[c] = active_mask\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0).fillna(0.0)\n",
    "            act_c = active_mask.mean(axis=1).astype(\"float32\")\n",
    "            ret_c = (R_c * w).sum(axis=1).astype(\"float32\")\n",
    "        act_cols.append(act_c.rename(c))\n",
    "        ret_cols.append(ret_c.rename(c))\n",
    "\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "\n",
    "    def stack_lookback(df, K):\n",
    "        mats = []\n",
    "        for k in range(K):\n",
    "            mats.append(df.shift(k).fillna(0.0))\n",
    "        return np.concatenate([m.values[:, :, None] for m in mats], axis=2)\n",
    "\n",
    "    A3 = stack_lookback(act_df, K)\n",
    "    R3 = stack_lookback(cret_df, K)\n",
    "    valid = np.arange(A3.shape[0]) >= (K - 1)\n",
    "    dates2 = dates[valid]\n",
    "    S_mat = np.concatenate([A3[valid].reshape(len(dates2), -1),\n",
    "                            R3[valid].reshape(len(dates2), -1)], axis=1).astype(\"float32\")\n",
    "    R_mat = cret_df.loc[dates2].values.astype(\"float32\")\n",
    "    ACTIVE_masks = {c: ACTIVE_masks[c].loc[dates2] for c in clusters}\n",
    "    return S_mat, R_mat, dates2, ACTIVE_masks\n",
    "\n",
    "# ===== Train/Test split =====\n",
    "TRAIN_START = pd.Timestamp(\"2023-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")\n",
    "TEST_END    = ret_wide.index.max()\n",
    "\n",
    "S_train, R_train, d_train, ACTIVE_train = build_state_arrays(ret_wide, sig_wide_lag, TRAIN_START, TRAIN_END)\n",
    "S_test,  R_test,  d_test,  ACTIVE_test  = build_state_arrays(ret_wide, sig_wide_lag, TEST_START, TEST_END)\n",
    "\n",
    "# ===== Save to disk =====\n",
    "np.save(os.path.join(OUTPUT_DIR,\"S_train.npy\"), S_train)\n",
    "np.save(os.path.join(OUTPUT_DIR,\"R_train.npy\"), R_train)\n",
    "np.save(os.path.join(OUTPUT_DIR,\"S_test.npy\"),  S_test)\n",
    "np.save(os.path.join(OUTPUT_DIR,\"R_test.npy\"),  R_test)\n",
    "\n",
    "pd.Series(d_train).to_csv(os.path.join(OUTPUT_DIR,\"dates_train.csv\"), index=False)\n",
    "pd.Series(d_test).to_csv(os.path.join(OUTPUT_DIR,\"dates_test.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Block 10-prep done.\")\n",
    "print(\"Train:\", S_train.shape, R_train.shape, \"from\", d_train.min(), \"to\", d_train.max())\n",
    "print(\"Test:\",  S_test.shape,  R_test.shape,  \"from\", d_test.min(),  \"to\", d_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfc544a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (490, 1560) (490, 78)\n",
      "[Epoch 1/60] Critic=0.0305 | Actor=-23.3010\n",
      "[Epoch 2/60] Critic=0.0398 | Actor=-26.1175\n",
      "[Epoch 3/60] Critic=0.0699 | Actor=-23.5888\n",
      "[Epoch 4/60] Critic=0.0645 | Actor=-21.8119\n",
      "[Epoch 5/60] Critic=0.0579 | Actor=-20.4562\n",
      "[Epoch 6/60] Critic=0.0515 | Actor=-18.6667\n",
      "[Epoch 7/60] Critic=0.0443 | Actor=-16.4401\n",
      "[Epoch 8/60] Critic=0.0324 | Actor=-14.2746\n",
      "[Epoch 9/60] Critic=0.0313 | Actor=-13.3598\n",
      "[Epoch 10/60] Critic=0.0282 | Actor=-12.9135\n",
      "[Epoch 11/60] Critic=0.0269 | Actor=-11.8459\n",
      "[Epoch 12/60] Critic=0.0287 | Actor=-11.3130\n",
      "[Epoch 13/60] Critic=0.0289 | Actor=-12.0125\n",
      "[Epoch 14/60] Critic=0.0283 | Actor=-9.9463\n",
      "[Epoch 15/60] Critic=0.0271 | Actor=-8.0240\n",
      "[Epoch 16/60] Critic=0.0358 | Actor=-8.6355\n",
      "[Epoch 17/60] Critic=0.0420 | Actor=-8.6133\n",
      "[Epoch 18/60] Critic=0.0409 | Actor=-6.7488\n",
      "[Epoch 19/60] Critic=0.0379 | Actor=-5.4266\n",
      "[Epoch 20/60] Critic=0.0351 | Actor=-4.7666\n",
      "[Epoch 21/60] Critic=0.0345 | Actor=-3.9104\n",
      "[Epoch 22/60] Critic=0.0292 | Actor=-3.0645\n",
      "[Epoch 23/60] Critic=0.0296 | Actor=-2.7861\n",
      "[Epoch 24/60] Critic=0.0269 | Actor=-2.6357\n",
      "[Epoch 25/60] Critic=0.0249 | Actor=-2.3732\n",
      "[Epoch 26/60] Critic=0.0231 | Actor=-2.3460\n",
      "[Epoch 27/60] Critic=0.0208 | Actor=-2.4994\n",
      "[Epoch 28/60] Critic=0.0210 | Actor=-2.5969\n",
      "[Epoch 29/60] Critic=0.0190 | Actor=-2.5679\n",
      "[Epoch 30/60] Critic=0.0178 | Actor=-2.5918\n",
      "[Epoch 31/60] Critic=0.0157 | Actor=-2.6692\n",
      "[Epoch 32/60] Critic=0.0153 | Actor=-2.7594\n",
      "[Epoch 33/60] Critic=0.0123 | Actor=-2.9769\n",
      "[Epoch 34/60] Critic=0.0117 | Actor=-3.0925\n",
      "[Epoch 35/60] Critic=0.0111 | Actor=-2.9079\n",
      "[Epoch 36/60] Critic=0.0114 | Actor=-2.8667\n",
      "[Epoch 37/60] Critic=0.0104 | Actor=-3.0070\n",
      "[Epoch 38/60] Critic=0.0090 | Actor=-2.8460\n",
      "[Epoch 39/60] Critic=0.0124 | Actor=-3.0430\n",
      "[Epoch 40/60] Critic=0.0094 | Actor=-2.9743\n",
      "[Epoch 41/60] Critic=0.0093 | Actor=-3.0097\n",
      "[Epoch 42/60] Critic=0.0081 | Actor=-3.0493\n",
      "[Epoch 43/60] Critic=0.0082 | Actor=-3.1889\n",
      "[Epoch 44/60] Critic=0.0067 | Actor=-3.2992\n",
      "[Epoch 45/60] Critic=0.0060 | Actor=-3.1776\n",
      "[Epoch 46/60] Critic=0.0073 | Actor=-3.3127\n",
      "[Epoch 47/60] Critic=0.0049 | Actor=-3.2396\n",
      "[Epoch 48/60] Critic=0.0055 | Actor=-3.2892\n",
      "[Epoch 49/60] Critic=0.0059 | Actor=-3.2354\n",
      "[Epoch 50/60] Critic=0.0055 | Actor=-3.4513\n",
      "[Epoch 51/60] Critic=0.0057 | Actor=-3.3074\n",
      "[Epoch 52/60] Critic=0.0040 | Actor=-3.4411\n",
      "[Epoch 53/60] Critic=0.0052 | Actor=-3.4509\n",
      "[Epoch 54/60] Critic=0.0042 | Actor=-3.2825\n",
      "[Epoch 55/60] Critic=0.0041 | Actor=-3.0999\n",
      "[Epoch 56/60] Critic=0.0031 | Actor=-3.5161\n",
      "[Epoch 57/60] Critic=0.0027 | Actor=-3.2832\n",
      "[Epoch 58/60] Critic=0.0024 | Actor=-3.3800\n",
      "[Epoch 59/60] Critic=0.0023 | Actor=-3.3876\n",
      "[Epoch 60/60] Critic=0.0026 | Actor=-3.2579\n",
      "✅ Block 10a done. Saved models to ./ddpg_models/\n"
     ]
    }
   ],
   "source": [
    "# Block 10a — Train DDPG Actor–Critic\n",
    "\n",
    "import os, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "PREP_DIR   = \"./ddpg_prep/\"\n",
    "MODEL_DIR  = \"./ddpg_models/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ===== Hyper-params =====\n",
    "EPOCHS     = 60\n",
    "BATCH_SIZE = 64\n",
    "LR_ACTOR   = 1e-4\n",
    "LR_CRITIC  = 5e-4\n",
    "GAMMA      = 0.95\n",
    "TAU        = 1e-2\n",
    "NOISE_STD  = 0.05\n",
    "HIDDEN     = 128\n",
    "SEED       = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ===== Load data =====\n",
    "S_train = np.load(os.path.join(PREP_DIR,\"S_train.npy\"))\n",
    "R_train = np.load(os.path.join(PREP_DIR,\"R_train.npy\"))\n",
    "dates_train = np.loadtxt(os.path.join(PREP_DIR,\"dates_train.csv\"), dtype=str, delimiter=\",\", skiprows=1)\n",
    "\n",
    "s_dim = S_train.shape[1]\n",
    "a_dim = R_train.shape[1]\n",
    "\n",
    "print(\"Train set:\", S_train.shape, R_train.shape)\n",
    "\n",
    "# ===== Define networks =====\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, a_dim)\n",
    "        )\n",
    "    def forward(self, s):\n",
    "        return torch.softmax(self.net(s), dim=-1)  # simplex\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim+a_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s, a], dim=-1))\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, maxlen=20000):\n",
    "        self.buf=[]; self.maxlen=maxlen\n",
    "    def push(self, s,a,r,s2):\n",
    "        if len(self.buf)>=self.maxlen: self.buf.pop(0)\n",
    "        self.buf.append((s,a,r,s2))\n",
    "    def sample(self, bs):\n",
    "        n=min(bs,len(self.buf))\n",
    "        idx=np.random.choice(len(self.buf), n, replace=False)\n",
    "        s,a,r,s2 = zip(*[self.buf[i] for i in idx])\n",
    "        return (np.array(s,np.float32), np.array(a,np.float32),\n",
    "                np.array(r,np.float32).reshape(-1,1), np.array(s2,np.float32))\n",
    "\n",
    "def soft_update(src, tgt, tau):\n",
    "    with torch.no_grad():\n",
    "        for p, tp in zip(src.parameters(), tgt.parameters()):\n",
    "            tp.data.mul_(1-tau); tp.data.add_(tau*p.data)\n",
    "\n",
    "def port_reward(w, r):\n",
    "    return float(np.dot(w, r))\n",
    "\n",
    "# ===== Init models =====\n",
    "actor = Actor(s_dim, a_dim).to(device)\n",
    "critic = Critic(s_dim, a_dim).to(device)\n",
    "t_actor = Actor(s_dim, a_dim).to(device); t_actor.load_state_dict(actor.state_dict())\n",
    "t_critic= Critic(s_dim, a_dim).to(device); t_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "optA = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "optC = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "mse = nn.MSELoss()\n",
    "buf = Buffer()\n",
    "\n",
    "# ===== Training loop =====\n",
    "for ep in range(EPOCHS):\n",
    "    c_loss=a_loss=0.0\n",
    "    for t in range(len(S_train)-1):\n",
    "        s  = torch.from_numpy(S_train[t]).float().to(device).unsqueeze(0)\n",
    "        s2 = torch.from_numpy(S_train[t+1]).float().to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            w = actor(s).cpu().numpy()[0]\n",
    "        # exploration: add noise in logit space\n",
    "        logits = np.log(w+1e-9)+np.random.normal(0,NOISE_STD,size=a_dim)\n",
    "        w_e = np.exp(logits); w_e = (w_e/w_e.sum()).astype(\"float32\")\n",
    "\n",
    "        r = port_reward(w_e, R_train[t])\n",
    "        buf.push(S_train[t], w_e, r, S_train[t+1])\n",
    "\n",
    "        if len(buf.buf) >= BATCH_SIZE:\n",
    "            sb, ab, rb, s2b = buf.sample(BATCH_SIZE)\n",
    "            sb,ab,rb,s2b = (torch.tensor(sb).to(device),\n",
    "                            torch.tensor(ab).to(device),\n",
    "                            torch.tensor(rb).to(device),\n",
    "                            torch.tensor(s2b).to(device))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                a2 = t_actor(s2b)\n",
    "                q2 = t_critic(s2b,a2)\n",
    "                y  = rb + GAMMA*q2\n",
    "\n",
    "            q = critic(sb,ab)\n",
    "            lc = mse(q,y)\n",
    "            optC.zero_grad(); lc.backward(); optC.step()\n",
    "\n",
    "            ap = actor(sb)\n",
    "            la = -critic(sb,ap).mean()\n",
    "            optA.zero_grad(); la.backward(); optA.step()\n",
    "\n",
    "            soft_update(actor,t_actor,TAU)\n",
    "            soft_update(critic,t_critic,TAU)\n",
    "\n",
    "            c_loss += lc.item(); a_loss += la.item()\n",
    "\n",
    "    print(f\"[Epoch {ep+1}/{EPOCHS}] Critic={c_loss:.4f} | Actor={a_loss:.4f}\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Save checkpoint =====\n",
    "torch.save(actor.state_dict(), os.path.join(MODEL_DIR,\"actor.pt\"))\n",
    "torch.save(critic.state_dict(), os.path.join(MODEL_DIR,\"critic.pt\"))\n",
    "print(f\"✅ Block 10a done. Saved models to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a07be0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading price data...\n",
      "Test range available: 2025-01-01 -> 2025-10-02\n",
      "Loading signals...\n",
      "Tickers available in price & mapping: 388\n",
      "State matrix prepared: (176, 1560) dates: 176\n",
      "Loading actor model...\n",
      "\n",
      "=== Running config 1: SL1_TP2 (sl=0.010, tp=0.020) ===\n",
      "Finished SL1_TP2: ROI=127.583%, Sharpe=5.801, MaxDD=-7.540%\n",
      "\n",
      "=== Running config 2: SL1_TP4 (sl=0.010, tp=0.040) ===\n",
      "Finished SL1_TP4: ROI=274.212%, Sharpe=8.173, MaxDD=-6.566%\n",
      "\n",
      "=== Running config 3: SL1_TP6 (sl=0.010, tp=0.060) ===\n",
      "Finished SL1_TP6: ROI=371.965%, Sharpe=8.442, MaxDD=-6.307%\n",
      "\n",
      "=== Running config 4: SL1_TP10 (sl=0.010, tp=0.100) ===\n",
      "Finished SL1_TP10: ROI=401.417%, Sharpe=7.862, MaxDD=-7.590%\n",
      "\n",
      "=== Running config 5: SL2_TP4 (sl=0.020, tp=0.040) ===\n",
      "Finished SL2_TP4: ROI=171.099%, Sharpe=5.930, MaxDD=-12.056%\n",
      "\n",
      "=== Running config 6: SL2_TP6 (sl=0.020, tp=0.060) ===\n",
      "Finished SL2_TP6: ROI=257.940%, Sharpe=6.689, MaxDD=-10.943%\n",
      "\n",
      "=== Running config 7: SL2_TP10 (sl=0.020, tp=0.100) ===\n",
      "Finished SL2_TP10: ROI=313.261%, Sharpe=6.717, MaxDD=-12.890%\n",
      "\n",
      "=== Running config 8: SL3_TP4 (sl=0.030, tp=0.040) ===\n",
      "Finished SL3_TP4: ROI=148.834%, Sharpe=5.226, MaxDD=-14.104%\n",
      "\n",
      "=== Running config 9: SL3_TP6 (sl=0.030, tp=0.060) ===\n",
      "Finished SL3_TP6: ROI=219.777%, Sharpe=5.895, MaxDD=-11.266%\n",
      "\n",
      "=== Running config 10: SL3_TP10 (sl=0.030, tp=0.100) ===\n",
      "Finished SL3_TP10: ROI=275.732%, Sharpe=6.113, MaxDD=-13.498%\n",
      "\n",
      "=== Running config 11: SL5_TP6 (sl=0.050, tp=0.060) ===\n",
      "Finished SL5_TP6: ROI=180.368%, Sharpe=4.875, MaxDD=-14.825%\n",
      "\n",
      "=== Running config 12: SL5_TP10 (sl=0.050, tp=0.100) ===\n",
      "Finished SL5_TP10: ROI=263.746%, Sharpe=5.910, MaxDD=-14.623%\n",
      "\n",
      "=== Grid search complete. Summary saved to: ./backtest_grid/grid_summary.csv\n",
      "     config    sl    tp       ROI       Vol    Sharpe     MaxDD   WinRate  \\\n",
      "0   SL1_TP6  0.01  0.06  3.719650  0.270100  8.441815 -0.063068  0.645714   \n",
      "1   SL1_TP4  0.01  0.04  2.742123  0.236755  8.173172 -0.065660  0.674286   \n",
      "2  SL1_TP10  0.01  0.10  4.014173  0.302312  7.862489 -0.075899  0.605714   \n",
      "3  SL2_TP10  0.02  0.10  3.132609  0.312554  6.716664 -0.128900  0.577143   \n",
      "4   SL2_TP6  0.02  0.06  2.579398  0.281348  6.688665 -0.109426  0.622857   \n",
      "5  SL3_TP10  0.03  0.10  2.757320  0.321302  6.112563 -0.134976  0.571429   \n",
      "6   SL2_TP4  0.02  0.04  1.710989  0.248000  5.929799 -0.120556  0.622857   \n",
      "7  SL5_TP10  0.05  0.10  2.637465  0.324555  5.909699 -0.146230  0.582857   \n",
      "8   SL3_TP6  0.03  0.06  2.197770  0.291998  5.895344 -0.112662  0.588571   \n",
      "9   SL1_TP2  0.01  0.02  1.275829  0.208327  5.800518 -0.075404  0.645714   \n",
      "\n",
      "   final_capital  n_days  \n",
      "0   47054.913636     176  \n",
      "1   37308.970045     176  \n",
      "2   49991.306848     176  \n",
      "3   41202.114993     176  \n",
      "4   35686.595740     176  \n",
      "5   37460.480209     176  \n",
      "6   27028.560914     176  \n",
      "7   36265.522975     176  \n",
      "8   31881.771425     176  \n",
      "9   22690.017356     176  \n"
     ]
    }
   ],
   "source": [
    "# Block 10b — Backtest & Inference (grid-search SL/TP) — FULL (robust version)\n",
    "import os, gc, json, csv, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_DIR     = \"./tensors/\"\n",
    "SIG_DIR      = \"./signals/\"\n",
    "MODEL_DIR    = \"./ddpg_models/\"\n",
    "OUTPUT_DIR   = \"./backtest_grid/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE     = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")   # from Block 9\n",
    "DF_PRICE     = \"./backtest_ddpg/df_backtest.csv\"               # expected path (Open/High/Low/Close)\n",
    "FALLBACK_PRICE = \"vnindex_price_fa_merged.csv\"                # fallback if DF_PRICE missing or incomplete\n",
    "ACTOR_PATH   = os.path.join(MODEL_DIR, \"actor.pt\")             # actor checkpoint from 10a\n",
    "TENSOR_INDEX = os.path.join(DATA_DIR, \"tensor_index.json\")\n",
    "\n",
    "# grid for SL/TP (as fraction, e.g. 0.01 = 1%)\n",
    "GRID_SL = [0.01, 0.02, 0.03, 0.05]   # stop-loss distances\n",
    "GRID_TP = [0.02, 0.04, 0.06, 0.10]   # take-profit distances\n",
    "\n",
    "# other backtest params\n",
    "ENTRY_LAG = 1         # signal at day T -> entry at open T+ENTRY_LAG\n",
    "INIT_CAP   = 10_000.0\n",
    "TOP_K_PER_CLUSTER = 5\n",
    "MIN_NAMES_PER_CLUSTER = 1\n",
    "MAX_HOLD_DAYS = 30     # optional max hold to avoid infinite long holds\n",
    "ENTRY_ON = \"open\"\n",
    "USE_LOG_RET = False    # returns additive or multiplicative\n",
    "\n",
    "# test split\n",
    "TEST_START = pd.Timestamp(\"2025-01-01\")\n",
    "# TEST_END inferred from price\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------- HELPERS -----------------\n",
    "def load_signals(path):\n",
    "    df = pd.read_csv(path)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    elif \"timestamp\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    else:\n",
    "        raise ValueError(\"signals file must have 'date' or 'timestamp' column\")\n",
    "    return df[[\"date\",\"ticker\",\"signal\"]]\n",
    "\n",
    "def compute_stats_from_series(port_series):\n",
    "    ret = port_series.pct_change().dropna()\n",
    "    if len(ret)==0:\n",
    "        return {\"ROI\":0.0,\"Vol\":0.0,\"Sharpe\":0.0,\"MaxDD\":0.0,\"WinRate\":0.0}\n",
    "    roi = port_series.iloc[-1]/port_series.iloc[0]-1.0\n",
    "    vol = ret.std() * (252**0.5)\n",
    "    sharpe = (ret.mean()/ret.std()) * (252**0.5) if ret.std()>0 else 0.0\n",
    "    dd = (port_series/port_series.cummax()-1.0).min()\n",
    "    winrate = (ret>0).mean()\n",
    "    return {\"ROI\":roi, \"Vol\":vol, \"Sharpe\":sharpe, \"MaxDD\":dd, \"WinRate\":winrate}\n",
    "\n",
    "def intraday_check(entry_price, day_high, day_low, sl_pct, tp_pct):\n",
    "    sl_price = entry_price * (1 - sl_pct)\n",
    "    tp_price = entry_price * (1 + tp_pct)\n",
    "    # conservative: stop-first if both hit\n",
    "    if day_low <= sl_price:\n",
    "        return sl_price, \"sl\"\n",
    "    if day_high >= tp_price:\n",
    "        return tp_price, \"tp\"\n",
    "    return None, \"hold\"\n",
    "\n",
    "# ----------------- Load price data (with fallback) -----------------\n",
    "print(\"Loading price data...\")\n",
    "if not os.path.exists(DF_PRICE):\n",
    "    print(f\"Warning: {DF_PRICE} not found, trying fallback {FALLBACK_PRICE}\")\n",
    "    if not os.path.exists(FALLBACK_PRICE):\n",
    "        raise FileNotFoundError(f\"Neither {DF_PRICE} nor fallback {FALLBACK_PRICE} found. Put OHLC data into {DF_PRICE}.\")\n",
    "    df_px = pd.read_csv(FALLBACK_PRICE, parse_dates=[\"timestamp\"])\n",
    "else:\n",
    "    df_px = pd.read_csv(DF_PRICE, parse_dates=[\"timestamp\"])\n",
    "\n",
    "# standardize column name\n",
    "if \"timestamp\" in df_px.columns:\n",
    "    df_px = df_px.rename(columns={\"timestamp\":\"date\"})\n",
    "df_px[\"date\"] = pd.to_datetime(df_px[\"date\"])\n",
    "\n",
    "# ensure required OHLC columns\n",
    "for c in [\"open\",\"high\",\"low\",\"close\"]:\n",
    "    if c not in df_px.columns:\n",
    "        raise ValueError(f\"Price Data must contain '{c}' column (found columns: {df_px.columns.tolist()})\")\n",
    "\n",
    "# pivot wide\n",
    "px_wide_open = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"open\").sort_index()\n",
    "px_wide_high = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"high\").sort_index()\n",
    "px_wide_low  = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"low\").sort_index()\n",
    "px_wide_close= df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "\n",
    "# align full calendar and fill small gaps\n",
    "idx_all = px_wide_close.index\n",
    "px_wide_open  = px_wide_open.reindex(idx_all).ffill().bfill()\n",
    "px_wide_high  = px_wide_high.reindex(idx_all).ffill().bfill()\n",
    "px_wide_low   = px_wide_low.reindex(idx_all).ffill().bfill()\n",
    "px_wide_close = px_wide_close.reindex(idx_all).ffill().bfill()\n",
    "\n",
    "TEST_END = px_wide_close.index.max()\n",
    "print(\"Test range available:\", TEST_START.date(), \"->\", TEST_END.date())\n",
    "\n",
    "# ----------------- Load signals & tensor_index -----------------\n",
    "print(\"Loading signals...\")\n",
    "signals = load_signals(SIG_FILE)\n",
    "\n",
    "with open(TENSOR_INDEX,\"r\") as fh:\n",
    "    tensor_index = json.load(fh)\n",
    "\n",
    "# build ticker->cluster mapping (first seen)\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    c = int(meta[\"cluster\"])\n",
    "    for tk in meta[\"tickers\"]:\n",
    "        if tk not in ticker2cluster:\n",
    "            ticker2cluster[tk] = c\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "# restrict to tickers present in price & mapping\n",
    "tickers = sorted(list(set(px_wide_close.columns).intersection(set(ticker2cluster.index))))\n",
    "print(f\"Tickers available in price & mapping: {len(tickers)}\")\n",
    "px_wide_open  = px_wide_open.reindex(columns=tickers)\n",
    "px_wide_high  = px_wide_high.reindex(columns=tickers)\n",
    "px_wide_low   = px_wide_low.reindex(columns=tickers)\n",
    "px_wide_close = px_wide_close.reindex(columns=tickers)\n",
    "\n",
    "# signals -> wide (last signal per ticker/day)\n",
    "sig_wide = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\").reindex(idx_all).fillna(0.0)\n",
    "# shift for entry lag\n",
    "sig_entry = sig_wide.shift(ENTRY_LAG).fillna(0.0)\n",
    "\n",
    "# build clusters\n",
    "clusters = sorted(list(set(ticker2cluster.loc[tickers].values)))\n",
    "cluster_members = {c: [tk for tk in tickers if ticker2cluster.get(tk)==c] for c in clusters}\n",
    "\n",
    "# ---------- Build cluster-level state arrays (for actor input) ----------\n",
    "STATE_LKBK = 10\n",
    "\n",
    "def build_state_arrays_for_backtest(sig_lag, ret_wide, clusters, cluster_members, K=STATE_LKBK, start=None, end=None):\n",
    "    R = ret_wide.loc[start:end]\n",
    "    S = sig_lag.loc[start:end]\n",
    "    dates = R.index\n",
    "\n",
    "    act_cols, ret_cols = [], []\n",
    "    ACTIVE_masks = {}\n",
    "    for c in clusters:\n",
    "        tks = cluster_members.get(c, [])\n",
    "        if len(tks)==0:\n",
    "            act_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ACTIVE_masks[c] = pd.DataFrame(0.0,index=dates,columns=tks)\n",
    "        else:\n",
    "            S_c = S.reindex(columns=tks).fillna(0.0)\n",
    "            R_c = R.reindex(columns=tks).fillna(0.0)\n",
    "            active_mask = (S_c > 0).astype(\"float32\")\n",
    "            ACTIVE_masks[c] = active_mask\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0).fillna(0.0)\n",
    "            act_c = active_mask.mean(axis=1).astype(\"float32\")\n",
    "            ret_c = (R_c * w).sum(axis=1).astype(\"float32\")\n",
    "        act_cols.append(act_c.rename(c))\n",
    "        ret_cols.append(ret_c.rename(c))\n",
    "\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "\n",
    "    def stack_lookback(df, K):\n",
    "        mats = []\n",
    "        for k in range(K):\n",
    "            mats.append(df.shift(k).fillna(0.0))\n",
    "        return np.concatenate([m.values[:,:,None] for m in mats], axis=2)\n",
    "\n",
    "    A3 = stack_lookback(act_df, K)\n",
    "    R3 = stack_lookback(cret_df, K)\n",
    "\n",
    "    valid = np.arange(A3.shape[0]) >= (K-1)\n",
    "    A3 = A3[valid]; R3 = R3[valid]\n",
    "    dates2 = dates[valid]\n",
    "\n",
    "    S_mat = np.concatenate([A3.reshape(len(dates2), -1), R3.reshape(len(dates2), -1)], axis=1).astype(\"float32\")\n",
    "    R_mat = cret_df.loc[dates2].values.astype(\"float32\")\n",
    "    ACTIVE_masks = {c: ACTIVE_masks[c].loc[dates2] for c in clusters}\n",
    "    return S_mat, R_mat, dates2, ACTIVE_masks\n",
    "\n",
    "# compute returns wide\n",
    "ret_wide = px_wide_close.pct_change().fillna(0.0)\n",
    "\n",
    "S_all, R_all, dates_all, ACTIVE_all = build_state_arrays_for_backtest(sig_entry, ret_wide, clusters, cluster_members, K=STATE_LKBK, start=TEST_START, end=TEST_END)\n",
    "print(\"State matrix prepared:\", S_all.shape, \"dates:\", len(dates_all))\n",
    "s_dim = S_all.shape[1]; a_dim = len(clusters)\n",
    "\n",
    "# ---------- Load actor (cluster-level) ----------\n",
    "if not os.path.exists(ACTOR_PATH):\n",
    "    raise FileNotFoundError(f\"Actor model not found at {ACTOR_PATH}. Run Block 10a first.\")\n",
    "print(\"Loading actor model...\")\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, a_dim)\n",
    "        )\n",
    "    def forward(self, s):\n",
    "        return torch.softmax(self.net(s), dim=-1)\n",
    "\n",
    "actor = ActorNet(s_dim, a_dim).to(DEVICE)\n",
    "actor.load_state_dict(torch.load(ACTOR_PATH, map_location=DEVICE))\n",
    "actor.eval()\n",
    "\n",
    "# ---------- Grid-search loop ----------\n",
    "summary_rows = []\n",
    "cfg_id = 0\n",
    "\n",
    "for sl in GRID_SL:\n",
    "    for tp in GRID_TP:\n",
    "        if tp <= sl: continue\n",
    "        cfg_id += 1\n",
    "        cfg_name = f\"SL{int(sl*100)}_TP{int(tp*100)}\"\n",
    "        print(f\"\\n=== Running config {cfg_id}: {cfg_name} (sl={sl:.3f}, tp={tp:.3f}) ===\")\n",
    "\n",
    "        out_dir = os.path.join(OUTPUT_DIR, cfg_name)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        dates = dates_all\n",
    "        n_days = len(dates)\n",
    "        capital = INIT_CAP\n",
    "        port_vals = pd.Series(index=dates, dtype=\"float64\")\n",
    "        port_vals.iloc[0] = capital\n",
    "\n",
    "        prev_weights = pd.Series(0.0,index=tickers)\n",
    "        pos_info = {}  # ticker -> {entry_price, entry_date, weight, hold_days}\n",
    "        turnover_series = pd.Series(0.0,index=dates)\n",
    "        daily_returns = pd.Series(0.0,index=dates)\n",
    "\n",
    "        # compute cluster weights\n",
    "        cluster_weights = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_days):\n",
    "                s = torch.from_numpy(S_all[i:i+1]).float().to(DEVICE)\n",
    "                w_c = actor(s).cpu().numpy()[0].astype(\"float32\")\n",
    "                w_c = np.clip(w_c, 0, None)\n",
    "                if w_c.sum() <= 1e-9:\n",
    "                    w_c = np.ones_like(w_c)/len(w_c)\n",
    "                else:\n",
    "                    w_c = w_c / w_c.sum()\n",
    "                cluster_weights.append(w_c)\n",
    "        cluster_weights = np.stack(cluster_weights, axis=0)  # (n_days, C)\n",
    "\n",
    "        momentum_window = 5\n",
    "        for i, dt in enumerate(dates):\n",
    "            w_c = cluster_weights[i]\n",
    "\n",
    "            # build target ticker weights\n",
    "            target_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "            for j, c in enumerate(clusters):\n",
    "                members = cluster_members.get(c, [])\n",
    "                if not members: continue\n",
    "                # candidates: active signal > 0 (sig_entry)\n",
    "                candidates = [tk for tk in members if (tk in sig_entry.columns and sig_entry.loc[dt, tk] > 0)]\n",
    "                if len(candidates) < MIN_NAMES_PER_CLUSTER:\n",
    "                    continue\n",
    "                # momentum ranking\n",
    "                mom_scores = {}\n",
    "                for tk in candidates:\n",
    "                    try:\n",
    "                        idx_dt = px_wide_close.index.get_loc(dt)\n",
    "                        start_idx = max(0, idx_dt-momentum_window)\n",
    "                        start_date = px_wide_close.index[start_idx]\n",
    "                        vstart = px_wide_close.loc[start_date, tk]\n",
    "                        vend = px_wide_close.loc[dt, tk]\n",
    "                        r = (vend / vstart - 1.0) if vstart>0 else 0.0\n",
    "                    except Exception:\n",
    "                        r = 0.0\n",
    "                    mom_scores[tk] = r\n",
    "                topk = sorted(mom_scores.keys(), key=lambda x: mom_scores[x], reverse=True)[:TOP_K_PER_CLUSTER]\n",
    "                if len(topk)==0: continue\n",
    "                share = float(w_c[j]) / len(topk)\n",
    "                for tk in topk:\n",
    "                    target_w_ticker[tk] += share\n",
    "\n",
    "            # normalize\n",
    "            ssum = target_w_ticker.sum()\n",
    "            if ssum <= 1e-12:\n",
    "                target_w_ticker[:] = 1.0/len(target_w_ticker)\n",
    "            else:\n",
    "                target_w_ticker = target_w_ticker / ssum\n",
    "\n",
    "            turnover = float(np.sum(np.abs(target_w_ticker.values - prev_weights.values)))\n",
    "            turnover_series.iloc[i] = turnover\n",
    "\n",
    "            # Intraday SL/TP check for existing positions\n",
    "            realized_pnl = 0.0\n",
    "            closed_tickers = []\n",
    "            for tk, info in list(pos_info.items()):\n",
    "                entry_price = info[\"entry_price\"]\n",
    "                day_high = px_wide_high.loc[dt, tk]\n",
    "                day_low  = px_wide_low.loc[dt, tk]\n",
    "                exit_price, exit_type = intraday_check(entry_price, day_high, day_low, sl, tp)\n",
    "                if exit_price is not None:\n",
    "                    wpos = info[\"weight\"]\n",
    "                    pnl = wpos * ((exit_price / entry_price) - 1.0)\n",
    "                    realized_pnl += pnl\n",
    "                    closed_tickers.append(tk)\n",
    "                    del pos_info[tk]\n",
    "\n",
    "            # New entries / rebalancing executed at open today\n",
    "            new_entries = []\n",
    "            for tk in tickers:\n",
    "                new_w = float(target_w_ticker[tk])\n",
    "                old_w = float(prev_weights.get(tk, 0.0))\n",
    "                if new_w > 0 and (tk not in pos_info or abs(new_w - old_w) > 1e-6):\n",
    "                    entry_price = px_wide_open.loc[dt, tk]\n",
    "                    pos_info[tk] = {\"entry_price\": entry_price, \"entry_date\": dt, \"weight\": new_w, \"hold_days\": 0}\n",
    "                    new_entries.append(tk)\n",
    "                else:\n",
    "                    if tk in pos_info:\n",
    "                        pos_info[tk][\"weight\"] = new_w\n",
    "\n",
    "            prev_weights = target_w_ticker.copy()\n",
    "\n",
    "            # Compute day return:\n",
    "            # - for new entries: open->close\n",
    "            # - for existing: close / prev_close -1\n",
    "            day_ret = 0.0\n",
    "            prev_close_all = px_wide_close.shift(1).loc[dt]\n",
    "            close_today = px_wide_close.loc[dt]\n",
    "            for tk in tickers:\n",
    "                w = float(prev_weights[tk])\n",
    "                if math.isclose(w, 0.0): continue\n",
    "                if tk in new_entries:\n",
    "                    op = px_wide_open.loc[dt, tk]\n",
    "                    cl = close_today[tk]\n",
    "                    ret = (cl / op - 1.0) if op>0 else 0.0\n",
    "                else:\n",
    "                    prev_c = prev_close_all[tk]\n",
    "                    cl = close_today[tk]\n",
    "                    ret = (cl / prev_c - 1.0) if prev_c>0 else 0.0\n",
    "                day_ret += w * ret\n",
    "\n",
    "            total_ret = day_ret + realized_pnl\n",
    "            COST_BPS = 30\n",
    "            fee = (COST_BPS / 1e4) * turnover\n",
    "            total_ret_net = total_ret - fee\n",
    "\n",
    "            capital = capital * (1.0 + total_ret_net)\n",
    "            port_vals.iloc[i] = capital\n",
    "            daily_returns.iloc[i] = total_ret_net\n",
    "\n",
    "            # update hold days & forced close if exceed MAX_HOLD_DAYS\n",
    "            for tk in list(pos_info.keys()):\n",
    "                pos_info[tk][\"hold_days\"] = pos_info[tk].get(\"hold_days\",0) + 1\n",
    "                if pos_info[tk][\"hold_days\"] >= MAX_HOLD_DAYS:\n",
    "                    # we close at close price (already mark-to-market applied), remove position\n",
    "                    del pos_info[tk]\n",
    "\n",
    "            if (i % 50) == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        # end daily loop\n",
    "\n",
    "        stats = compute_stats_from_series(port_vals)\n",
    "        port_vals.to_csv(os.path.join(out_dir, \"portfolio_value.csv\"))\n",
    "        daily_returns.to_csv(os.path.join(out_dir, \"daily_returns.csv\"))\n",
    "        turnover_series.to_csv(os.path.join(out_dir, \"turnover.csv\"))\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"config\": cfg_name,\n",
    "            \"sl\": sl,\n",
    "            \"tp\": tp,\n",
    "            \"ROI\": stats[\"ROI\"],\n",
    "            \"Vol\": stats[\"Vol\"],\n",
    "            \"Sharpe\": stats[\"Sharpe\"],\n",
    "            \"MaxDD\": stats[\"MaxDD\"],\n",
    "            \"WinRate\": stats[\"WinRate\"],\n",
    "            \"final_capital\": float(port_vals.dropna().iloc[-1]) if port_vals.dropna().size>0 else float(capital),\n",
    "            \"n_days\": int(port_vals.dropna().shape[0])\n",
    "        })\n",
    "\n",
    "        print(f\"Finished {cfg_name}: ROI={stats['ROI']:.3%}, Sharpe={stats['Sharpe']:.3f}, MaxDD={stats['MaxDD']:.3%}\")\n",
    "        pd.DataFrame(summary_rows).to_csv(os.path.join(OUTPUT_DIR, \"grid_summary_partial.csv\"), index=False)\n",
    "\n",
    "# final summary\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df = summary_df.sort_values(by=\"Sharpe\", ascending=False).reset_index(drop=True)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, \"grid_summary.csv\"), index=False)\n",
    "print(\"\\n=== Grid search complete. Summary saved to:\", os.path.join(OUTPUT_DIR, \"grid_summary.csv\"))\n",
    "print(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ea5e2",
   "metadata": {},
   "source": [
    "**code mới nhất nằm ngày trên**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e0e1c",
   "metadata": {},
   "source": [
    "**BLOCK 10.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc5b470",
   "metadata": {},
   "source": [
    "**Block 11: Thống kê kết quả và vẽ biểu đồ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11a — Hiệu suất & Stress Test (Auto chọn Sharpe tốt nhất từ Grid)\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "OUTPUT_DIR   = \"./backtest_grid/\"\n",
    "BEST_DIR     = \"./backtest_best/\"\n",
    "os.makedirs(BEST_DIR, exist_ok=True)\n",
    "\n",
    "GRID_SUMMARY = os.path.join(OUTPUT_DIR, \"grid_summary.csv\")\n",
    "if not os.path.exists(GRID_SUMMARY):\n",
    "    raise FileNotFoundError(\"Chưa có grid_summary.csv từ Block 10B\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    peak = series.cummax()\n",
    "    dd = (series / peak) - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def compute_stats(port_val: pd.Series, bench_val: pd.Series | None = None):\n",
    "    port_ret = port_val.pct_change().fillna(0.0)\n",
    "    stats = {\n",
    "        \"Ngày bắt đầu\": port_val.index.min().strftime(\"%Y-%m-%d\"),\n",
    "        \"Ngày kết thúc\": port_val.index.max().strftime(\"%Y-%m-%d\"),\n",
    "        \"Giá trị cuối\": float(port_val.iloc[-1]),\n",
    "        \"ROI (%)\": float((port_val.iloc[-1] / port_val.iloc[0] - 1.0) * 100),\n",
    "        \"Biến động (năm, %)\": float(port_ret.std() * np.sqrt(252) * 100),\n",
    "        \"Sharpe\": float((port_ret.mean() / port_ret.std()) * np.sqrt(252)) if port_ret.std() > 0 else 0,\n",
    "        \"Sortino\": float((port_ret.mean() / port_ret[port_ret < 0].std()) * np.sqrt(252)) if port_ret[port_ret < 0].std() > 0 else 0,\n",
    "        \"MaxDrawdown (%)\": float(max_drawdown(port_val) * 100),\n",
    "        \"Tỷ lệ phiên thắng (%)\": float((port_ret > 0).mean() * 100),\n",
    "        \"Số ngày\": int(len(port_ret))\n",
    "    }\n",
    "    if bench_val is not None and len(bench_val) > 1:\n",
    "        stats.update({\n",
    "            \"Giá trị cuối (VNINDEX)\": float(bench_val.iloc[-1]),\n",
    "            \"ROI VNINDEX (%)\": float((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100),\n",
    "            \"Chênh lệch so với VNINDEX (pp)\": stats[\"ROI (%)\"] - ((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100)\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "def plot_equity(port_val, bench_val, title, path):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(port_val, label=\"Chiến lược\", linewidth=1.6)\n",
    "    if bench_val is not None:\n",
    "        plt.plot(bench_val, label=\"VNINDEX\", linewidth=1.2)\n",
    "    plt.title(title); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "# ---------- 1. Chọn config Sharpe cao nhất ----------\n",
    "summary = pd.read_csv(GRID_SUMMARY)\n",
    "best_cfg = summary.sort_values(by=\"Sharpe\", ascending=False).iloc[0]\n",
    "best_name = best_cfg[\"config\"]\n",
    "print(f\"🏆 Config tốt nhất: {best_name} | Sharpe={best_cfg['Sharpe']:.3f}, ROI={best_cfg['ROI']:.2%}\")\n",
    "\n",
    "cfg_dir = os.path.join(OUTPUT_DIR, best_name)\n",
    "port_path = os.path.join(cfg_dir, \"portfolio_value.csv\")\n",
    "daily_path = os.path.join(cfg_dir, \"daily_returns.csv\")\n",
    "\n",
    "# ---------- 2. Load dữ liệu ----------\n",
    "port_val = pd.read_csv(port_path, index_col=0, parse_dates=True).iloc[:,0].sort_index()\n",
    "daily_returns = pd.read_csv(daily_path, index_col=0, parse_dates=True).iloc[:,0].sort_index()\n",
    "\n",
    "# benchmark từ df_backtest\n",
    "df_backtest_path = \"./backtest_ddpg/df_backtest.csv\"\n",
    "if not os.path.exists(df_backtest_path):\n",
    "    raise FileNotFoundError(\"df_backtest.csv chưa sẵn sàng\")\n",
    "df_backtest = pd.read_csv(df_backtest_path, parse_dates=[\"timestamp\"])\n",
    "df_backtest = df_backtest.groupby([\"timestamp\",\"ticker\"],as_index=False).agg({\"close\":\"last\"})\n",
    "px = df_backtest.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "bench_val = px[\"VNINDEX\"].reindex(port_val.index).ffill().bfill() if \"VNINDEX\" in px.columns else None\n",
    "bench_ret = bench_val.pct_change().fillna(0.0) if bench_val is not None else None\n",
    "\n",
    "# ---------- 3. Compute Stats ----------\n",
    "stats_test = compute_stats(port_val, bench_val)\n",
    "\n",
    "# ---------- 4. Plot cơ bản ----------\n",
    "plot_equity(port_val, bench_val, f\"Equity Curve ({best_name})\", os.path.join(BEST_DIR,\"equity.png\"))\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.hist(daily_returns.dropna(), bins=50, alpha=0.6, label=\"Chiến lược\")\n",
    "if bench_ret is not None:\n",
    "    plt.hist(bench_ret.dropna(), bins=50, alpha=0.6, label=\"VNINDEX\")\n",
    "plt.title(\"Histogram lợi nhuận ngày\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(BEST_DIR,\"hist.png\")); plt.close()\n",
    "\n",
    "# ---------- 5. Stress Test (ví dụ Trump 46%) ----------\n",
    "stress_start, stress_end = pd.Timestamp(\"2025-03-26\"), pd.Timestamp(\"2025-04-15\")\n",
    "sub_port = port_val.loc[stress_start:stress_end]\n",
    "sub_bench = bench_val.loc[stress_start:stress_end] if bench_val is not None else None\n",
    "stats_stress = {}\n",
    "if len(sub_port) > 1:\n",
    "    stats_stress = compute_stats(sub_port, sub_bench)\n",
    "    plot_equity(sub_port, sub_bench, f\"Stress Test ({best_name})\", os.path.join(BEST_DIR,\"equity_stress.png\"))\n",
    "\n",
    "# ---------- 6. Save ----------\n",
    "all_stats={\"BestConfig\":stats_test}\n",
    "if stats_stress: all_stats[\"StressTest\"]=stats_stress\n",
    "pd.DataFrame(all_stats).T.to_csv(os.path.join(BEST_DIR,\"stats.csv\"))\n",
    "\n",
    "print(\"\\n📊 Kết quả config tốt nhất:\"); print(pd.Series(stats_test))\n",
    "if stats_stress: \n",
    "    print(\"\\n📊 Stress Test:\"); print(pd.Series(stats_stress))\n",
    "print(f\"\\n✅ Block 11a hoàn tất. Lưu kết quả tại {BEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3329dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Config Sharpe cao nhất: SL1_TP6 | Sharpe=8.442, ROI=371.97%\n",
      "📈 Fetching VNINDEX for benchmark (buy & hold)...\n",
      "Fetching data, it may take a while. Please wait...\n",
      "📊 Kết quả Test:\n",
      "Ngày bắt đầu                        2025-01-15\n",
      "Ngày kết thúc                       2025-10-02\n",
      "Giá trị cuối                      47054.913636\n",
      "ROI (%)                             371.965031\n",
      "Biến động (năm, %)                    26.95445\n",
      "Sharpe                                8.411143\n",
      "Sortino                              21.632261\n",
      "MaxDrawdown (%)                      -6.306809\n",
      "Tỷ lệ phiên thắng (%)                64.204545\n",
      "Số ngày                                    176\n",
      "Giá trị cuối (VNINDEX)            13369.493116\n",
      "ROI VNINDEX (%)                      33.694931\n",
      "Chênh lệch so với VNINDEX (pp)        338.2701\n",
      "dtype: object\n",
      "\n",
      "📊 Stress Test (Trump thông báo đánh thuế 46% hàng hóa Việt Nam):\n",
      "Ngày bắt đầu                        2025-03-26\n",
      "Ngày kết thúc                       2025-04-15\n",
      "Giá trị cuối                      13263.783711\n",
      "ROI (%)                               8.410838\n",
      "Biến động (năm, %)                   38.946031\n",
      "Sharpe                                3.920345\n",
      "Sortino                               8.054535\n",
      "MaxDrawdown (%)                      -4.264895\n",
      "Tỷ lệ phiên thắng (%)                57.142857\n",
      "Số ngày                                     14\n",
      "Giá trị cuối (VNINDEX)             9932.129625\n",
      "ROI VNINDEX (%)                       -7.41277\n",
      "Chênh lệch so với VNINDEX (pp)       15.823608\n",
      "dtype: object\n",
      "\n",
      "✅ Block 11 hoàn tất. Stats lưu tại: ./backtest_ddpg/stats_test.csv, Charts in ./backtest_ddpg/\n"
     ]
    }
   ],
   "source": [
    "# Block 11 — Hiệu suất & Stress Test (VNINDEX Benchmark)\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from FiinQuantX import FiinSession   # <-- cần package này\n",
    "\n",
    "GRID_DIR   = \"./backtest_grid/\"\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "STATS_FILE = os.path.join(OUTPUT_DIR, \"stats_test.csv\")\n",
    "\n",
    "INIT_CAPITAL = 10_000.0\n",
    "BENCHMARK_TKR = \"VNINDEX\"\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    peak = series.cummax()\n",
    "    dd = (series / peak) - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def compute_stats(port_val: pd.Series, bench_val: pd.Series | None = None):\n",
    "    port_ret = port_val.pct_change().fillna(0.0)\n",
    "    stats = {\n",
    "        \"Ngày bắt đầu\": port_val.index.min().strftime(\"%Y-%m-%d\"),\n",
    "        \"Ngày kết thúc\": port_val.index.max().strftime(\"%Y-%m-%d\"),\n",
    "        \"Giá trị cuối\": float(port_val.iloc[-1]),\n",
    "        \"ROI (%)\": float((port_val.iloc[-1] / port_val.iloc[0] - 1.0) * 100),\n",
    "        \"Biến động (năm, %)\": float(port_ret.std() * np.sqrt(252) * 100),\n",
    "        \"Sharpe\": float((port_ret.mean() / port_ret.std()) * np.sqrt(252)) if port_ret.std() > 0 else 0,\n",
    "        \"Sortino\": float((port_ret.mean() / port_ret[port_ret < 0].std()) * np.sqrt(252)) if port_ret[port_ret < 0].std() > 0 else 0,\n",
    "        \"MaxDrawdown (%)\": float(max_drawdown(port_val) * 100),\n",
    "        \"Tỷ lệ phiên thắng (%)\": float((port_ret > 0).mean() * 100),\n",
    "        \"Số ngày\": int(len(port_ret))\n",
    "    }\n",
    "    if bench_val is not None and len(bench_val) > 1:\n",
    "        stats.update({\n",
    "            \"Giá trị cuối (VNINDEX)\": float(bench_val.iloc[-1]),\n",
    "            \"ROI VNINDEX (%)\": float((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100),\n",
    "            \"Chênh lệch so với VNINDEX (pp)\": stats[\"ROI (%)\"] - ((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100)\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "# ---------- 1. Chọn config Sharpe cao nhất ----------\n",
    "summary_path = os.path.join(GRID_DIR, \"grid_summary.csv\")\n",
    "if not os.path.exists(summary_path):\n",
    "    raise FileNotFoundError(\"grid_summary.csv chưa có. Hãy chạy Block 10B trước.\")\n",
    "\n",
    "summary = pd.read_csv(summary_path)\n",
    "best_cfg = summary.sort_values(by=\"Sharpe\", ascending=False).iloc[0]\n",
    "best_name = best_cfg[\"config\"]\n",
    "print(f\"🏆 Config Sharpe cao nhất: {best_name} | Sharpe={best_cfg['Sharpe']:.3f}, ROI={best_cfg['ROI']:.2%}\")\n",
    "\n",
    "cfg_dir = os.path.join(GRID_DIR, best_name)\n",
    "port_val = pd.read_csv(os.path.join(cfg_dir, \"portfolio_value.csv\"), index_col=0, parse_dates=True).iloc[:,0].sort_index()\n",
    "returns = port_val.pct_change().fillna(0.0)\n",
    "\n",
    "# ---------- 2. Fetch Benchmark VNINDEX ----------\n",
    "print(\"📈 Fetching VNINDEX for benchmark (buy & hold)...\")\n",
    "client = FiinSession(username=\"DSTC_18@fiinquant.vn\", password=\"Fiinquant0606\").login()\n",
    "bench = client.Fetch_Trading_Data(\n",
    "    realtime=False, tickers=BENCHMARK_TKR, fields=['close'],\n",
    "    adjusted=True, by=\"1d\", from_date=str(port_val.index.min().date())\n",
    ").get_data()\n",
    "bench[\"date\"] = pd.to_datetime(bench[\"timestamp\"])\n",
    "bench = bench.set_index(\"date\")[\"close\"].sort_index().reindex(port_val.index).ffill().bfill()\n",
    "bench_ret = bench.pct_change().fillna(0.0)\n",
    "bench_val = (1 + bench_ret).cumprod() * INIT_CAPITAL\n",
    "\n",
    "# ---------- 3. Stats toàn kỳ ----------\n",
    "stats_test = compute_stats(port_val, bench_val)\n",
    "\n",
    "# Equity\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(port_val, label=\"Chiến lược\", linewidth=1.6)\n",
    "plt.plot(bench_val, label=\"VNINDEX (Buy&Hold)\", linewidth=1.2)\n",
    "plt.title(\"Equity Curve (Test)\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"equity_test.png\")); plt.close()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.hist(returns.dropna(), bins=50, alpha=0.6, label=\"Chiến lược\")\n",
    "plt.hist(bench_ret.dropna(), bins=50, alpha=0.6, label=\"VNINDEX\")\n",
    "plt.title(\"Histogram lợi nhuận ngày (Test)\"); plt.xlabel(\"Daily Return\"); plt.ylabel(\"Tần suất\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"hist_test.png\")); plt.close()\n",
    "\n",
    "# ---------- 4. Stress Test (Trump 46%) ----------\n",
    "stress_start, stress_end = pd.Timestamp(\"2025-03-26\"), pd.Timestamp(\"2025-04-15\")\n",
    "stress_name = \"Trump thông báo đánh thuế 46% hàng hóa Việt Nam\"\n",
    "sub_port, sub_bench = port_val.loc[stress_start:stress_end], bench_val.loc[stress_start:stress_end]\n",
    "stats_stress = {}\n",
    "if len(sub_port)>1:\n",
    "    stats_stress = compute_stats(sub_port, sub_bench)\n",
    "\n",
    "# ---------- Save Stats ----------\n",
    "all_stats={\"Test\":stats_test}\n",
    "if stats_stress: all_stats[\"Stress Test\"]=stats_stress\n",
    "pd.DataFrame(all_stats).T.to_csv(STATS_FILE)\n",
    "\n",
    "print(\"📊 Kết quả Test:\"); print(pd.Series(stats_test))\n",
    "if stats_stress: print(f\"\\n📊 Stress Test ({stress_name}):\"); print(pd.Series(stats_stress))\n",
    "print(f\"\\n✅ Block 11 hoàn tất. Stats lưu tại: {STATS_FILE}, Charts in {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd5497",
   "metadata": {},
   "source": [
    "**Block 12A: gửi tín hiệu lên telegram với dữ liệu quá khứ**\n",
    "\n",
    "`Lưu ý` tuy quá khứ nhưng nhóm không để bị nhìn trước tương lai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ef6b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best config: SL1_TP6\n"
     ]
    }
   ],
   "source": [
    "#Block 12a — Gửi tín hiệu Telegram (Offline replay, simple version)\n",
    "\n",
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== Đường dẫn ====\n",
    "OUTPUT_DIR = \"./backtest_grid/\"\n",
    "SUMMARY_PATH = os.path.join(OUTPUT_DIR, \"grid_summary.csv\")\n",
    "\n",
    "# ==== Load config ====\n",
    "with open(\"config.json\",\"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "TG_TOKEN = cfg[\"telegram\"][\"bot_token\"]\n",
    "TG_CHAT_ID = cfg[\"telegram\"][\"chat_id\"]\n",
    "TG_THREAD_ID = cfg[\"telegram\"][\"message_thread_id\"]\n",
    "\n",
    "# ==== Chọn config tốt nhất (theo Sharpe) ====\n",
    "summary = pd.read_csv(SUMMARY_PATH)\n",
    "best_cfg = summary.sort_values(by=\"Sharpe\", ascending=False).iloc[0][\"config\"]\n",
    "cfg_dir = os.path.join(OUTPUT_DIR, best_cfg)\n",
    "print(f\"Using best config: {best_cfg}\")\n",
    "\n",
    "def send_daily_report(report_date: str | pd.Timestamp, sleep_sec:int=2):\n",
    "    \"\"\"\n",
    "    Gửi báo cáo Telegram cho 1 ngày trong backtest (offline replay).\n",
    "    \"\"\"\n",
    "    report_date = pd.Timestamp(report_date)\n",
    "\n",
    "    # --- NAV ---\n",
    "    pv = pd.read_csv(os.path.join(cfg_dir,\"portfolio_value.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "    if report_date not in pv.index:\n",
    "        print(f\"⚠️ {report_date.date()} không có trong NAV index\")\n",
    "        return\n",
    "    nav_today = pv.loc[:report_date].iloc[-1]\n",
    "\n",
    "    # --- Snapshot danh mục ---\n",
    "    snap_path = os.path.join(cfg_dir, f\"positions_snapshot_{report_date.date()}.csv\")\n",
    "    positions_txt = \"\"\n",
    "    if os.path.exists(snap_path):\n",
    "        pos = pd.read_csv(snap_path, parse_dates=[\"snapshot_date\",\"entry_date\"])\n",
    "        pos = pos[pos[\"position_size_pct\"] > 0]  # chỉ giữ vị thế còn vốn\n",
    "        if len(pos)>0:\n",
    "            for _,r in pos.iterrows():\n",
    "                positions_txt += (\n",
    "                    f\"— {r['ticker']}: Mua {r['entry_price']:.2f} ngày {r['entry_date'].date()}, \"\n",
    "                    f\"SL {r['sl_level']:.2f}, TP {r['tp_level']:.2f}, \"\n",
    "                    f\"Giá hiện {r['last_price']:.2f}, \"\n",
    "                    f\"Lãi/lỗ {r['current_unrealized_pct']:.2f}%, \"\n",
    "                    f\"Tỷ trọng {r['position_size_pct']*100:.1f}%\\n\"\n",
    "                )\n",
    "        else:\n",
    "            positions_txt = \"— Không có vị thế nào đang mở\\n\"\n",
    "    else:\n",
    "        positions_txt = \"— (Không tìm thấy snapshot)\\n\"\n",
    "\n",
    "    # --- Tín hiệu hôm đó ---\n",
    "    sig_path = os.path.join(cfg_dir, f\"signals_today_{report_date.date()}.csv\")\n",
    "    signals_txt, closed_txt, opened_txt = \"\", \"\", \"\"\n",
    "    if os.path.exists(sig_path):\n",
    "        sigs = pd.read_csv(sig_path, parse_dates=[\"entry_date\",\"exit_date\"])\n",
    "        if len(sigs)>0:\n",
    "            # Lệnh mở\n",
    "            opened = sigs[sigs[\"action\"]==\"BUY\"].copy()\n",
    "            if len(opened)>0:\n",
    "                for _,r in opened.iterrows():\n",
    "                    pos_size = r.get(\"position_size_pct\", 0.0) * 100\n",
    "                    opened_txt += (\n",
    "                        f\"— {r['ticker']}: Mua {r['entry_price']:.2f}, \"\n",
    "                        f\"TP {r['tp_level']:.2f}, SL {r['sl_level']:.2f}, \"\n",
    "                        f\"Tỷ trọng {pos_size:.1f}%\\n\"\n",
    "                    )\n",
    "            else:\n",
    "                opened_txt = \"— Không có lệnh mở nào\\n\"\n",
    "\n",
    "            # Lệnh đóng\n",
    "            closed = sigs[sigs[\"action\"]==\"SELL\"].copy()\n",
    "            if len(closed)>0:\n",
    "                for _,r in closed.iterrows():\n",
    "                    entry_price = r.get(\"entry_price\", None)\n",
    "                    exit_price  = r.get(\"exit_price\", None)\n",
    "                    pos_size    = r.get(\"position_size_pct\", 0.0) * 100\n",
    "                    if pd.notna(entry_price) and pd.notna(exit_price) and entry_price>0:\n",
    "                        pnl_pct = (exit_price/entry_price - 1)*100\n",
    "                    else:\n",
    "                        pnl_pct = float(\"nan\")\n",
    "                    closed_txt += (\n",
    "                        f\"— {r['ticker']}: Mua {entry_price:.2f} → \"\n",
    "                        f\"Bán {exit_price:.2f}, \"\n",
    "                        f\"{'Lãi' if pnl_pct>0 else 'Lỗ'} {pnl_pct:.2f}%, \"\n",
    "                        f\"Tỷ trọng {pos_size:.1f}%\\n\"\n",
    "                    )\n",
    "            else:\n",
    "                closed_txt = \"— Không có lệnh đóng nào\\n\"\n",
    "\n",
    "            # Tín hiệu gốc\n",
    "            for _,r in sigs.iterrows():\n",
    "                if r[\"action\"]==\"BUY\":\n",
    "                    signals_txt += (\n",
    "                        f\"🟢 MUA {r['ticker']} giá {r['entry_price']:.2f}, \"\n",
    "                        f\"TP {r['tp_level']:.2f}, SL {r['sl_level']:.2f}\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    exit_price = r[\"exit_price\"] if pd.notna(r[\"exit_price\"]) else 0.0\n",
    "                    signals_txt += (\n",
    "                        f\"🔴 BÁN {r['ticker']} giá {exit_price:.2f}, \"\n",
    "                        f\"loại {r.get('exit_type','NA')}\\n\"\n",
    "                    )\n",
    "        else:\n",
    "            signals_txt = \"— Không có tín hiệu giao dịch hôm nay\\n\"\n",
    "    else:\n",
    "        signals_txt = \"— (Không tìm thấy file tín hiệu)\\n\"\n",
    "        opened_txt = \"— (Không tìm thấy file tín hiệu)\\n\"\n",
    "        closed_txt = \"— (Không tìm thấy file tín hiệu)\\n\"\n",
    "\n",
    "    # --- Chart equity đến ngày đó ---\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(pv.loc[:report_date], label=\"Chiến lược\")\n",
    "    plt.title(f\"Equity đến {report_date.date()}\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend()\n",
    "    chart_path = os.path.join(cfg_dir, f\"equity_until_{report_date.date()}.png\")\n",
    "    plt.savefig(chart_path, dpi=150); plt.close()\n",
    "\n",
    "    # --- Compose message ---\n",
    "    msg = f\"\"\"\n",
    "📅 Ngày {report_date.date()}\n",
    "\n",
    "💰 Giá trị tài sản: {nav_today:,.0f}\n",
    "\n",
    "📊 Danh mục hiện tại:\n",
    "{positions_txt}\n",
    "\n",
    "💡 Lệnh đóng hôm nay:\n",
    "{closed_txt}\n",
    "\n",
    "🟢 Lệnh mở hôm nay:\n",
    "{opened_txt}\n",
    "\n",
    "📌 Tín hiệu trong ngày:\n",
    "{signals_txt}\n",
    "\"\"\".strip()\n",
    "\n",
    "    # --- Gửi text ---\n",
    "    send_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendMessage\"\n",
    "    requests.post(send_url, data={\n",
    "        \"chat_id\": TG_CHAT_ID,\n",
    "        \"message_thread_id\": TG_THREAD_ID,\n",
    "        \"text\": msg\n",
    "    })\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # --- Gửi chart ---\n",
    "    photo_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendPhoto\"\n",
    "    with open(chart_path,\"rb\") as f:\n",
    "        requests.post(photo_url, data={\n",
    "            \"chat_id\": TG_CHAT_ID,\n",
    "            \"message_thread_id\": TG_THREAD_ID,\n",
    "            \"caption\": f\"Equity Curve đến {report_date.date()}\"\n",
    "        }, files={\"photo\":f})\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    print(f\"✅ Đã gửi báo cáo Telegram cho ngày {report_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell runner — gửi báo cáo Telegram từ 26/03 đến 16/04/2025\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "start = pd.Timestamp(\"2025-03-26\")\n",
    "end   = pd.Timestamp(\"2025-04-16\")\n",
    "\n",
    "for d in pd.date_range(start, end, freq=\"D\"):\n",
    "    try:\n",
    "        send_daily_report(d, sleep_sec=10)  # sleep 2 giây giữa mỗi tin nhắn\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi gửi ngày {d.date()}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
