{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4d6cad",
   "metadata": {},
   "source": [
    "**HƯỚNG DẪN CHẠY**\n",
    "\n",
    "*Nhóm chạy code theo thứ tự từng cell từ trên xuống xuống dưới*\n",
    "\n",
    "**Một số điểm lưu ý:**\n",
    "\n",
    "- *Thời gian chạy các block đa số lâu (Khoảng 10 phút riêng block 6 khoảng 25 phút)*\n",
    "\n",
    "- *Các block 8,9,10,11 có lưu kết quả file csv* \n",
    "\n",
    "- **Các file csv kết quả nhóm có upload lên github**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bbe97",
   "metadata": {},
   "source": [
    "**Tải các thư viện cần thiết** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "47e11e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://fiinquant.github.io/fiinquantx/simple\n",
      "Requirement already satisfied: fiinquantx in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.1.35)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.1.2)\n",
      "Requirement already satisfied: signalrcore in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (0.9.5)\n",
      "Requirement already satisfied: fastdtw in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (0.3.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (3.10.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (1.15.3)\n",
      "Requirement already satisfied: python_dotenv in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (1.6.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (6.3.0)\n",
      "Requirement already satisfied: stumpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (1.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib->fiinquantx) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil->fiinquantx) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->fiinquantx) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->fiinquantx) (2024.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from plotly->fiinquantx) (1.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->fiinquantx) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->fiinquantx) (3.6.0)\n",
      "Requirement already satisfied: websocket-client==1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from signalrcore->fiinquantx) (1.0.0)\n",
      "Requirement already satisfied: msgpack==1.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from signalrcore->fiinquantx) (1.0.2)\n",
      "Requirement already satisfied: numba>=0.57.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stumpy->fiinquantx) (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numba>=0.57.1->stumpy->fiinquantx) (0.44.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://fiinquant.github.io/fiinquantx/simple\n",
      "Requirement already satisfied: fiinquantx in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.1.35)\n",
      "Collecting fiinquantx\n",
      "  Downloading https://github.com/fiinquant/fiinquantx/releases/download/0.1.41/fiinquantx-0.1.41-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (2.1.2)\n",
      "Requirement already satisfied: signalrcore in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (0.9.5)\n",
      "Requirement already satisfied: python_dotenv in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (1.1.0)\n",
      "Collecting asyncio (from fiinquantx)\n",
      "  Downloading asyncio-4.0.0-py3-none-any.whl.metadata (994 bytes)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fiinquantx) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->fiinquantx) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->fiinquantx) (3.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->fiinquantx) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->fiinquantx) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil->fiinquantx) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->fiinquantx) (2024.8.30)\n",
      "Requirement already satisfied: websocket-client==1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from signalrcore->fiinquantx) (1.0.0)\n",
      "Requirement already satisfied: msgpack==1.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from signalrcore->fiinquantx) (1.0.2)\n",
      "Downloading asyncio-4.0.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: asyncio, fiinquantx\n",
      "\n",
      "  Attempting uninstall: fiinquantx\n",
      "\n",
      "    Found existing installation: FiinQuantX 0.1.35\n",
      "\n",
      "    Uninstalling FiinQuantX-0.1.35:\n",
      "\n",
      "      Successfully uninstalled FiinQuantX-0.1.35\n",
      "\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   -------------------- ------------------- 1/2 [fiinquantx]\n",
      "   ---------------------------------------- 2/2 [fiinquantx]\n",
      "\n",
      "Successfully installed asyncio-4.0.0 fiinquantx-0.1.41\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch scikit-learn matplotlib\n",
    "!pip install --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx\n",
    "!pip install --upgrade --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565db80b",
   "metadata": {},
   "source": [
    "**Block 1: tải dữ liệu lịch sử và realtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc527032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 — Login & Lấy dữ liệu tất cả HOSE/HNX/UPCOM\n",
    "import pandas as pd\n",
    "from FiinQuantX import FiinSession, BarDataUpdate\n",
    "# --- Login ---\n",
    "username = \"DSTC_18@fiinquant.vn\"\n",
    "password = \"Fiinquant0606\"\n",
    "\n",
    "client = FiinSession(\n",
    "    username=username,\n",
    "    password=password\n",
    ").login()\n",
    "\n",
    "# --- Lấy danh sách cổ phiếu từng sàn ---\n",
    "tickers_hose  = list(client.TickerList(ticker=\"VNINDEX\"))     # HOSE\n",
    "print(f\"Số mã HOSE: {len(tickers_hose)}\")\n",
    "\n",
    "# --- Lấy dữ liệu lịch sử toàn bộ ---\n",
    "event_history = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2020-01-01\"   # Lấy dữ liệu từ 2021 tới nay\n",
    ")\n",
    "\n",
    "df_all = event_history.get_data()\n",
    "print(\"History ban đầu:\", df_all.head())\n",
    "\n",
    "# --- Callback realtime ---\n",
    "def onDataUpdate(data: BarDataUpdate):\n",
    "    global df_all\n",
    "    df_update = data.to_dataFrame()\n",
    "    df_all = pd.concat([df_all, df_update])\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    print(\"Realtime update:\")\n",
    "    print(df_update.head())\n",
    "\n",
    "# --- Bật realtime nối tiếp dữ liệu ---\n",
    "event_realtime = client.Fetch_Trading_Data(\n",
    "    realtime=True,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    period=1,\n",
    "    callback=onDataUpdate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1122224",
   "metadata": {},
   "source": [
    "**Block 2: lấy dữ liệu FA, lọc các mã không hợp lệ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mã VNINDEX tổng: 413\n",
      "Resume: đã phát hiện 13756 ticker đã xử lý trong 'vnindex_fa_quarterly_vnstock.csv'.\n",
      "Resume: 22 ticker đã được đánh dấu INVALID và sẽ bị bỏ qua.\n",
      "Skip CCC (đã có trong processed/master/invalid).\n",
      "Skip SBG (đã có trong processed/master/invalid).\n",
      "Skip FUCTVGF3 (đã có trong processed/master/invalid).\n",
      "Skip FUEIP100 (đã có trong processed/master/invalid).\n",
      "[GMH] ✓ Lấy xong và lưu (rows=18).\n",
      "Skip FUEKIV30 (đã có trong processed/master/invalid).\n",
      "Skip NO1 (đã có trong processed/master/invalid).\n",
      "Skip FUCTVGF4 (đã có trong processed/master/invalid).\n",
      "[RYG] ✓ Lấy xong và lưu (rows=7).\n",
      "Skip FUEDCMID (đã có trong processed/master/invalid).\n",
      "Skip FUEKIVFS (đã có trong processed/master/invalid).\n",
      "Skip FUEMAVND (đã có trong processed/master/invalid).\n",
      "Skip FUEFCV50 (đã có trong processed/master/invalid).\n",
      "Skip FUEBFVND (đã có trong processed/master/invalid).\n",
      "Skip FUCTVGF5 (đã có trong processed/master/invalid).\n",
      "Skip FUEKIVND (đã có trong processed/master/invalid).\n",
      "Skip FUEABVND (đã có trong processed/master/invalid).\n",
      "Skip FUETCC50 (đã có trong processed/master/invalid).\n",
      "Skip FUETPVND (đã có trong processed/master/invalid).\n",
      "[AAA] ✓ Lấy xong và lưu (rows=50).\n",
      "[AAM] ✓ Lấy xong và lưu (rows=50).\n",
      "[AAT] ✓ Lấy xong và lưu (rows=28).\n",
      "[ABR] ✓ Lấy xong và lưu (rows=23).\n",
      "[ABS] ✓ Lấy xong và lưu (rows=26).\n",
      "[ABT] ✓ Lấy xong và lưu (rows=50).\n",
      "[ACB] ✓ Lấy xong và lưu (rows=51).\n",
      "[ACC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ACL] ✓ Lấy xong và lưu (rows=50).\n",
      "[ADP] ✓ Lấy xong và lưu (rows=35).\n",
      "[AGR] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip AGG (đã có trong processed/master/invalid).\n",
      "[ACG] ✓ Lấy xong và lưu (rows=29).\n",
      "[ANV] ✓ Lấy xong và lưu (rows=52).\n",
      "[APG] ✓ Lấy xong và lưu (rows=50).\n",
      "[APH] ✓ Lấy xong và lưu (rows=22).\n",
      "[HII] ✓ Lấy xong và lưu (rows=34).\n",
      "[ASG] ✓ Lấy xong và lưu (rows=28).\n",
      "[ASM] ✓ Lấy xong và lưu (rows=50).\n",
      "[ASP] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip BAF (đã có trong processed/master/invalid).\n",
      "[BBC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BCE] ✓ Lấy xong và lưu (rows=50).\n",
      "[BCG] ✓ Lấy xong và lưu (rows=39).\n",
      "[BFC] ✓ Lấy xong và lưu (rows=43).\n",
      "[BHN] ✓ Lấy xong và lưu (rows=37).\n",
      "[BIC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BID] ✓ Lấy xong và lưu (rows=50).\n",
      "[BCM] ✓ Lấy xong và lưu (rows=30).\n",
      "[DBD] ✓ Lấy xong và lưu (rows=42).\n",
      "[BWE] ✓ Lấy xong và lưu (rows=35).\n",
      "[BKG] ✓ Lấy xong và lưu (rows=21).\n",
      "[BMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BMI] ✓ Lấy xong và lưu (rows=50).\n",
      "[BMP] ✓ Lấy xong và lưu (rows=50).\n",
      "[BRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[BSI] ✓ Lấy xong và lưu (rows=50).\n",
      "[BSR] ✓ Lấy xong và lưu (rows=31).\n",
      "[BTP] ✓ Lấy xong và lưu (rows=50).\n",
      "[BTT] ✓ Lấy xong và lưu (rows=50).\n",
      "[BVH] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip TNH (đã có trong processed/master/invalid).\n",
      "[C32] ✓ Lấy xong và lưu (rows=50).\n",
      "[C47] ✓ Lấy xong và lưu (rows=50).\n",
      "[CCI] ✓ Lấy xong và lưu (rows=50).\n",
      "[CCL] ✓ Lấy xong và lưu (rows=50).\n",
      "[CDC] ✓ Lấy xong và lưu (rows=52).\n",
      "[CRE] ✓ Lấy xong và lưu (rows=30).\n",
      "[STK] ✓ Lấy xong và lưu (rows=43).\n",
      "[CHP] ✓ Lấy xong và lưu (rows=50).\n",
      "[CIG] ✓ Lấy xong và lưu (rows=50).\n",
      "[CII] ✓ Lấy xong và lưu (rows=50).\n",
      "[CKG] ✓ Lấy xong và lưu (rows=38).\n",
      "[CLC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ADG] ✓ Lấy xong và lưu (rows=20).\n",
      "[CLL] ✓ Lấy xong và lưu (rows=50).\n",
      "[CLW] ✓ Lấy xong và lưu (rows=50).\n",
      "[CMG] ✓ Lấy xong và lưu (rows=50).\n",
      "[CMV] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip CMX (đã có trong processed/master/invalid).\n",
      "[CNG] ✓ Lấy xong và lưu (rows=50).\n",
      "[COM] ✓ Lấy xong và lưu (rows=50).\n",
      "[CRC] ✓ Lấy xong và lưu (rows=34).\n",
      "[CSM] ✓ Lấy xong và lưu (rows=50).\n",
      "[CSV] ✓ Lấy xong và lưu (rows=44).\n",
      "[CTD] ✓ Lấy xong và lưu (rows=50).\n",
      "[CTF] ✓ Lấy xong và lưu (rows=35).\n",
      "[CTG] ✓ Lấy xong và lưu (rows=53).\n",
      "[CTI] ✓ Lấy xong và lưu (rows=50).\n",
      "[ICT] ✓ Lấy xong và lưu (rows=31).\n",
      "[CTR] ✓ Lấy xong và lưu (rows=50).\n",
      "[CTS] ✓ Lấy xong và lưu (rows=50).\n",
      "[CVT] ✓ Lấy xong và lưu (rows=50).\n",
      "[D2D] ✓ Lấy xong và lưu (rows=50).\n",
      "[DAH] ✓ Lấy xong và lưu (rows=37).\n",
      "[ADS] ✓ Lấy xong và lưu (rows=38).\n",
      "[DPG] ✓ Lấy xong và lưu (rows=31).\n",
      "[DBC] ✓ Lấy xong và lưu (rows=51).\n",
      "[DBT] ✓ Lấy xong và lưu (rows=50).\n",
      "[DC4] ✓ Lấy xong và lưu (rows=50).\n",
      "[DCL] ✓ Lấy xong và lưu (rows=50).\n",
      "[DCM] ✓ Lấy xong và lưu (rows=42).\n",
      "[DGC] ✓ Lấy xong và lưu (rows=47).\n",
      "[DGW] ✓ Lấy xong và lưu (rows=42).\n",
      "[DHA] ✓ Lấy xong và lưu (rows=50).\n",
      "[DHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[DHG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DHM] ✓ Lấy xong và lưu (rows=50).\n",
      "[TTE] ✓ Lấy xong và lưu (rows=30).\n",
      "[DIG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DLG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DMC] ✓ Lấy xong và lưu (rows=52).\n",
      "[DSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[DSE] ✓ Lấy xong và lưu (rows=52).\n",
      "[DPM] ✓ Lấy xong và lưu (rows=50).\n",
      "[DPR] ✓ Lấy xong và lưu (rows=50).\n",
      "[DQC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip DRC (đã có trong processed/master/invalid).\n",
      "Skip DRH (đã có trong processed/master/invalid).\n",
      "Skip DRL (đã có trong processed/master/invalid).\n",
      "[DSN] ✓ Lấy xong và lưu (rows=50).\n",
      "[DTA] ✓ Lấy xong và lưu (rows=50).\n",
      "[DTL] ✓ Lấy xong và lưu (rows=50).\n",
      "[DTT] ✓ Lấy xong và lưu (rows=50).\n",
      "[DVP] ✓ Lấy xong và lưu (rows=50).\n",
      "[DXG] ✓ Lấy xong và lưu (rows=50).\n",
      "[DXS] ✓ Lấy xong và lưu (rows=18).\n",
      "[DXV] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip FUESSV50 (đã có trong processed/master/invalid).\n",
      "Skip E1VFVN30 (đã có trong processed/master/invalid).\n",
      "[EIB] ✓ Lấy xong và lưu (rows=50).\n",
      "[ELC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip FUESSVFL (đã có trong processed/master/invalid).\n",
      "Skip EVE (đã có trong processed/master/invalid).\n",
      "[EVG] ✓ Lấy xong và lưu (rows=35).\n",
      "[EVF] ✓ Lấy xong và lưu (rows=46).\n",
      "[FCM] ✓ Lấy xong và lưu (rows=50).\n",
      "[FCN] ✓ Lấy xong và lưu (rows=50).\n",
      "[FDC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip FIR (đã có trong processed/master/invalid).\n",
      "[FIT] ✓ Lấy xong và lưu (rows=50).\n",
      "[FMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[FPT] ✓ Lấy xong và lưu (rows=52).\n",
      "[FRT] ✓ Lấy xong và lưu (rows=33).\n",
      "[FTS] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip FUEMAV30 (đã có trong processed/master/invalid).\n",
      "Skip FUESSV30 (đã có trong processed/master/invalid).\n",
      "Skip FUEVFVND (đã có trong processed/master/invalid).\n",
      "Skip FUEVN100 (đã có trong processed/master/invalid).\n",
      "[GAS] ✓ Lấy xong và lưu (rows=50).\n",
      "[GDT] ✓ Lấy xong và lưu (rows=50).\n",
      "[GEX] ✓ Lấy xong và lưu (rows=50).\n",
      "[GEE] ✓ Lấy xong và lưu (rows=15).\n",
      "[PGV] ✓ Lấy xong và lưu (rows=34).\n",
      "[GIL] ✓ Lấy xong và lưu (rows=50).\n",
      "[GEG] ✓ Lấy xong và lưu (rows=42).\n",
      "Skip GMD (đã có trong processed/master/invalid).\n",
      "[GSP] ✓ Lấy xong và lưu (rows=50).\n",
      "[GTA] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAG] ✓ Lấy xong và lưu (rows=54).\n",
      "[HAH] ✓ Lấy xong và lưu (rows=44).\n",
      "[HPX] ✓ Lấy xong và lưu (rows=30).\n",
      "[HID] ✓ Lấy xong và lưu (rows=39).\n",
      "[HHV] ✓ Lấy xong và lưu (rows=23).\n",
      "[HAP] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAR] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAS] ✓ Lấy xong và lưu (rows=50).\n",
      "[HAX] ✓ Lấy xong và lưu (rows=51).\n",
      "[HCD] ✓ Lấy xong và lưu (rows=38).\n",
      "[HCM] ✓ Lấy xong và lưu (rows=52).\n",
      "[HDB] ✓ Lấy xong và lưu (rows=45).\n",
      "[HDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HDG] ✓ Lấy xong và lưu (rows=52).\n",
      "[HHP] ✓ Lấy xong và lưu (rows=28).\n",
      "[HHS] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCH] ✓ Lấy xong và lưu (rows=39).\n",
      "[HSL] ✓ Lấy xong và lưu (rows=31).\n",
      "[HMC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HNA] ✓ Lấy xong và lưu (rows=39).\n",
      "[HPG] ✓ Lấy xong và lưu (rows=50).\n",
      "[HQC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[HSG] ✓ Lấy xong và lưu (rows=50).\n",
      "[HT1] ✓ Lấy xong và lưu (rows=50).\n",
      "[HTG] ✓ Lấy xong và lưu (rows=42).\n",
      "[HTI] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip HTN (đã có trong processed/master/invalid).\n",
      "[HTL] ✓ Lấy xong và lưu (rows=50).\n",
      "[HTV] ✓ Lấy xong và lưu (rows=49).\n",
      "[HU1] ✓ Lấy xong và lưu (rows=50).\n",
      "[HVH] ✓ Lấy xong và lưu (rows=30).\n",
      "[HVX] ✓ Lấy xong và lưu (rows=50).\n",
      "[ILB] ✓ Lấy xong và lưu (rows=42).\n",
      "[IDI] ✓ Lấy xong và lưu (rows=50).\n",
      "[IJC] ✓ Lấy xong và lưu (rows=50).\n",
      "[IMP] ✓ Lấy xong và lưu (rows=50).\n",
      "[ITC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ITD] ✓ Lấy xong và lưu (rows=50).\n",
      "[JVC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip KBC (đã có trong processed/master/invalid).\n",
      "[KDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[KDH] ✓ Lấy xong và lưu (rows=50).\n",
      "[KHG] ✓ Lấy xong và lưu (rows=18).\n",
      "[KHP] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip KMR (đã có trong processed/master/invalid).\n",
      "[KOS] ✓ Lấy xong và lưu (rows=32).\n",
      "[KSB] ✓ Lấy xong và lưu (rows=50).\n",
      "[L10] ✓ Lấy xong và lưu (rows=50).\n",
      "[LAF] ✓ Lấy xong và lưu (rows=50).\n",
      "[LBM] ✓ Lấy xong và lưu (rows=51).\n",
      "[LCG] ✓ Lấy xong và lưu (rows=51).\n",
      "Skip LDG (đã có trong processed/master/invalid).\n",
      "[LGC] ✓ Lấy xong và lưu (rows=50).\n",
      "[LGL] ✓ Lấy xong và lưu (rows=50).\n",
      "[LHG] ✓ Lấy xong và lưu (rows=50).\n",
      "[LIX] ✓ Lấy xong và lưu (rows=50).\n",
      "[LM8] ✓ Lấy xong và lưu (rows=50).\n",
      "[LSS] ✓ Lấy xong và lưu (rows=50).\n",
      "[LPB] ✓ Lấy xong và lưu (rows=47).\n",
      "[MBB] ✓ Lấy xong và lưu (rows=56).\n",
      "[MCM] ✓ Lấy xong và lưu (rows=22).\n",
      "[MCP] ✓ Lấy xong và lưu (rows=50).\n",
      "[MDG] ✓ Lấy xong và lưu (rows=50).\n",
      "[MHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[MIG] ✓ Lấy xong và lưu (rows=42).\n",
      "[MSB] ✓ Lấy xong và lưu (rows=39).\n",
      "Skip MSN (đã có trong processed/master/invalid).\n",
      "[MWG] ✓ Lấy xong và lưu (rows=46).\n",
      "[NAB] ✓ Lấy xong và lưu (rows=50).\n",
      "[NAF] ✓ Lấy xong và lưu (rows=41).\n",
      "[NAV] ✓ Lấy xong và lưu (rows=50).\n",
      "[NBB] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip NCT (đã có trong processed/master/invalid).\n",
      "[NHA] ✓ Lấy xong và lưu (rows=50).\n",
      "[NHH] ✓ Lấy xong và lưu (rows=26).\n",
      "Skip VHM (đã có trong processed/master/invalid).\n",
      "[NHT] ✓ Lấy xong và lưu (rows=27).\n",
      "[NKG] ✓ Lấy xong và lưu (rows=50).\n",
      "[NLG] ✓ Lấy xong và lưu (rows=50).\n",
      "[NNC] ✓ Lấy xong và lưu (rows=50).\n",
      "[NVL] ✓ Lấy xong và lưu (rows=38).\n",
      "[NSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[NT2] ✓ Lấy xong và lưu (rows=50).\n",
      "[NTL] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip NVT (đã có trong processed/master/invalid).\n",
      "[OCB] ✓ Lấy xong và lưu (rows=46).\n",
      "Skip OGC (đã có trong processed/master/invalid).\n",
      "[OPC] ✓ Lấy xong và lưu (rows=50).\n",
      "[ORS] ✓ Lấy xong và lưu (rows=50).\n",
      "[PAC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PAN] ✓ Lấy xong và lưu (rows=50).\n",
      "[PC1] ✓ Lấy xong và lưu (rows=49).\n",
      "[PDN] ✓ Lấy xong và lưu (rows=50).\n",
      "[PDR] ⚠️ Rate limit detected (attempt 1/6). Đợi 502.9s rồi thử lại. Message: Failed to fetch data: 502 - Bad Gateway\n",
      "[PDR] ✓ Lấy xong và lưu (rows=50).\n",
      "[PET] ✓ Lấy xong và lưu (rows=50).\n",
      "[PLX] ✓ Lấy xong và lưu (rows=47).\n",
      "[PGC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PGD] ✓ Lấy xong và lưu (rows=50).\n",
      "[PGI] ✓ Lấy xong và lưu (rows=50).\n",
      "[PLP] ✓ Lấy xong và lưu (rows=33).\n",
      "[PHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PHR] ✓ Lấy xong và lưu (rows=50).\n",
      "[PIT] ✓ Lấy xong và lưu (rows=50).\n",
      "[PJT] ✓ Lấy xong và lưu (rows=50).\n",
      "[PMG] ✓ Lấy xong và lưu (rows=32).\n",
      "[PNC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PNJ] ✓ Lấy xong và lưu (rows=50).\n",
      "[PPC] ✓ Lấy xong và lưu (rows=50).\n",
      "[PTB] ✓ Lấy xong và lưu (rows=50).\n",
      "[PTC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip PTL (đã có trong processed/master/invalid).\n",
      "[DAT] ✓ Lấy xong và lưu (rows=40).\n",
      "Skip PVD (đã có trong processed/master/invalid).\n",
      "[POW] ✓ Lấy xong và lưu (rows=34).\n",
      "[PVT] ✓ Lấy xong và lưu (rows=51).\n",
      "[PVP] ✓ Lấy xong và lưu (rows=49).\n",
      "[QCG] ✓ Lấy xong và lưu (rows=50).\n",
      "[QNP] ✓ Lấy xong và lưu (rows=34).\n",
      "[RAL] ✓ Lấy xong và lưu (rows=50).\n",
      "[REE] ✓ Lấy xong và lưu (rows=50).\n",
      "[SGN] ✓ Lấy xong và lưu (rows=42).\n",
      "[SAM] ✓ Lấy xong và lưu (rows=50).\n",
      "[SAV] ✓ Lấy xong và lưu (rows=50).\n",
      "[SBA] ✓ Lấy xong và lưu (rows=50).\n",
      "[SAB] ✓ Lấy xong và lưu (rows=48).\n",
      "[SBT] ✓ Lấy xong và lưu (rows=50).\n",
      "[SC5] ✓ Lấy xong và lưu (rows=50).\n",
      "[SCR] ✓ Lấy xong và lưu (rows=50).\n",
      "[SCS] ✓ Lấy xong và lưu (rows=43).\n",
      "[SSB] ✓ Lấy xong và lưu (rows=39).\n",
      "[SFC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SFG] ✓ Lấy xong và lưu (rows=49).\n",
      "[SFI] ✓ Lấy xong và lưu (rows=50).\n",
      "[SGR] ✓ Lấy xong và lưu (rows=38).\n",
      "Skip SGT (đã có trong processed/master/invalid).\n",
      "[SIP] ✓ Lấy xong và lưu (rows=26).\n",
      "[SHA] ✓ Lấy xong và lưu (rows=50).\n",
      "[SHB] ✓ Lấy xong và lưu (rows=50).\n",
      "[MSH] ✓ Lấy xong và lưu (rows=38).\n",
      "[SHI] ✓ Lấy xong và lưu (rows=50).\n",
      "[SHP] ✓ Lấy xong và lưu (rows=50).\n",
      "[SBV] ✓ Lấy xong và lưu (rows=35).\n",
      "[SZC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SJD] ✓ Lấy xong và lưu (rows=50).\n",
      "[SJS] ✓ Lấy xong và lưu (rows=51).\n",
      "[SKG] ✓ Lấy xong và lưu (rows=48).\n",
      "[SMA] ✓ Lấy xong và lưu (rows=50).\n",
      "[SMB] ✓ Lấy xong và lưu (rows=49).\n",
      "[SMC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip SPM (đã có trong processed/master/invalid).\n",
      "[SRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SRF] ✓ Lấy xong và lưu (rows=50).\n",
      "[S4A] ✓ Lấy xong và lưu (rows=42).\n",
      "[SSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[SSI] ✓ Lấy xong và lưu (rows=50).\n",
      "[ST8] ✓ Lấy xong và lưu (rows=50).\n",
      "[STB] ✓ Lấy xong và lưu (rows=50).\n",
      "[STG] ✓ Lấy xong và lưu (rows=50).\n",
      "[SVC] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip SVD (đã có trong processed/master/invalid).\n",
      "[SVI] ✓ Lấy xong và lưu (rows=50).\n",
      "[SVT] ✓ Lấy xong và lưu (rows=50).\n",
      "[SZL] ✓ Lấy xong và lưu (rows=50).\n",
      "[AST] ✓ Lấy xong và lưu (rows=33).\n",
      "[TAL] ✓ Lấy xong và lưu (rows=8).\n",
      "[TBC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCB] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCL] ✓ Lấy xong và lưu (rows=49).\n",
      "[TCM] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCO] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip TCR (đã có trong processed/master/invalid).\n",
      "Skip FUCVREIT (đã có trong processed/master/invalid).\n",
      "[TCI] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCT] ✓ Lấy xong và lưu (rows=50).\n",
      "[TDC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TDG] ✓ Lấy xong và lưu (rows=35).\n",
      "[TDH] ✓ Lấy xong và lưu (rows=50).\n",
      "[TDM] ✓ Lấy xong và lưu (rows=38).\n",
      "[TDP] ✓ Lấy xong và lưu (rows=30).\n",
      "[TDW] ✓ Lấy xong và lưu (rows=50).\n",
      "[TEG] ✓ Lấy xong và lưu (rows=39).\n",
      "[THG] ✓ Lấy xong và lưu (rows=50).\n",
      "[TIP] ✓ Lấy xong và lưu (rows=50).\n",
      "[TIX] ✓ Lấy xong và lưu (rows=50).\n",
      "[TLD] ✓ Lấy xong và lưu (rows=32).\n",
      "[TLG] ✓ Lấy xong và lưu (rows=50).\n",
      "[TLH] ✓ Lấy xong và lưu (rows=50).\n",
      "[TMP] ✓ Lấy xong và lưu (rows=50).\n",
      "[TMS] ✓ Lấy xong và lưu (rows=50).\n",
      "[TMT] ✓ Lấy xong và lưu (rows=50).\n",
      "[TN1] ✓ Lấy xong và lưu (rows=29).\n",
      "[TNC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TNI] ✓ Lấy xong và lưu (rows=38).\n",
      "Skip TNT (đã có trong processed/master/invalid).\n",
      "[TPB] ✓ Lấy xong và lưu (rows=42).\n",
      "[TPC] ✓ Lấy xong và lưu (rows=50).\n",
      "[TRA] ✓ Lấy xong và lưu (rows=50).\n",
      "[TCD] ✓ Lấy xong và lưu (rows=35).\n",
      "[TRC] ✓ Lấy xong và lưu (rows=51).\n",
      "[TVB] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip TSC (đã có trong processed/master/invalid).\n",
      "[TTA] ✓ Lấy xong và lưu (rows=21).\n",
      "[TTF] ✓ Lấy xong và lưu (rows=50).\n",
      "[HUB] ✓ Lấy xong và lưu (rows=37).\n",
      "[TV2] ✓ Lấy xong và lưu (rows=50).\n",
      "[TVS] ✓ Lấy xong và lưu (rows=50).\n",
      "[TYA] ✓ Lấy xong và lưu (rows=50).\n",
      "[UIC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VAB] ✓ Lấy xong và lưu (rows=41).\n",
      "[VAF] ✓ Lấy xong và lưu (rows=48).\n",
      "[VCA] ✓ Lấy xong và lưu (rows=50).\n",
      "[VCB] ✓ Lấy xong và lưu (rows=50).\n",
      "[VCF] ✓ Lấy xong và lưu (rows=50).\n",
      "[VCG] ✓ Lấy xong và lưu (rows=52).\n",
      "[VCI] ✓ Lấy xong và lưu (rows=50).\n",
      "[VDS] ✓ Lấy xong và lưu (rows=50).\n",
      "[VFG] ✓ Lấy xong và lưu (rows=50).\n",
      "[VGC] ✓ Lấy xong và lưu (rows=42).\n",
      "[VHC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VIB] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip VIC (đã có trong processed/master/invalid).\n",
      "[TVT] ✓ Lấy xong và lưu (rows=45).\n",
      "[VID] ✓ Lấy xong và lưu (rows=50).\n",
      "[VDP] ✓ Lấy xong và lưu (rows=35).\n",
      "[VJC] ✓ Lấy xong và lưu (rows=35).\n",
      "[VIP] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPS] ✓ Lấy xong và lưu (rows=42).\n",
      "Skip VIX (đã có trong processed/master/invalid).\n",
      "[VMD] ✓ Lấy xong và lưu (rows=49).\n",
      "[HVN] ✓ Lấy xong và lưu (rows=42).\n",
      "[VND] ✓ Lấy xong và lưu (rows=51).\n",
      "[VNE] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip VNG (đã có trong processed/master/invalid).\n",
      "[VNL] ✓ Lấy xong và lưu (rows=50).\n",
      "[VNM] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPD] ✓ Lấy xong và lưu (rows=43).\n",
      "[GVR] ✓ Lấy xong và lưu (rows=28).\n",
      "[VNS] ✓ Lấy xong và lưu (rows=50).\n",
      "Skip VOS (đã có trong processed/master/invalid).\n",
      "[VPB] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPG] ✓ Lấy xong và lưu (rows=33).\n",
      "[VPH] ✓ Lấy xong và lưu (rows=50).\n",
      "[VPL] ✓ Lấy xong và lưu (rows=22).\n",
      "[VPI] ✓ Lấy xong và lưu (rows=32).\n",
      "[VRC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VRE] ✓ Lấy xong và lưu (rows=33).\n",
      "[VSC] ✓ Lấy xong và lưu (rows=50).\n",
      "[VSH] ✓ Lấy xong và lưu (rows=50).\n",
      "[VSI] ✓ Lấy xong và lưu (rows=50).\n",
      "[VTB] ✓ Lấy xong và lưu (rows=51).\n",
      "[VTO] ✓ Lấy xong và lưu (rows=50).\n",
      "[VTP] ✓ Lấy xong và lưu (rows=37).\n",
      "Skip YBM (đã có trong processed/master/invalid).\n",
      "[YEG] ✓ Lấy xong và lưu (rows=31).\n",
      "=== Hoàn tất Block 2 (vnstock) ===\n",
      "Tổng tickers xử lý thành công: 356\n",
      "Tổng tickers bị đánh dấu INVALID và bỏ qua: 0\n",
      "Tổng tickers thất bại (retry hết nhưng không invalid): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Meta</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Chỉ tiêu cơ cấu nguồn vốn</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Chỉ tiêu hiệu quả hoạt động</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"9\" halign=\"left\">Chỉ tiêu định giá</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>yearReport</th>\n",
       "      <th>lengthReport</th>\n",
       "      <th>(ST+LT borrowings)/Equity</th>\n",
       "      <th>Debt/Equity</th>\n",
       "      <th>Fixed Asset-To-Equity</th>\n",
       "      <th>Owners' Equity/Charter Capital</th>\n",
       "      <th>Asset Turnover</th>\n",
       "      <th>Fixed Asset Turnover</th>\n",
       "      <th>Days Sales Outstanding</th>\n",
       "      <th>...</th>\n",
       "      <th>Market Capital (Bn. VND)</th>\n",
       "      <th>Outstanding Share (Mil. Shares)</th>\n",
       "      <th>P/E</th>\n",
       "      <th>P/B</th>\n",
       "      <th>P/S</th>\n",
       "      <th>P/Cash Flow</th>\n",
       "      <th>EPS (VND)</th>\n",
       "      <th>BVPS (VND)</th>\n",
       "      <th>EV/EBITDA</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051588</td>\n",
       "      <td>0.124650</td>\n",
       "      <td>1.090538</td>\n",
       "      <td>0.562538</td>\n",
       "      <td>4.422492</td>\n",
       "      <td>63.351078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452000e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>11.480752</td>\n",
       "      <td>0.806941</td>\n",
       "      <td>1.362896</td>\n",
       "      <td>3.795689</td>\n",
       "      <td>356.816813</td>\n",
       "      <td>10905.381178</td>\n",
       "      <td>11.204889</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022473</td>\n",
       "      <td>0.051838</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>1.089856</td>\n",
       "      <td>0.495824</td>\n",
       "      <td>3.693597</td>\n",
       "      <td>71.229634</td>\n",
       "      <td>...</td>\n",
       "      <td>1.311750e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>16.681350</td>\n",
       "      <td>0.729454</td>\n",
       "      <td>1.406508</td>\n",
       "      <td>3.940808</td>\n",
       "      <td>121.619694</td>\n",
       "      <td>10898.564366</td>\n",
       "      <td>20.561323</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038005</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>1.077694</td>\n",
       "      <td>0.461186</td>\n",
       "      <td>3.248087</td>\n",
       "      <td>69.708499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.229250e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>20.639945</td>\n",
       "      <td>0.691291</td>\n",
       "      <td>1.395969</td>\n",
       "      <td>10.732768</td>\n",
       "      <td>120.501797</td>\n",
       "      <td>10776.944672</td>\n",
       "      <td>19.378872</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.106987</td>\n",
       "      <td>0.146877</td>\n",
       "      <td>1.065434</td>\n",
       "      <td>0.456444</td>\n",
       "      <td>3.178684</td>\n",
       "      <td>66.693418</td>\n",
       "      <td>...</td>\n",
       "      <td>1.313400e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>21.487381</td>\n",
       "      <td>0.747113</td>\n",
       "      <td>1.491061</td>\n",
       "      <td>7.664694</td>\n",
       "      <td>167.561987</td>\n",
       "      <td>10654.338632</td>\n",
       "      <td>31.526449</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.063503</td>\n",
       "      <td>0.156407</td>\n",
       "      <td>1.048888</td>\n",
       "      <td>0.438959</td>\n",
       "      <td>2.972644</td>\n",
       "      <td>64.985075</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366200e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>19.196756</td>\n",
       "      <td>0.789407</td>\n",
       "      <td>1.585955</td>\n",
       "      <td>11.360902</td>\n",
       "      <td>66.896642</td>\n",
       "      <td>10488.880887</td>\n",
       "      <td>18.004344</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.032987</td>\n",
       "      <td>0.150374</td>\n",
       "      <td>1.142198</td>\n",
       "      <td>0.473768</td>\n",
       "      <td>3.133843</td>\n",
       "      <td>60.882722</td>\n",
       "      <td>...</td>\n",
       "      <td>1.485000e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>16.702805</td>\n",
       "      <td>0.787954</td>\n",
       "      <td>1.568495</td>\n",
       "      <td>14.300980</td>\n",
       "      <td>5.990141</td>\n",
       "      <td>11421.984245</td>\n",
       "      <td>15.236970</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047413</td>\n",
       "      <td>0.157043</td>\n",
       "      <td>1.142922</td>\n",
       "      <td>0.572311</td>\n",
       "      <td>3.624814</td>\n",
       "      <td>53.195105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658250e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>12.102004</td>\n",
       "      <td>0.880344</td>\n",
       "      <td>1.460188</td>\n",
       "      <td>-104.265001</td>\n",
       "      <td>130.001223</td>\n",
       "      <td>11415.994104</td>\n",
       "      <td>12.309960</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>0.115587</td>\n",
       "      <td>0.165952</td>\n",
       "      <td>1.128155</td>\n",
       "      <td>0.637397</td>\n",
       "      <td>4.058792</td>\n",
       "      <td>50.114334</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600500e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>9.127335</td>\n",
       "      <td>0.859811</td>\n",
       "      <td>1.234861</td>\n",
       "      <td>59.962717</td>\n",
       "      <td>228.434875</td>\n",
       "      <td>11281.546389</td>\n",
       "      <td>9.337824</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.175449</td>\n",
       "      <td>1.105311</td>\n",
       "      <td>0.682272</td>\n",
       "      <td>4.248544</td>\n",
       "      <td>47.441126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592250e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>8.101948</td>\n",
       "      <td>0.873057</td>\n",
       "      <td>1.143669</td>\n",
       "      <td>24.127825</td>\n",
       "      <td>174.405400</td>\n",
       "      <td>11053.111514</td>\n",
       "      <td>9.445047</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GMH</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043288</td>\n",
       "      <td>0.101775</td>\n",
       "      <td>0.169373</td>\n",
       "      <td>1.137871</td>\n",
       "      <td>0.724659</td>\n",
       "      <td>4.420403</td>\n",
       "      <td>40.665490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.666500e+11</td>\n",
       "      <td>16500000.0</td>\n",
       "      <td>6.932984</td>\n",
       "      <td>0.887623</td>\n",
       "      <td>1.121519</td>\n",
       "      <td>12.311260</td>\n",
       "      <td>293.153010</td>\n",
       "      <td>11378.706114</td>\n",
       "      <td>7.042915</td>\n",
       "      <td>GMH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Meta                         Chỉ tiêu cơ cấu nguồn vốn              \\\n",
       "  ticker yearReport lengthReport (ST+LT borrowings)/Equity Debt/Equity   \n",
       "0    GMH       2025            2                  0.000000    0.051588   \n",
       "1    GMH       2025            1                  0.022473    0.051838   \n",
       "2    GMH       2024            4                  0.000000    0.038005   \n",
       "3    GMH       2024            3                  0.038418    0.106987   \n",
       "4    GMH       2024            2                  0.015499    0.063503   \n",
       "5    GMH       2024            1                  0.009567    0.032987   \n",
       "6    GMH       2023            4                  0.000000    0.047413   \n",
       "7    GMH       2023            3                  0.040551    0.115587   \n",
       "8    GMH       2023            2                  0.000000    0.088011   \n",
       "9    GMH       2023            1                  0.043288    0.101775   \n",
       "\n",
       "                                                        \\\n",
       "  Fixed Asset-To-Equity Owners' Equity/Charter Capital   \n",
       "0              0.124650                       1.090538   \n",
       "1              0.130708                       1.089856   \n",
       "2              0.138373                       1.077694   \n",
       "3              0.146877                       1.065434   \n",
       "4              0.156407                       1.048888   \n",
       "5              0.150374                       1.142198   \n",
       "6              0.157043                       1.142922   \n",
       "7              0.165952                       1.128155   \n",
       "8              0.175449                       1.105311   \n",
       "9              0.169373                       1.137871   \n",
       "\n",
       "  Chỉ tiêu hiệu quả hoạt động                                              \\\n",
       "               Asset Turnover Fixed Asset Turnover Days Sales Outstanding   \n",
       "0                    0.562538             4.422492              63.351078   \n",
       "1                    0.495824             3.693597              71.229634   \n",
       "2                    0.461186             3.248087              69.708499   \n",
       "3                    0.456444             3.178684              66.693418   \n",
       "4                    0.438959             2.972644              64.985075   \n",
       "5                    0.473768             3.133843              60.882722   \n",
       "6                    0.572311             3.624814              53.195105   \n",
       "7                    0.637397             4.058792              50.114334   \n",
       "8                    0.682272             4.248544              47.441126   \n",
       "9                    0.724659             4.420403              40.665490   \n",
       "\n",
       "   ...        Chỉ tiêu định giá                                             \\\n",
       "   ... Market Capital (Bn. VND) Outstanding Share (Mil. Shares)        P/E   \n",
       "0  ...             1.452000e+11                      16500000.0  11.480752   \n",
       "1  ...             1.311750e+11                      16500000.0  16.681350   \n",
       "2  ...             1.229250e+11                      16500000.0  20.639945   \n",
       "3  ...             1.313400e+11                      16500000.0  21.487381   \n",
       "4  ...             1.366200e+11                      16500000.0  19.196756   \n",
       "5  ...             1.485000e+11                      16500000.0  16.702805   \n",
       "6  ...             1.658250e+11                      16500000.0  12.102004   \n",
       "7  ...             1.600500e+11                      16500000.0   9.127335   \n",
       "8  ...             1.592250e+11                      16500000.0   8.101948   \n",
       "9  ...             1.666500e+11                      16500000.0   6.932984   \n",
       "\n",
       "                                                                       ticker  \n",
       "        P/B       P/S P/Cash Flow   EPS (VND)    BVPS (VND)  EV/EBITDA         \n",
       "0  0.806941  1.362896    3.795689  356.816813  10905.381178  11.204889    GMH  \n",
       "1  0.729454  1.406508    3.940808  121.619694  10898.564366  20.561323    GMH  \n",
       "2  0.691291  1.395969   10.732768  120.501797  10776.944672  19.378872    GMH  \n",
       "3  0.747113  1.491061    7.664694  167.561987  10654.338632  31.526449    GMH  \n",
       "4  0.789407  1.585955   11.360902   66.896642  10488.880887  18.004344    GMH  \n",
       "5  0.787954  1.568495   14.300980    5.990141  11421.984245  15.236970    GMH  \n",
       "6  0.880344  1.460188 -104.265001  130.001223  11415.994104  12.309960    GMH  \n",
       "7  0.859811  1.234861   59.962717  228.434875  11281.546389   9.337824    GMH  \n",
       "8  0.873057  1.143669   24.127825  174.405400  11053.111514   9.445047    GMH  \n",
       "9  0.887623  1.121519   12.311260  293.153010  11378.706114   7.042915    GMH  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 2 — Lấy dữ liệu FA theo quý (VNINDEX) với retry + backoff, skip ticker không hợp lệ và lưu incremental\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from vnstock import Finance\n",
    "\n",
    "# --- Dùng tickers_hose từ Block 1 nếu có, nếu không lấy lại từ client ---\n",
    "try:\n",
    "    tickers_vnindex = tickers_hose\n",
    "except NameError:\n",
    "    tickers_vnindex = list(client.TickerList(ticker=\"VNINDEX\"))\n",
    "\n",
    "print(f\"Số mã VNINDEX tổng: {len(tickers_vnindex)}\")\n",
    "\n",
    "# --- Output files ---\n",
    "master_file = \"vnindex_fa_quarterly_vnstock.csv\"\n",
    "per_ticker_dir = \"vnindex_fa_by_ticker\"\n",
    "invalid_file = \"vnindex_invalid_tickers.txt\"\n",
    "os.makedirs(per_ticker_dir, exist_ok=True)\n",
    "\n",
    "# --- Retry / backoff parameters ---\n",
    "MAX_RETRIES = 6\n",
    "BASE_DELAY = 5\n",
    "BACKOFF_BASE = 2.0\n",
    "JITTER = 1.0\n",
    "RATE_LIMIT_PATTERNS = [\n",
    "    r\"rate limit exceeded\",\n",
    "    r\"you have sent too many requests\",\n",
    "    r\"too many requests\",\n",
    "    r\"rate limit\",\n",
    "    r\"vci.*rate\",\n",
    "]\n",
    "INVALID_TICKER_PATTERNS = [\n",
    "    r\"mã chứng khoán không hợp lệ\",\n",
    "    r\"chỉ cổ phiếu mới có thông tin\",\n",
    "    r\"không phải mã cổ phiếu\",\n",
    "    r\"invalid symbol\",\n",
    "    r\"symbol is invalid\",\n",
    "]\n",
    "\n",
    "def parse_wait_seconds_from_msg(msg):\n",
    "    if not msg:\n",
    "        return None\n",
    "    m = re.search(r\"after\\s+(\\d+)\\s*seconds?\", msg, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m2 = re.search(r\"after\\s+(\\d+)\\s*s\\b\", msg, flags=re.IGNORECASE)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "    m3 = re.search(r\"(\\d+)\", msg)\n",
    "    if m3:\n",
    "        return int(m3.group(1))\n",
    "    return None\n",
    "\n",
    "# --- Resume: đọc danh sách tickers đã xử lý trong master file nếu có ---\n",
    "processed = set()\n",
    "if os.path.exists(master_file):\n",
    "    try:\n",
    "        df_exist = pd.read_csv(master_file, usecols=['ticker'])\n",
    "        processed.update(df_exist['ticker'].astype(str).unique().tolist())\n",
    "        print(f\"Resume: đã phát hiện {len(processed)} ticker đã xử lý trong '{master_file}'.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Resume: đọc danh sách ticker invalid đã lưu trước đó ---\n",
    "invalid_set = set()\n",
    "if os.path.exists(invalid_file):\n",
    "    try:\n",
    "        with open(invalid_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    invalid_set.add(s)\n",
    "        processed.update(invalid_set)  # coi invalid như đã xử lý để skip lần sau\n",
    "        print(f\"Resume: {len(invalid_set)} ticker đã được đánh dấu INVALID và sẽ bị bỏ qua.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Thu thập dữ liệu ---\n",
    "fa_list = []\n",
    "successful = []\n",
    "skipped_invalid = []\n",
    "failed = []\n",
    "\n",
    "for t in tickers_vnindex:\n",
    "    if t in processed:\n",
    "        print(f\"Skip {t} (đã có trong processed/master/invalid).\")\n",
    "        continue\n",
    "\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    last_exception_msg = None\n",
    "\n",
    "    while attempt < MAX_RETRIES and not success:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            finance = Finance(symbol=t, source='VCI')\n",
    "            df = finance.ratio(period='quarterly')\n",
    "            df = pd.DataFrame(df) if df is not None else pd.DataFrame()\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"[{t}] ⚠️ Không có dữ liệu FA (vnstock). Bỏ qua (không lưu).\")\n",
    "                # không ghi master nếu trống; đánh dấu là đã thử (không add vào processed để có thể thử lại sau nếu muốn)\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            # Đảm bảo có cột ticker\n",
    "            if 'ticker' not in df.columns:\n",
    "                df['ticker'] = t\n",
    "            else:\n",
    "                df['ticker'] = df['ticker'].fillna(t)\n",
    "\n",
    "            # Nếu có ReportDate (meta) thì drop đi\n",
    "            if 'ReportDate' in df.columns:\n",
    "                df = df.drop(columns=['ReportDate'])\n",
    "\n",
    "            # Coerce numeric cho các cột (nếu có thể)\n",
    "            for c in df.columns:\n",
    "                if c != 'ticker':\n",
    "                    df[c] = pd.to_numeric(df[c], errors='ignore')\n",
    "\n",
    "            # --- Lưu per-ticker CSV ---\n",
    "            per_file = os.path.join(per_ticker_dir, f\"{t}_fa.csv\")\n",
    "            df.to_csv(per_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "            # --- Append vào master CSV ---\n",
    "            write_header = not os.path.exists(master_file)\n",
    "            df.to_csv(master_file, mode='a', header=write_header, index=False, encoding='utf-8-sig')\n",
    "\n",
    "            print(f\"[{t}] ✓ Lấy xong và lưu (rows={len(df)}).\")\n",
    "            fa_list.append(df)\n",
    "            successful.append(t)\n",
    "            processed.add(t)  # mark processed so resume will skip next time\n",
    "            success = True\n",
    "\n",
    "            # nhẹ nhàng sleep giữa requests\n",
    "            time.sleep(BASE_DELAY + random.uniform(0, JITTER))\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            last_exception_msg = msg\n",
    "            lower_msg = msg.lower()\n",
    "\n",
    "            # --- Nếu lỗi do ticker không hợp lệ -> bỏ luôn, không retry ---\n",
    "            is_invalid = any(re.search(pat, lower_msg) for pat in INVALID_TICKER_PATTERNS)\n",
    "            if is_invalid:\n",
    "                print(f\"[{t}] ❌ Mã không hợp lệ — BỎ QUA (không retry). Message: {msg}\")\n",
    "                skipped_invalid.append(t)\n",
    "                processed.add(t)\n",
    "                # lưu vào file invalid để lần sau không thử lại\n",
    "                try:\n",
    "                    with open(invalid_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(t + \"\\n\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                break\n",
    "\n",
    "            # --- Nếu nghi ngờ rate-limit hoặc server báo wait time ---\n",
    "            is_rate_limit = any(pat in lower_msg for pat in RATE_LIMIT_PATTERNS)\n",
    "            parsed_wait = parse_wait_seconds_from_msg(msg)\n",
    "\n",
    "            if is_rate_limit or parsed_wait:\n",
    "                wait_sec = parsed_wait if parsed_wait and parsed_wait > 0 else (BACKOFF_BASE ** attempt)\n",
    "                wait_sec = wait_sec + random.uniform(0, JITTER)\n",
    "                print(f\"[{t}] ⚠️ Rate limit detected (attempt {attempt}/{MAX_RETRIES}). \"\n",
    "                      f\"Đợi {wait_sec:.1f}s rồi thử lại. Message: {msg}\")\n",
    "                time.sleep(wait_sec)\n",
    "                continue\n",
    "            else:\n",
    "                backoff = (BACKOFF_BASE ** attempt) + random.uniform(0, JITTER)\n",
    "                print(f\"[{t}] ⚠️ Lỗi khi fetch (attempt {attempt}/{MAX_RETRIES}): {msg}. \"\n",
    "                      f\"Đợi {backoff:.1f}s rồi thử lại.\")\n",
    "                time.sleep(backoff)\n",
    "                continue\n",
    "\n",
    "    # nếu vòng retry kết thúc mà không success và không thuộc invalid -> mark failed\n",
    "    if not success and t not in skipped_invalid:\n",
    "        print(f\"[{t}] ❌ Không lấy được dữ liệu sau {MAX_RETRIES} lần. Bỏ qua ticker này. Lỗi cuối: {last_exception_msg}\")\n",
    "        failed.append(t)\n",
    "\n",
    "# --- Kết quả tổng kết ---\n",
    "print(\"=== Hoàn tất Block 2 (vnstock) ===\")\n",
    "print(f\"Tổng tickers xử lý thành công: {len(successful)}\")\n",
    "print(f\"Tổng tickers bị đánh dấu INVALID và bỏ qua: {len(skipped_invalid)}\")\n",
    "print(f\"Tổng tickers thất bại (retry hết nhưng không invalid): {len(failed)}\")\n",
    "\n",
    "if fa_list:\n",
    "    sample_df = pd.concat(fa_list, ignore_index=True, sort=False).head(10)\n",
    "    display(sample_df)\n",
    "else:\n",
    "    print(\"❗ Không có dữ liệu FA thu được cho các ticker đã chạy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9ef83",
   "metadata": {},
   "source": [
    "**Block 3: Chuẩn hóa FA và gộp dữ liệu với giá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã đọc FA từ 391 file, tổng dòng = 17586\n",
      "Số bản ghi giá (sau filter theo FA): 546084  | Số ticker: 391\n",
      "FA cleaned: tickers: 391 rows: 17586\n",
      "Merged shape: (546084, 17)\n",
      "Merged time range: 2020-01-02 00:00:00 -> 2025-10-02 00:00:00\n",
      "Ticker count: 391\n",
      "Số dòng vẫn thiếu toàn bộ FA sau ffill: 245 (có thể là ticker hoàn toàn không có báo cáo trước thời điểm đó)\n",
      "✅ Đã lưu merged -> vnindex_price_fa_merged.csv\n",
      "Split sizes (rows): TRAIN 376939 VAL 96924 TEST 72221\n",
      "TRAIN period: 2020-01-02 00:00:00 -> 2023-12-29 00:00:00\n",
      "VAL period: 2024-01-02 00:00:00 -> 2024-12-31 00:00:00\n",
      "TEST period: 2025-01-02 00:00:00 -> 2025-10-02 00:00:00\n",
      "✅ Saved splits -> vnindex_price_fa_train.parquet, vnindex_price_fa_val.parquet, vnindex_price_fa_test.parquet\n",
      "Top tickers by data rows:\n",
      "    ticker first_date  last_date  n_rows\n",
      "390    YEG 2020-01-02 2025-10-02    1435\n",
      "0      AAA 2020-01-02 2025-10-02    1435\n",
      "1      AAM 2020-01-02 2025-10-02    1435\n",
      "389    YBM 2020-01-02 2025-10-02    1435\n",
      "371    VNM 2020-01-02 2025-10-02    1435\n",
      "370    VNL 2020-01-02 2025-10-02    1435\n",
      "369    VNG 2020-01-02 2025-10-02    1435\n",
      "368    VNE 2020-01-02 2025-10-02    1435\n"
     ]
    }
   ],
   "source": [
    "# Block 3 (fixed) — Chuẩn hoá FA (từ file per-ticker có 2 dòng header) + Merge với giá + Split train/val/test\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Config (chỉnh nếu cần) ----------\n",
    "fa_folder = \"vnindex_fa_by_ticker\"\n",
    "output_merged = \"vnindex_price_fa_merged.csv\"\n",
    "output_train = \"vnindex_price_fa_train.parquet\"\n",
    "output_val   = \"vnindex_price_fa_val.parquet\"\n",
    "output_test  = \"vnindex_price_fa_test.parquet\"\n",
    "\n",
    "# Default splits (thay đổi nếu muốn)\n",
    "TRAIN_END = pd.Timestamp(\"2023-12-31\")   # bao gồm ngày này -> TRAIN\n",
    "VAL_END   = pd.Timestamp(\"2024-12-31\")   # ngày > TRAIN_END & <= VAL_END -> VAL\n",
    "# TEST = ngày > VAL_END\n",
    "\n",
    "# Các cột FA bạn cần (theo yêu cầu)\n",
    "fa_fields = [\"Debt/Equity\", \"Net Profit Margin (%)\", \"P/E\", \"P/B\"]\n",
    "need_cols = [\"ticker\",\"yearReport\",\"lengthReport\"] + fa_fields\n",
    "\n",
    "# ---------- 1) Đọc FA từ thư mục ----------\n",
    "fa_list = []\n",
    "if not os.path.isdir(fa_folder):\n",
    "    raise FileNotFoundError(f\"FA folder not found: {fa_folder}\")\n",
    "\n",
    "for file in os.listdir(fa_folder):\n",
    "    if not file.endswith(\"_fa.csv\"):\n",
    "        continue\n",
    "    path = os.path.join(fa_folder, file)\n",
    "    try:\n",
    "        # Bỏ dòng meta đầu, dòng thứ 2 làm header\n",
    "        df = pd.read_csv(path, header=1, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "        # Đảm bảo có tất cả các cột\n",
    "        for col in need_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "\n",
    "        df = df[need_cols].copy()\n",
    "        # Chuẩn hoá ticker (loại khoảng trắng)\n",
    "        df[\"ticker\"] = df[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "        fa_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi đọc {file}: {e}\")\n",
    "\n",
    "if len(fa_list) == 0:\n",
    "    raise RuntimeError(\"Không đọc được file FA nào từ folder. Kiểm tra đường dẫn và tên file *_fa.csv\")\n",
    "\n",
    "fa_data = pd.concat(fa_list, ignore_index=True)\n",
    "print(f\"Đã đọc FA từ {len(fa_list)} file, tổng dòng = {len(fa_data)}\")\n",
    "\n",
    "# ---------- 2) Load price df_all nếu cần ----------\n",
    "if \"df_all\" not in globals():\n",
    "    # cố gắng load từ file tiêu chuẩn nếu tồn tại\n",
    "    possible_paths = [\"vnindex_price_all.csv\", \"vnindex_price.csv\", \"vnindex_price_fa_merged.csv\"]\n",
    "    loaded = False\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            print(f\"⚡ Không tìm thấy df_all trong memory, đang load từ: {p}\")\n",
    "            df_all = pd.read_csv(p, parse_dates=[\"timestamp\"])\n",
    "            loaded = True\n",
    "            break\n",
    "    if not loaded:\n",
    "        raise RuntimeError(\"Không tìm thấy biến df_all trong bộ nhớ và không tìm được file giá mặc định. Vui lòng chạy Block 1 hoặc đặt file price tại one of: \" + \", \".join(possible_paths))\n",
    "\n",
    "# Chuẩn hoá df_all minimal\n",
    "df_all = df_all.rename(columns={c:c for c in df_all.columns})\n",
    "if \"timestamp\" not in df_all.columns:\n",
    "    raise RuntimeError(\"df_all phải có cột 'timestamp' (datetime).\")\n",
    "\n",
    "df_all[\"timestamp\"] = pd.to_datetime(df_all[\"timestamp\"])\n",
    "df_all[\"ticker\"] = df_all[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "df_all = df_all.sort_values([\"ticker\",\"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# Giới hạn df_price chỉ bao những ticker có FA (đỡ nặng)\n",
    "tickers_with_fa = fa_data[\"ticker\"].dropna().unique().tolist()\n",
    "df_price = df_all[df_all[\"ticker\"].isin(tickers_with_fa)].copy()\n",
    "print(\"Số bản ghi giá (sau filter theo FA):\", len(df_price), \" | Số ticker:\", df_price[\"ticker\"].nunique())\n",
    "\n",
    "# ---------- 3) Tạo key fa_year/fa_quarter = quý TRƯỚC của price timestamp ----------\n",
    "# (Giả định: FA áp dụng lag 1 quarter)\n",
    "pi = df_price[\"timestamp\"].dt.to_period(\"Q\")\n",
    "prev_pi = pi - 1\n",
    "df_price[\"fa_year\"] = prev_pi.dt.year.astype(int)\n",
    "df_price[\"fa_quarter\"] = prev_pi.dt.quarter.astype(int)\n",
    "\n",
    "# ---------- 4) Chuẩn hoá bảng FA (rename + dedup) ----------\n",
    "fa_clean = fa_data.rename(columns={\"yearReport\": \"fa_year\", \"lengthReport\": \"fa_quarter\"}).copy()\n",
    "\n",
    "# ép kiểu numeric nơi có thể (giữ NA nếu không parse được)\n",
    "for f in fa_fields:\n",
    "    if f in fa_clean.columns:\n",
    "        fa_clean[f] = pd.to_numeric(fa_clean[f], errors=\"coerce\")\n",
    "\n",
    "# đảm bảo ticker, fa_year, fa_quarter có\n",
    "fa_clean[\"ticker\"] = fa_clean[\"ticker\"].astype(str).str.strip().str.upper()\n",
    "# cast year/quarter sang Int64 (nullable) nếu có\n",
    "if \"fa_year\" in fa_clean.columns:\n",
    "    fa_clean[\"fa_year\"] = pd.to_numeric(fa_clean[\"fa_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"fa_quarter\" in fa_clean.columns:\n",
    "    fa_clean[\"fa_quarter\"] = pd.to_numeric(fa_clean[\"fa_quarter\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# drop duplicate (giữ bản cuối cùng theo sort)\n",
    "fa_clean = fa_clean.sort_values([\"ticker\",\"fa_year\",\"fa_quarter\"]).drop_duplicates(subset=[\"ticker\",\"fa_year\",\"fa_quarter\"], keep=\"last\")\n",
    "\n",
    "print(\"FA cleaned: tickers:\", fa_clean[\"ticker\"].nunique(), \"rows:\", len(fa_clean))\n",
    "\n",
    "# ---------- 5) Merge price + FA on (ticker, fa_year, fa_quarter) ----------\n",
    "df_merged = df_price.merge(\n",
    "    fa_clean,\n",
    "    on=[\"ticker\",\"fa_year\",\"fa_quarter\"],\n",
    "    how=\"left\",\n",
    "    sort=True,\n",
    "    suffixes=(\"\",\"_fa\")\n",
    ")\n",
    "\n",
    "# ---------- 6) Forward-fill FA theo ticker (chỉ sử dụng thông tin quá khứ) ----------\n",
    "# Lưu ý: ffill groupby theo timestamp tăng dần đảm bảo không leak (ffill chỉ dùng giá trị trước đó)\n",
    "df_merged = df_merged.sort_values([\"ticker\",\"timestamp\"]).reset_index(drop=True)\n",
    "# Cast fa fields numeric to avoid object issues\n",
    "for f in fa_fields:\n",
    "    if f not in df_merged.columns:\n",
    "        df_merged[f] = np.nan\n",
    "    else:\n",
    "        df_merged[f] = pd.to_numeric(df_merged[f], errors=\"coerce\")\n",
    "\n",
    "# ffill per ticker\n",
    "df_merged[fa_fields] = df_merged.groupby(\"ticker\")[fa_fields].ffill()\n",
    "\n",
    "# ---------- 7) Sanity checks ----------\n",
    "print(\"Merged shape:\", df_merged.shape)\n",
    "print(\"Merged time range:\", df_merged[\"timestamp\"].min(), \"->\", df_merged[\"timestamp\"].max())\n",
    "print(\"Ticker count:\", df_merged[\"ticker\"].nunique())\n",
    "\n",
    "# Báo cáo số row không có FA (sau ffill)\n",
    "no_fa_rows = df_merged[fa_fields].isna().all(axis=1).sum()\n",
    "print(f\"Số dòng vẫn thiếu toàn bộ FA sau ffill: {no_fa_rows} (có thể là ticker hoàn toàn không có báo cáo trước thời điểm đó)\")\n",
    "\n",
    "# ---------- 😎 Lưu merged CSV (backup) ----------\n",
    "df_merged.to_csv(output_merged, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Đã lưu merged -> {output_merged}\")\n",
    "\n",
    "# ---------- 9) Split train / val / test theo timestamp (lưu parquet để dùng tiếp) ----------\n",
    "df_merged[\"timestamp\"] = pd.to_datetime(df_merged[\"timestamp\"])\n",
    "\n",
    "df_train = df_merged[df_merged[\"timestamp\"] <= TRAIN_END].copy()\n",
    "df_val   = df_merged[(df_merged[\"timestamp\"] > TRAIN_END) & (df_merged[\"timestamp\"] <= VAL_END)].copy()\n",
    "df_test  = df_merged[df_merged[\"timestamp\"] > VAL_END].copy()\n",
    "\n",
    "print(\"Split sizes (rows): TRAIN\", len(df_train), \"VAL\", len(df_val), \"TEST\", len(df_test))\n",
    "print(\"TRAIN period:\", df_train[\"timestamp\"].min(), \"->\", df_train[\"timestamp\"].max() if len(df_train)>0 else \"N/A\")\n",
    "print(\"VAL period:\", df_val[\"timestamp\"].min() if len(df_val)>0 else \"N/A\", \"->\", df_val[\"timestamp\"].max() if len(df_val)>0 else \"N/A\")\n",
    "print(\"TEST period:\", df_test[\"timestamp\"].min() if len(df_test)>0 else \"N/A\", \"->\", df_test[\"timestamp\"].max() if len(df_test)>0 else \"N/A\")\n",
    "\n",
    "# Save splits (parquet preserves dtypes)\n",
    "df_train.to_parquet(output_train, index=False)\n",
    "df_val.to_parquet(output_val, index=False)\n",
    "df_test.to_parquet(output_test, index=False)\n",
    "print(f\"✅ Saved splits -> {output_train}, {output_val}, {output_test}\")\n",
    "\n",
    "# ---------- 10) Quick summary per ticker (optional) ----------\n",
    "summary = df_merged.groupby(\"ticker\").agg({\n",
    "    \"timestamp\": [\"min\",\"max\",\"count\"]\n",
    "})\n",
    "summary.columns = [\"first_date\",\"last_date\",\"n_rows\"]\n",
    "summary = summary.reset_index().sort_values(\"n_rows\", ascending=False)\n",
    "print(\"Top tickers by data rows:\")\n",
    "print(summary.head(8))\n",
    "\n",
    "# ---------- End ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08faae38",
   "metadata": {},
   "source": [
    "**Xóa biến df_all không cần thiết nữa để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331523dd",
   "metadata": {},
   "source": [
    "**Block 4: Tính các chỉ số TA dựa vào thư viện FiinQuant và ghép dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged file: vnindex_price_fa_merged.csv\n",
      "Data range: 2020-01-02 → 2025-10-02\n",
      "Using splits:\n",
      " TRAIN: 2020-01-01 → 2023-12-31\n",
      " VAL  : 2024-01-01 → 2024-12-31\n",
      " TEST : 2025-01-01 → 2025-10-02\n",
      "Using client.FiinIndicator() for TA.\n",
      "Processing TRAIN (2020-01-01 → 2023-12-31): rows=376939 | tickers=385\n",
      "Saved TRAIN (2020-01-01 → 2023-12-31) -> vnindex_price_fa_ta_train.csv | shape: (376939, 29)\n",
      "Processing VAL   (2024): rows=96924 | tickers=390\n",
      "Saved VAL   (2024) -> vnindex_price_fa_ta_val.csv | shape: (96924, 29)\n",
      "Processing TEST  (2025→): rows=72221 | tickers=391\n",
      "Saved TEST  (2025→) -> vnindex_price_fa_ta_test.csv | shape: (72221, 29)\n",
      "✅ Block 4 complete. Generated files:\n",
      "  - vnindex_price_fa_ta_train.csv\n",
      "  - vnindex_price_fa_ta_val.csv\n",
      "  - vnindex_price_fa_ta_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 4 — Tính các chỉ số TA cho từng mã (split cố định: Train 2020-2023, Val 2024, Test 2025->)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Config (đã chốt) ----------\n",
    "MERGED_FILE = \"vnindex_price_fa_merged.csv\"   # file output từ Block 3\n",
    "TRAIN_START = pd.Timestamp(\"2020-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2023-12-31\")\n",
    "VAL_START   = pd.Timestamp(\"2024-01-01\")\n",
    "VAL_END     = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")\n",
    "# TEST_END = inferred from data\n",
    "\n",
    "OUT_TRAIN = \"vnindex_price_fa_ta_train.csv\"\n",
    "OUT_VAL   = \"vnindex_price_fa_ta_val.csv\"\n",
    "OUT_TEST  = \"vnindex_price_fa_ta_test.csv\"\n",
    "\n",
    "# ---------- Load merged ----------\n",
    "print(\"Loading merged file:\", MERGED_FILE)\n",
    "df = pd.read_csv(MERGED_FILE, parse_dates=True)\n",
    "\n",
    "# support both 'timestamp' or 'date' column names\n",
    "if \"timestamp\" in df.columns:\n",
    "    df.rename(columns={\"timestamp\":\"date\"}, inplace=True)\n",
    "if \"date\" not in df.columns:\n",
    "    raise ValueError(\"Input merged file must contain a 'timestamp' or 'date' column.\")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "# infer TEST_END as last date in data\n",
    "TEST_END = df[\"date\"].max()\n",
    "print(f\"Data range: {df['date'].min().date()} → {df['date'].max().date()}\")\n",
    "print(\"Using splits:\")\n",
    "print(\" TRAIN:\", TRAIN_START.date(), \"→\", TRAIN_END.date())\n",
    "print(\" VAL  :\", VAL_START.date(), \"→\", VAL_END.date())\n",
    "print(\" TEST :\", TEST_START.date(), \"→\", TEST_END.date())\n",
    "\n",
    "# ---------- TA helper: try client.FiinIndicator else fallback ----------\n",
    "use_fi = False\n",
    "try:\n",
    "    fi = client.FiinIndicator()\n",
    "    if hasattr(fi, \"ema\"):\n",
    "        use_fi = True\n",
    "        print(\"Using client.FiinIndicator() for TA.\")\n",
    "except Exception:\n",
    "    use_fi = False\n",
    "    print(\"FiIndicator not available — using pandas fallback implementations for TA.\")\n",
    "\n",
    "# fallback TA implementations\n",
    "def ema(series, window):\n",
    "    return series.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "def macd_series(close, fast=12, slow=26, signal=9):\n",
    "    ema_fast = ema(close, fast)\n",
    "    ema_slow = ema(close, slow)\n",
    "    macd = ema_fast - ema_slow\n",
    "    macd_signal = ema(macd, signal)\n",
    "    macd_diff = macd - macd_signal\n",
    "    return macd, macd_signal, macd_diff\n",
    "\n",
    "def rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0.0)\n",
    "    down = -delta.clip(upper=0.0)\n",
    "    ma_up = up.rolling(window).mean()\n",
    "    ma_down = down.rolling(window).mean()\n",
    "    rs = ma_up / (ma_down.replace(0, np.nan))\n",
    "    rsi_v = 100 - (100 / (1 + rs))\n",
    "    return rsi_v.fillna(0.0)\n",
    "\n",
    "def bollinger_bands(close, window=20, dev=2):\n",
    "    m = close.rolling(window).mean()\n",
    "    s = close.rolling(window).std()\n",
    "    return m + dev*s, m - dev*s\n",
    "\n",
    "def atr(high, low, close, window=14):\n",
    "    prev_close = close.shift(1)\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - prev_close).abs()\n",
    "    tr3 = (low - prev_close).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    return tr.rolling(window).mean()\n",
    "\n",
    "def obv(close, volume):\n",
    "    sign = np.sign(close.diff().fillna(0.0))\n",
    "    return (sign * volume.fillna(0.0)).cumsum()\n",
    "\n",
    "def vwap(high, low, close, volume, window=14):\n",
    "    tp = (high + low + close) / 3.0\n",
    "    pv = tp * volume\n",
    "    vp = pv.rolling(window).sum()\n",
    "    vv = volume.rolling(window).sum()\n",
    "    return (vp / vv).fillna(np.nan)\n",
    "\n",
    "# ---------- function to compute TA per ticker ----------\n",
    "def add_ta_indicators(df_t):\n",
    "    df_t = df_t.sort_values(\"date\").copy().reset_index(drop=True)\n",
    "    close = df_t[\"close\"].astype(float)\n",
    "\n",
    "    # Prefer fi if available\n",
    "    if use_fi:\n",
    "        try:\n",
    "            df_t['ema_5']  = fi.ema(df_t['close'], window=5)\n",
    "            df_t['ema_20'] = fi.ema(df_t['close'], window=20)\n",
    "            df_t['ema_50'] = fi.ema(df_t['close'], window=50)\n",
    "            df_t['macd']        = fi.macd(df_t['close'], window_fast=12, window_slow=26)\n",
    "            df_t['macd_signal'] = fi.macd_signal(df_t['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "            df_t['macd_diff']   = fi.macd_diff(df_t['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "            df_t['rsi'] = fi.rsi(df_t['close'], window=14)\n",
    "            df_t['bollinger_hband'] = fi.bollinger_hband(df_t['close'], window=20, window_dev=2)\n",
    "            df_t['bollinger_lband'] = fi.bollinger_lband(df_t['close'], window=20, window_dev=2)\n",
    "            df_t['atr'] = fi.atr(df_t['high'], df_t['low'], df_t['close'], window=14)\n",
    "            df_t['obv'] = fi.obv(df_t['close'], df_t['volume'])\n",
    "            df_t['vwap'] = fi.vwap(df_t['high'], df_t['low'], df_t['close'], df_t['volume'], window=14)\n",
    "            return df_t\n",
    "        except Exception:\n",
    "            # fallback to pandas implementations if fi call fails for any ticker\n",
    "            pass\n",
    "\n",
    "    # fallback calculations\n",
    "    df_t['ema_5']  = ema(close, 5)\n",
    "    df_t['ema_20'] = ema(close, 20)\n",
    "    df_t['ema_50'] = ema(close, 50)\n",
    "    macd, macd_signal, macd_diff = macd_series(close, 12, 26, 9)\n",
    "    df_t['macd'] = macd\n",
    "    df_t['macd_signal'] = macd_signal\n",
    "    df_t['macd_diff'] = macd_diff\n",
    "    df_t['rsi'] = rsi(close, 14)\n",
    "    bh, bl = bollinger_bands(close, 20, 2)\n",
    "    df_t['bollinger_hband'] = bh\n",
    "    df_t['bollinger_lband'] = bl\n",
    "\n",
    "    try:\n",
    "        df_t['atr'] = atr(df_t['high'].astype(float), df_t['low'].astype(float), close, window=14)\n",
    "    except Exception:\n",
    "        df_t['atr'] = np.nan\n",
    "\n",
    "    try:\n",
    "        df_t['obv'] = obv(close, df_t['volume'].fillna(0.0).astype(float))\n",
    "    except Exception:\n",
    "        df_t['obv'] = np.nan\n",
    "\n",
    "    try:\n",
    "        df_t['vwap'] = vwap(df_t['high'].astype(float), df_t['low'].astype(float), close, df_t['volume'].fillna(0.0).astype(float), window=14)\n",
    "    except Exception:\n",
    "        df_t['vwap'] = np.nan\n",
    "\n",
    "    return df_t\n",
    "\n",
    "# ---------- process function and save ----------\n",
    "def process_and_save(df_slice, name, out_path):\n",
    "    print(f\"Processing {name}: rows={df_slice.shape[0]} | tickers={df_slice['ticker'].nunique()}\")\n",
    "    res = df_slice.groupby(\"ticker\", group_keys=False).apply(add_ta_indicators)\n",
    "    res = res.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "    res.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {name} -> {out_path} | shape: {res.shape}\")\n",
    "    return res\n",
    "\n",
    "# ---------- create splits ----------\n",
    "df_train = df[(df[\"date\"] >= TRAIN_START) & (df[\"date\"] <= TRAIN_END)].copy()\n",
    "df_val   = df[(df[\"date\"] >= VAL_START)   & (df[\"date\"] <= VAL_END)].copy()\n",
    "df_test  = df[(df[\"date\"] >= TEST_START)  & (df[\"date\"] <= TEST_END)].copy()\n",
    "\n",
    "# safety warnings\n",
    "if df_train.empty:\n",
    "    print(\"⚠️ TRAIN split empty — check data and TRAIN_START/END.\")\n",
    "if df_val.empty:\n",
    "    print(\"⚠️ VAL split empty — check VAL_START/END.\")\n",
    "if df_test.empty:\n",
    "    print(\"⚠️ TEST split empty — check TEST_START / data availability (need 2025+).\")\n",
    "\n",
    "# run\n",
    "df_train_ta = process_and_save(df_train, \"TRAIN (2020-01-01 → 2023-12-31)\", OUT_TRAIN) if not df_train.empty else pd.DataFrame()\n",
    "df_val_ta   = process_and_save(df_val,   \"VAL   (2024)\", OUT_VAL)   if not df_val.empty else pd.DataFrame()\n",
    "df_test_ta  = process_and_save(df_test,  \"TEST  (2025→)\", OUT_TEST)  if not df_test.empty else pd.DataFrame()\n",
    "\n",
    "print(\"✅ Block 4 complete. Generated files:\")\n",
    "print(\"  -\", OUT_TRAIN)\n",
    "print(\"  -\", OUT_VAL)\n",
    "print(\"  -\", OUT_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235303e",
   "metadata": {},
   "source": [
    "**Xóa bớt biến df_merged không còn cần thiết để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26204a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_merged\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614073e",
   "metadata": {},
   "source": [
    "**Block 5: Chuẩn hóa dữ liệu FA và TA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59579dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full TA file: vnindex_price_fa_ta.csv\n",
      "Data timeline: 2023-01-03 → 2025-10-02\n",
      "Computing FA cross-sectional min-max per date...\n",
      "Computing rolling z-score (window=60) per ticker for TA features...\n",
      "After feature construction, rows: 264103 tickers: 391\n",
      "Split sizes (train/val/test): (95000, 18) (96915, 18) (72188, 18)\n",
      "Saved features to:\n",
      "  - ./features/df_features_train.csv\n",
      "  - ./features/df_features_val.csv\n",
      "  - ./features/df_features_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 5 — Feature engineering & scaling (NO regime, compatible with splits)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Config ----------\n",
    "# Feature lists (the ones you specified)\n",
    "fa_features = [\"Debt/Equity\", \"Net Profit Margin (%)\", \"P/E\", \"P/B\"]\n",
    "\n",
    "ta_features = [\n",
    "    \"ema_5\",\"ema_20\",\"ema_50\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"rsi\",\"bollinger_hband\",\"bollinger_lband\",\"atr\",\"obv\",\"vwap\"\n",
    "]\n",
    "\n",
    "ROLL_WINDOW = 60   # rolling window for TA z-score (days)\n",
    "MIN_ROWS_KEEP = 1  # minimum rows per ticker to compute rolling z (adjust if needed)\n",
    "\n",
    "# File paths produced by Block 4\n",
    "FULL_TA_FILE = \"vnindex_price_fa_ta.csv\"   # if exists, use it (preferred)\n",
    "TRAIN_TA_FILE = \"vnindex_price_fa_ta_train.csv\"\n",
    "VAL_TA_FILE   = \"vnindex_price_fa_ta_val.csv\"\n",
    "TEST_TA_FILE  = \"vnindex_price_fa_ta_test.csv\"\n",
    "\n",
    "OUT_DIR = \"./features/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_TRAIN = os.path.join(OUT_DIR, \"df_features_train.csv\")\n",
    "OUT_VAL   = os.path.join(OUT_DIR, \"df_features_val.csv\")\n",
    "OUT_TEST  = os.path.join(OUT_DIR, \"df_features_test.csv\")\n",
    "\n",
    "# Date splits (you confirmed these)\n",
    "TRAIN_START = pd.Timestamp(\"2020-01-01\"); TRAIN_END = pd.Timestamp(\"2023-12-31\")\n",
    "VAL_START   = pd.Timestamp(\"2024-01-01\"); VAL_END   = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")  # TEST_END inferred from data\n",
    "\n",
    "# ---------- Load TA data (prefer full timeline to compute rolling correctly) ----------\n",
    "if os.path.exists(FULL_TA_FILE):\n",
    "    print(\"Loading full TA file:\", FULL_TA_FILE)\n",
    "    df_ta = pd.read_csv(FULL_TA_FILE, parse_dates=[\"date\"] if \"date\" in pd.read_csv(FULL_TA_FILE, nrows=0).columns else [\"timestamp\"])\n",
    "    # normalize column name to 'date'\n",
    "    if \"timestamp\" in df_ta.columns and \"date\" not in df_ta.columns:\n",
    "        df_ta = df_ta.rename(columns={\"timestamp\":\"date\"})\n",
    "else:\n",
    "    # try to load the three split files and concat (they were created earlier in Block 4)\n",
    "    parts = []\n",
    "    for p in [TRAIN_TA_FILE, VAL_TA_FILE, TEST_TA_FILE]:\n",
    "        if os.path.exists(p):\n",
    "            tmp = pd.read_csv(p, parse_dates=[\"date\"] if \"date\" in pd.read_csv(p, nrows=0).columns else [\"timestamp\"])\n",
    "            if \"timestamp\" in tmp.columns and \"date\" not in tmp.columns:\n",
    "                tmp = tmp.rename(columns={\"timestamp\":\"date\"})\n",
    "            parts.append(tmp)\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(\"Cannot find TA input files. Provide vnindex_price_fa_ta.csv or the 3 split files from Block 4.\")\n",
    "    df_ta = pd.concat(parts, ignore_index=True).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "    print(\"Concatenated TA splits; shape:\", df_ta.shape)\n",
    "\n",
    "# ensure date col and sorted\n",
    "if \"date\" not in df_ta.columns:\n",
    "    raise ValueError(\"Input TA dataframe must contain 'date' or 'timestamp' column (normalized to 'date').\")\n",
    "df_ta[\"date\"] = pd.to_datetime(df_ta[\"date\"])\n",
    "df_ta = df_ta.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "TEST_END = df_ta[\"date\"].max()\n",
    "print(\"Data timeline:\", df_ta[\"date\"].min().date(), \"→\", TEST_END.date())\n",
    "\n",
    "# ---------- 1) FA cross-sectional min-max per day ----------\n",
    "# We'll compute per-day min/max across tickers for each FA column, then scale per day.\n",
    "def scale_fa_minmax_group(df_day):\n",
    "    # df_day is all tickers for a single date\n",
    "    out = df_day.copy()\n",
    "    for f in fa_features:\n",
    "        vals = pd.to_numeric(df_day.get(f, pd.Series(dtype=float)), errors=\"coerce\")\n",
    "        vmin = vals.min()\n",
    "        vmax = vals.max()\n",
    "        if np.isfinite(vmin) and np.isfinite(vmax) and (vmax > vmin):\n",
    "            out[f] = (vals - vmin) / (vmax - vmin)\n",
    "        else:\n",
    "            out[f] = np.nan\n",
    "    return out\n",
    "\n",
    "print(\"Computing FA cross-sectional min-max per date...\")\n",
    "# apply per date\n",
    "df_fa_scaled = df_ta.groupby(\"date\", group_keys=False).apply(scale_fa_minmax_group)\n",
    "df_fa_scaled = df_fa_scaled.reset_index(drop=True)\n",
    "\n",
    "# ---------- 2) TA rolling z-score per ticker (computed on full timeline -> safe) ----------\n",
    "print(f\"Computing rolling z-score (window={ROLL_WINDOW}) per ticker for TA features...\")\n",
    "def compute_zscores_for_ticker(g):\n",
    "    g = g.sort_values(\"date\").copy()\n",
    "    for col in ta_features:\n",
    "        if col not in g.columns:\n",
    "            g[col] = np.nan\n",
    "        # compute rolling mean/std and z-score (use center=False so it's causal)\n",
    "        roll_mean = g[col].rolling(window=ROLL_WINDOW, min_periods=1).mean()\n",
    "        roll_std  = g[col].rolling(window=ROLL_WINDOW, min_periods=1).std(ddof=0).replace(0, np.nan)\n",
    "        g[f\"{col}_z\"] = (g[col] - roll_mean) / roll_std\n",
    "    return g\n",
    "\n",
    "df_all = df_fa_scaled.groupby(\"ticker\", group_keys=False).apply(compute_zscores_for_ticker).reset_index(drop=True)\n",
    "\n",
    "# ---------- 3) Keep only required columns and drop rows without features ----------\n",
    "keep_cols = [\"ticker\",\"date\"] + fa_features + [f\"{col}_z\" for col in ta_features]\n",
    "# ensure exist\n",
    "for c in keep_cols:\n",
    "    if c not in df_all.columns:\n",
    "        # if missing, create as NaN (to keep schema)\n",
    "        df_all[c] = np.nan\n",
    "\n",
    "df_features = df_all[keep_cols].copy()\n",
    "\n",
    "# Drop rows with NaN in all features (or you can be stricter: drop if any feature NaN)\n",
    "# Here: require FA and at least one TA_z not-null\n",
    "ta_z_cols = [f\"{col}_z\" for col in ta_features]\n",
    "mask_valid = df_features[fa_features].notna().any(axis=1) & df_features[ta_z_cols].notna().any(axis=1)\n",
    "df_features = df_features[mask_valid].reset_index(drop=True)\n",
    "\n",
    "print(\"After feature construction, rows:\", len(df_features), \"tickers:\", df_features['ticker'].nunique())\n",
    "\n",
    "# ---------- 4) Split back to TRAIN / VAL / TEST and save ----------\n",
    "df_train_feats = df_features[(df_features[\"date\"] >= TRAIN_START) & (df_features[\"date\"] <= TRAIN_END)].copy()\n",
    "df_val_feats   = df_features[(df_features[\"date\"] >= VAL_START)   & (df_features[\"date\"] <= VAL_END)].copy()\n",
    "df_test_feats  = df_features[(df_features[\"date\"] >= TEST_START)  & (df_features[\"date\"] <= TEST_END)].copy()\n",
    "\n",
    "print(\"Split sizes (train/val/test):\", df_train_feats.shape, df_val_feats.shape, df_test_feats.shape)\n",
    "\n",
    "# Save\n",
    "df_train_feats.to_csv(OUT_TRAIN, index=False, encoding=\"utf-8-sig\")\n",
    "df_val_feats.to_csv(OUT_VAL, index=False, encoding=\"utf-8-sig\")\n",
    "df_test_feats.to_csv(OUT_TEST, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Saved features to:\")\n",
    "print(\"  -\", OUT_TRAIN)\n",
    "print(\"  -\", OUT_VAL)\n",
    "print(\"  -\", OUT_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245a6ca",
   "metadata": {},
   "source": [
    "**Block 6: Giảm chiều dữ liệu bằng t-SNE và phân cụm bằng DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used for clustering: ['Debt/Equity', 'Net Profit Margin (%)', 'P/E', 'P/B', 'ema_5_z', 'ema_20_z', 'ema_50_z', 'macd_z', 'macd_signal_z', 'macd_diff_z', 'rsi_z', 'bollinger_hband_z', 'bollinger_lband_z', 'atr_z', 'obv_z', 'vwap_z']\n",
      "Train/Val/Test shapes: (95000, 18) (96915, 18) (72188, 18)\n",
      "Fitting KMeans with K=10 on TRAIN (n=95000 samples)...\n",
      "Saved cluster assignments to: ./clusters/df_clusters_assigned_K10.csv\n",
      "Saved df_clusters_for_tensors to: ./clusters/df_clusters_for_tensors_K10.csv\n",
      "Saved pipeline to: ./clusters/cluster_pipeline_K10.joblib\n",
      "Cluster counts (overall):\n",
      "cluster\n",
      "0    36093\n",
      "1    24082\n",
      "2    15667\n",
      "3    23544\n",
      "4    46406\n",
      "5    18496\n",
      "6    25158\n",
      "7    24518\n",
      "8    20265\n",
      "9    29874\n",
      "Name: count, dtype: int64\n",
      "Train cluster counts:\n",
      "cluster\n",
      "0     9593\n",
      "1    17201\n",
      "2     8306\n",
      "3     7842\n",
      "4    10192\n",
      "5    10874\n",
      "6     8078\n",
      "7     9160\n",
      "8     3216\n",
      "9    10538\n",
      "Name: count, dtype: int64\n",
      "Example rows (clusters):\n",
      "  ticker       date  cluster\n",
      "0    AAA 2023-01-04        1\n",
      "1    AAA 2023-01-05        1\n",
      "2    AAA 2023-01-06        1\n",
      "3    AAA 2023-01-09        1\n",
      "4    AAA 2023-01-10        1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3132"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 6 — Clustering safe: K = 10 on TRAIN + assign to VAL/TEST (fixed)\n",
    "import os, gc, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "TRAIN_FILE = \"./features/df_features_train.csv\"\n",
    "VAL_FILE   = \"./features/df_features_val.csv\"\n",
    "TEST_FILE  = \"./features/df_features_test.csv\"\n",
    "\n",
    "OUT_DIR = \"./clusters/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "K_FIXED = 10          # <- cố định thành 10 theo yêu cầu\n",
    "IMPUTE_STRAT = \"median\"\n",
    "PCA_DIM = None        # hoặc số nguyên (ví dụ 20) nếu muốn giảm chiều trước khi cluster\n",
    "VERBOSE = True\n",
    "# -------------------------------\n",
    "\n",
    "# --- Load splits (Block5 outputs) ---\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "df_val   = pd.read_csv(VAL_FILE)\n",
    "df_test  = pd.read_csv(TEST_FILE)\n",
    "\n",
    "# --- Normalize date column names (accept \"date\" or \"timestamp\") ---\n",
    "def ensure_date_col(df):\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    elif \"timestamp\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    else:\n",
    "        raise ValueError(\"Input file must contain 'date' or 'timestamp' column\")\n",
    "    return df\n",
    "\n",
    "df_train = ensure_date_col(df_train)\n",
    "df_val   = ensure_date_col(df_val)\n",
    "df_test  = ensure_date_col(df_test)\n",
    "\n",
    "# --- Feature columns inference (must match Block5) ---\n",
    "fa_features = [\"Debt/Equity\", \"Net Profit Margin (%)\", \"P/E\", \"P/B\"]\n",
    "ta_z_cols = [c for c in df_train.columns if c.endswith(\"_z\")]\n",
    "feature_cols = [c for c in (fa_features + ta_z_cols) if c in df_train.columns]\n",
    "\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(\"No feature columns found. Check Block5 output and feature names.\")\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"Features used for clustering:\", feature_cols)\n",
    "    print(\"Train/Val/Test shapes:\", df_train.shape, df_val.shape, df_test.shape)\n",
    "\n",
    "# --- Build raw matrices ---\n",
    "X_train_raw = df_train[feature_cols].values\n",
    "X_val_raw   = df_val[feature_cols].values\n",
    "X_test_raw  = df_test[feature_cols].values\n",
    "X_all_raw   = np.vstack([X_train_raw, X_val_raw, X_test_raw])\n",
    "\n",
    "# --- Imputer: fit on TRAIN only ---\n",
    "imp = SimpleImputer(strategy=IMPUTE_STRAT)\n",
    "imp.fit(X_train_raw)             # fit only on train\n",
    "X_train_imp = imp.transform(X_train_raw)\n",
    "X_val_imp   = imp.transform(X_val_raw)\n",
    "X_test_imp  = imp.transform(X_test_raw)\n",
    "X_all_imp   = imp.transform(X_all_raw)\n",
    "\n",
    "# --- Scaler: fit on TRAIN only ---\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imp)\n",
    "X_train_scaled = scaler.transform(X_train_imp)\n",
    "X_val_scaled   = scaler.transform(X_val_imp)\n",
    "X_test_scaled  = scaler.transform(X_test_imp)\n",
    "X_all_scaled   = scaler.transform(X_all_imp)\n",
    "\n",
    "# --- Optional PCA (fit on TRAIN only) ---\n",
    "pca = None\n",
    "if PCA_DIM is not None and isinstance(PCA_DIM, int) and 0 < PCA_DIM < X_train_scaled.shape[1]:\n",
    "    pca = PCA(n_components=PCA_DIM, random_state=RANDOM_STATE)\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_final = pca.transform(X_train_scaled)\n",
    "    X_val_final   = pca.transform(X_val_scaled)\n",
    "    X_test_final  = pca.transform(X_test_scaled)\n",
    "    X_all_final   = pca.transform(X_all_scaled)\n",
    "else:\n",
    "    X_train_final = X_train_scaled\n",
    "    X_val_final   = X_val_scaled\n",
    "    X_test_final  = X_test_scaled\n",
    "    X_all_final   = X_all_scaled\n",
    "\n",
    "# --- KMeans fit on TRAIN only (K_FIXED) ---\n",
    "if VERBOSE:\n",
    "    print(f\"Fitting KMeans with K={K_FIXED} on TRAIN (n={X_train_final.shape[0]} samples)...\")\n",
    "kmeans = KMeans(n_clusters=K_FIXED, random_state=RANDOM_STATE, n_init=10)\n",
    "kmeans.fit(X_train_final)\n",
    "\n",
    "# --- Predict cluster labels for train/val/test (nearest-centroid) ---\n",
    "all_preds = kmeans.predict(X_all_final)\n",
    "\n",
    "n_tr = len(df_train)\n",
    "n_val = len(df_val)\n",
    "n_te = len(df_test)\n",
    "assert len(all_preds) == (n_tr + n_val + n_te)\n",
    "\n",
    "df_train2 = df_train.reset_index(drop=True).copy()\n",
    "df_val2   = df_val.reset_index(drop=True).copy()\n",
    "df_test2  = df_test.reset_index(drop=True).copy()\n",
    "\n",
    "df_train2[\"cluster\"] = all_preds[:n_tr]\n",
    "df_val2[\"cluster\"]   = all_preds[n_tr:n_tr+n_val]\n",
    "df_test2[\"cluster\"]  = all_preds[n_tr+n_val:]\n",
    "\n",
    "# --- Combine and save cluster assignment (useful for next blocks) ---\n",
    "df_clusters_assigned = pd.concat([df_train2, df_val2, df_test2], ignore_index=True)\n",
    "out_csv = os.path.join(OUT_DIR, \"df_clusters_assigned_K10.csv\")\n",
    "df_clusters_assigned.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "if VERBOSE:\n",
    "    print(\"Saved cluster assignments to:\", out_csv)\n",
    "\n",
    "# --- Also produce a compact df_clusters (ticker,date,cluster,month) for Block7 tensor building ---\n",
    "df_clusters = df_clusters_assigned[[\"ticker\",\"date\",\"cluster\"]].copy()\n",
    "df_clusters[\"month\"] = df_clusters[\"date\"].dt.to_period(\"M\")\n",
    "df_clusters_out = os.path.join(OUT_DIR, \"df_clusters_for_tensors_K10.csv\")\n",
    "df_clusters.to_csv(df_clusters_out, index=False, encoding=\"utf-8-sig\")\n",
    "if VERBOSE:\n",
    "    print(\"Saved df_clusters_for_tensors to:\", df_clusters_out)\n",
    "\n",
    "# --- Save pipeline (imputer, scaler, pca, kmeans, feature_cols) for reuse ---\n",
    "pipe = {\n",
    "    \"imputer\": imp,\n",
    "    \"scaler\": scaler,\n",
    "    \"pca\": pca,\n",
    "    \"kmeans\": kmeans,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"train_range\": (df_train[\"date\"].min(), df_train[\"date\"].max())\n",
    "}\n",
    "joblib.dump(pipe, os.path.join(OUT_DIR, f\"cluster_pipeline_K{K_FIXED}.joblib\"))\n",
    "if VERBOSE:\n",
    "    print(\"Saved pipeline to:\", os.path.join(OUT_DIR, f\"cluster_pipeline_K{K_FIXED}.joblib\"))\n",
    "\n",
    "# --- Diagnostics ---\n",
    "if VERBOSE:\n",
    "    print(\"Cluster counts (overall):\")\n",
    "    print(df_clusters_assigned[\"cluster\"].value_counts().sort_index())\n",
    "    print(\"Train cluster counts:\")\n",
    "    print(df_train2[\"cluster\"].value_counts().sort_index())\n",
    "    print(\"Example rows (clusters):\")\n",
    "    print(df_clusters_assigned[[\"ticker\",\"date\",\"cluster\"]].head())\n",
    "\n",
    "# cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde0916",
   "metadata": {},
   "source": [
    "**Block 7: Xây tensors (clusters mapping) và masks (active stocks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor base dir: C:\\tensors_out\n",
      "[WARN] df_clusters missing 'split' column. Attempting to infer by dates if possible.\n",
      "Inferred split column from ./features/*.csv ranges.\n",
      "Found 16 feature columns. Example: ['Debt/Equity', 'Net Profit Margin (%)', 'P/E', 'P/B', 'ema_5_z', 'ema_20_z', 'ema_50_z', 'macd_z']\n",
      "[Saved] split=train cluster=0 windows=193 shape=(193, 20, 350, 16)\n",
      "[Saved] split=train cluster=1 windows=152 shape=(152, 20, 367, 16)\n",
      "[Saved] split=train cluster=2 windows=169 shape=(169, 20, 326, 16)\n",
      "[Saved] split=train cluster=3 windows=209 shape=(209, 20, 372, 16)\n",
      "[Saved] split=train cluster=4 windows=66 shape=(66, 20, 356, 16)\n",
      "[Saved] split=train cluster=5 windows=196 shape=(196, 20, 362, 16)\n",
      "[Saved] split=train cluster=6 windows=212 shape=(212, 20, 317, 16)\n",
      "[Saved] split=train cluster=7 windows=205 shape=(205, 20, 373, 16)\n",
      "[Saved] split=train cluster=8 windows=228 shape=(228, 20, 23, 16)\n",
      "[Saved] split=train cluster=9 windows=206 shape=(206, 20, 366, 16)\n",
      "[Saved] split=val cluster=0 windows=230 shape=(230, 20, 379, 16)\n",
      "[Saved] split=val cluster=1 windows=125 shape=(125, 20, 208, 16)\n",
      "[Saved] split=val cluster=2 windows=180 shape=(180, 20, 248, 16)\n",
      "[Saved] split=val cluster=3 windows=230 shape=(230, 20, 367, 16)\n",
      "[Saved] split=val cluster=4 windows=205 shape=(205, 20, 378, 16)\n",
      "[Saved] split=val cluster=5 windows=130 shape=(130, 20, 214, 16)\n",
      "[Saved] split=val cluster=6 windows=230 shape=(230, 20, 351, 16)\n",
      "[Saved] split=val cluster=7 windows=230 shape=(230, 20, 366, 16)\n",
      "[Saved] split=val cluster=8 windows=230 shape=(230, 20, 22, 16)\n",
      "[Saved] split=val cluster=9 windows=230 shape=(230, 20, 359, 16)\n",
      "[Saved] split=test cluster=0 windows=165 shape=(165, 20, 360, 16)\n",
      "[Saved] split=test cluster=1 windows=75 shape=(75, 20, 303, 16)\n",
      "[Saved] split=test cluster=2 windows=50 shape=(50, 20, 208, 16)\n",
      "[Saved] split=test cluster=3 windows=165 shape=(165, 20, 362, 16)\n",
      "[Saved] split=test cluster=4 windows=123 shape=(123, 20, 349, 16)\n",
      "[Saved] split=test cluster=5 windows=99 shape=(99, 20, 300, 16)\n",
      "[Saved] split=test cluster=6 windows=70 shape=(70, 20, 239, 16)\n",
      "[Saved] split=test cluster=7 windows=165 shape=(165, 20, 362, 16)\n",
      "[Saved] split=test cluster=8 windows=165 shape=(165, 20, 381, 16)\n",
      "[Saved] split=test cluster=9 windows=165 shape=(165, 20, 356, 16)\n",
      "✅ Done Block 7: tensors saved. Index: C:\\tensors_out\\tensor_index.json\n"
     ]
    }
   ],
   "source": [
    "# Block 7 — Tensors & Masks (fixed & compatible)\n",
    "import os, gc, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Config ----------\n",
    "LOOKBACK = 20\n",
    "# sửa path (không có khoảng trắng), mặc định dùng cwd nếu C: không hợp lệ\n",
    "BASE_DIR = Path(\"C:/tensors_out\")\n",
    "if not BASE_DIR.exists():\n",
    "    BASE_DIR = Path(\"./tensors_out\")\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLITS = [\"train\", \"val\", \"test\"]   # will create subfolders per split\n",
    "\n",
    "print(\"Tensor base dir:\", BASE_DIR.resolve())\n",
    "\n",
    "# ---------- Load inputs (try workspace vars, else files) ----------\n",
    "# Expecting: df_clusters_assigned (has columns ['ticker','date','cluster','split']) and df_features (has 'date','ticker',feature columns)\n",
    "if \"df_clusters_assigned\" in globals():\n",
    "    df_clusters_all = df_clusters_assigned.copy()\n",
    "else:\n",
    "    # try to load from cluster outputs folder\n",
    "    candidate = Path(\"./clusters/df_clusters_for_tensors_K10.csv\")\n",
    "    candidate2 = Path(\"./clusters/df_clusters_assigned_K10.csv\")\n",
    "    if candidate.exists():\n",
    "        df_clusters_all = pd.read_csv(candidate, parse_dates=[\"date\"])\n",
    "        print(\"Loaded clusters from\", candidate)\n",
    "    elif candidate2.exists():\n",
    "        df_clusters_all = pd.read_csv(candidate2, parse_dates=[\"date\"])\n",
    "        print(\"Loaded clusters from\", candidate2)\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find df_clusters_assigned in workspace or './clusters/df_clusters_for_tensors_K10.csv'. Please run Block 6 first.\")\n",
    "\n",
    "# df_features: prefer workspace var, else load files saved by Block5\n",
    "if \"df_features\" in globals():\n",
    "    df_features = df_features.copy()\n",
    "else:\n",
    "    # attempt load train/val/test feature files and concat\n",
    "    feat_dir = Path(\"./features\")\n",
    "    train_p = feat_dir / \"df_features_train.csv\"\n",
    "    val_p   = feat_dir / \"df_features_val.csv\"\n",
    "    test_p  = feat_dir / \"df_features_test.csv\"\n",
    "    parts = []\n",
    "    for p in [train_p, val_p, test_p]:\n",
    "        if p.exists():\n",
    "            parts.append(pd.read_csv(p, parse_dates=[\"date\"]))\n",
    "    if parts:\n",
    "        df_features = pd.concat(parts, ignore_index=True).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "        print(\"Loaded df_features from files in ./features/\")\n",
    "    else:\n",
    "        raise ValueError(\"df_features not found in workspace and ./features/*.csv missing. Run Block5 first.\")\n",
    "\n",
    "# ---------- Normalize column names ----------\n",
    "# ensure 'date' and 'ticker' exist in both\n",
    "def ensure_date_col(df, col_candidates=(\"date\",\"timestamp\")):\n",
    "    for c in col_candidates:\n",
    "        if c in df.columns:\n",
    "            if c != \"date\":\n",
    "                df = df.rename(columns={c:\"date\"})\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "            return df\n",
    "    raise ValueError(\"Dataframe must contain a date column named one of: \" + \", \".join(col_candidates))\n",
    "\n",
    "df_clusters_all = ensure_date_col(df_clusters_all)\n",
    "df_features     = ensure_date_col(df_features)\n",
    "\n",
    "# If df_clusters_all lacks 'split' column, try to infer from date ranges in feature split files\n",
    "if \"split\" not in df_clusters_all.columns:\n",
    "    # try to infer using known split date ranges (if available as files)\n",
    "    # fallback: mark all as 'train' (user should provide df_clusters_assigned if they want exact splits)\n",
    "    print(\"[WARN] df_clusters missing 'split' column. Attempting to infer by dates if possible.\")\n",
    "    # basic inference using features file min/max: require train/val/test files exist\n",
    "    train_p = Path(\"./features/df_features_train.csv\")\n",
    "    val_p   = Path(\"./features/df_features_val.csv\")\n",
    "    test_p  = Path(\"./features/df_features_test.csv\")\n",
    "    if train_p.exists() and val_p.exists() and test_p.exists():\n",
    "        tr = pd.read_csv(train_p, parse_dates=[\"date\"])\n",
    "        va = pd.read_csv(val_p, parse_dates=[\"date\"])\n",
    "        te = pd.read_csv(test_p, parse_dates=[\"date\"])\n",
    "        def assign_split(row):\n",
    "            d = row[\"date\"]\n",
    "            if (d >= tr[\"date\"].min()) and (d <= tr[\"date\"].max()):\n",
    "                return \"train\"\n",
    "            if (d >= va[\"date\"].min()) and (d <= va[\"date\"].max()):\n",
    "                return \"val\"\n",
    "            if (d >= te[\"date\"].min()) and (d <= te[\"date\"].max()):\n",
    "                return \"test\"\n",
    "            return \"train\"\n",
    "        df_clusters_all[\"split\"] = df_clusters_all.apply(assign_split, axis=1)\n",
    "        print(\"Inferred split column from ./features/*.csv ranges.\")\n",
    "    else:\n",
    "        df_clusters_all[\"split\"] = \"train\"\n",
    "        print(\"Defaulted split='train' for all rows (no split files found).\")\n",
    "\n",
    "# ---------- Determine feature columns (exclude meta) ----------\n",
    "meta_cols = {\"ticker\",\"date\",\"cluster\",\"split\",\"month\"}\n",
    "# ensure df_features has 'ticker'\n",
    "if \"ticker\" not in df_features.columns:\n",
    "    raise ValueError(\"df_features must contain 'ticker' column.\")\n",
    "feature_cols = [c for c in df_features.columns if c not in meta_cols]\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(\"No feature columns found in df_features. Check Block 5 output.\")\n",
    "\n",
    "print(f\"Found {len(feature_cols)} feature columns. Example: {feature_cols[:8]}\")\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def safe_name(s: str) -> str:\n",
    "    # keep alnum and dash/underscore only\n",
    "    return \"\".join(ch if (ch.isalnum() or ch in (\"-\",\"\")) else \"\" for ch in str(s))\n",
    "\n",
    "# ---------- Prepare output containers ----------\n",
    "tensor_index = []\n",
    "\n",
    "# For speed, convert df_features to multiindex keyed by (ticker,date) for fast join\n",
    "df_features_indexed = df_features.set_index([\"ticker\",\"date\"])\n",
    "\n",
    "# ---------- Iterate splits/clusters ----------\n",
    "for split in SPLITS:\n",
    "    split_dir = BASE_DIR / split\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df_c_split = df_clusters_all[df_clusters_all[\"split\"] == split].copy()\n",
    "    if df_c_split.empty:\n",
    "        print(f\"[WARN] No clusters for split={split}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # group by cluster\n",
    "    for c_id, g in df_c_split.groupby(\"cluster\"):\n",
    "        if int(c_id) == -1:\n",
    "            continue\n",
    "        tickers = sorted(g[\"ticker\"].unique())\n",
    "        if len(tickers) == 0:\n",
    "            continue\n",
    "\n",
    "        # Build list of unique (ticker,date) pairs for this cluster in this split\n",
    "        key = g[[\"ticker\",\"date\"]].drop_duplicates()\n",
    "        # join with df_features using index to keep feature columns aligned\n",
    "        joined_rows = []\n",
    "        for _, row in key.iterrows():\n",
    "            tk = row[\"ticker\"]; dt = pd.to_datetime(row[\"date\"])\n",
    "            try:\n",
    "                feat_row = df_features_indexed.loc[(tk, dt)]\n",
    "                # feat_row can be Series or DataFrame if duplicates; coerce to Series\n",
    "                if isinstance(feat_row, pd.DataFrame):\n",
    "                    feat_row = feat_row.iloc[0]\n",
    "                rec = {\"ticker\": tk, \"date\": dt}\n",
    "                for f in feature_cols:\n",
    "                    rec[f] = feat_row.get(f, np.nan)\n",
    "                joined_rows.append(rec)\n",
    "            except KeyError:\n",
    "                # missing feature row -> create NaNs\n",
    "                rec = {\"ticker\": tk, \"date\": dt}\n",
    "                for f in feature_cols:\n",
    "                    rec[f] = np.nan\n",
    "                joined_rows.append(rec)\n",
    "\n",
    "        if len(joined_rows) == 0:\n",
    "            print(f\"[INFO] cluster {c_id} ({split}) no joined rows -> skip\")\n",
    "            continue\n",
    "\n",
    "        df_join = pd.DataFrame(joined_rows)\n",
    "        # pivot into (T, N, F)\n",
    "        dates = sorted(df_join[\"date\"].unique())\n",
    "        if len(dates) <= LOOKBACK:\n",
    "            print(f\"[INFO] cluster {c_id} ({split}) has {len(dates)} dates <= LOOKBACK({LOOKBACK}), skipping.\")\n",
    "            continue\n",
    "\n",
    "        N = len(tickers); T = len(dates); F = len(feature_cols)\n",
    "        X = np.full((T, N, F), np.nan, dtype=np.float32)\n",
    "\n",
    "        # fill X: for each feature fill pivot table values\n",
    "        # create a mapping ticker -> col index\n",
    "        tk2idx = {tk:i for i,tk in enumerate(tickers)}\n",
    "        date2idx = {d:i for i,d in enumerate(dates)}\n",
    "\n",
    "        for _, r in df_join.iterrows():\n",
    "            ti = tk2idx[r[\"ticker\"]]\n",
    "            di = date2idx[pd.to_datetime(r[\"date\"])]\n",
    "            for fi, f in enumerate(feature_cols):\n",
    "                val = r.get(f, np.nan)\n",
    "                X[di, ti, fi] = np.float32(val) if not pd.isna(val) else np.nan\n",
    "\n",
    "        # mask\n",
    "        M = (~np.isnan(X)).astype(np.uint8)\n",
    "\n",
    "        # forward/backfill along time per (ticker,feature)\n",
    "        for n in range(N):\n",
    "            for fi in range(F):\n",
    "                col = X[:, n, fi]\n",
    "                s = pd.Series(col, index=dates)\n",
    "                s = s.ffill().bfill()\n",
    "                X[:, n, fi] = s.values\n",
    "                M[:, n, fi] = (~np.isnan(X[:, n, fi])).astype(np.uint8)\n",
    "\n",
    "        # sliding windows of LOOKBACK -> B = T - LOOKBACK\n",
    "        cluster_tensors = []\n",
    "        cluster_masks   = []\n",
    "        cluster_dates   = []\n",
    "        for i in range(LOOKBACK, T):\n",
    "            wX = X[i-LOOKBACK:i, :, :].copy()\n",
    "            wM = M[i-LOOKBACK:i, :, :].copy()\n",
    "            # skip windows entirely NaN\n",
    "            if np.all(wM == 0):\n",
    "                continue\n",
    "            cluster_tensors.append(wX)\n",
    "            cluster_masks.append(wM)\n",
    "            cluster_dates.append(dates[i])  # window end date\n",
    "\n",
    "        if len(cluster_tensors) == 0:\n",
    "            print(f\"[INFO] cluster {c_id} ({split}) produced 0 windows, skipping.\")\n",
    "            continue\n",
    "\n",
    "        X_arr = np.stack(cluster_tensors, axis=0).astype(np.float32)  # (B, LOOKBACK, N, F)\n",
    "        M_arr = np.stack(cluster_masks, axis=0).astype(np.uint8)\n",
    "\n",
    "        # filenames\n",
    "        safe_cluster = safe_name(c_id)\n",
    "        tfile = f\"{split}_cluster_{safe_cluster}_tensor.npy\"\n",
    "        mfile = f\"{split}_cluster_{safe_cluster}_mask.npy\"\n",
    "        tpath = split_dir / tfile\n",
    "        mpath = split_dir / mfile\n",
    "\n",
    "        np.save(str(tpath), X_arr)\n",
    "        np.save(str(mpath), M_arr)\n",
    "\n",
    "        tensor_index.append({\n",
    "            \"split\": split,\n",
    "            \"cluster\": int(c_id),\n",
    "            \"tickers\": tickers,\n",
    "            \"n_windows\": int(X_arr.shape[0]),\n",
    "            \"window_size\": int(LOOKBACK),\n",
    "            \"tensor_file\": str(tpath.resolve()),\n",
    "            \"mask_file\": str(mpath.resolve()),\n",
    "            \"dates\": [d.isoformat() for d in cluster_dates]\n",
    "        })\n",
    "\n",
    "        print(f\"[Saved] split={split} cluster={c_id} windows={X_arr.shape[0]} shape={X_arr.shape}\")\n",
    "\n",
    "        # free mem\n",
    "        del X_arr, M_arr, X, M, cluster_tensors, cluster_masks\n",
    "        gc.collect()\n",
    "\n",
    "# Save tensor_index\n",
    "index_path = BASE_DIR / \"tensor_index.json\"\n",
    "with open(index_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(tensor_index, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Done Block 7: tensors saved. Index:\", index_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95417aff",
   "metadata": {},
   "source": [
    "**Block 7.5: Chuẩn bị dữ liệu backtest(loại bỏ các cột dữ liệu không cần thiết nữa)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done Block 7.5: df_backtest sẵn sàng cho reward/SL/TP.\n",
      "  Saved to: ./backtest_ddpg/df_backtest.csv\n",
      "  Shape: (546084, 6)\n",
      "  Unique tickers: 391\n",
      "  Timeline: 2020-01-02 → 2025-10-02\n"
     ]
    }
   ],
   "source": [
    "# Block 7.5 — Chuẩn bị dữ liệu backtest cho reward thật (từ CSV Block 3) — FIXED\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Config ---\n",
    "merged_file = \"vnindex_price_fa_merged.csv\"\n",
    "OUT_DIR = \"./backtest_ddpg/\"\n",
    "OUT_FILE = os.path.join(OUT_DIR, \"df_backtest.csv\")\n",
    "\n",
    "# --- Load kiểm tra ---\n",
    "if not os.path.exists(merged_file):\n",
    "    raise FileNotFoundError(f\"Không tìm thấy '{merged_file}'. Hãy chạy lại Block 3 trước.\")\n",
    "\n",
    "# đọc file, chấp nhận cả 'timestamp' hoặc 'date'\n",
    "df_price = pd.read_csv(merged_file, low_memory=False)\n",
    "# chuẩn hoá column ngày\n",
    "if \"timestamp\" in df_price.columns and \"date\" not in df_price.columns:\n",
    "    df_price = df_price.rename(columns={\"timestamp\": \"date\"})\n",
    "if \"date\" not in df_price.columns:\n",
    "    raise ValueError(\"File merged phải có cột 'timestamp' hoặc 'date'\")\n",
    "\n",
    "df_price[\"date\"] = pd.to_datetime(df_price[\"date\"], errors=\"coerce\")\n",
    "# drop rows có date NaT\n",
    "n_bad_date = df_price[\"date\"].isna().sum()\n",
    "if n_bad_date > 0:\n",
    "    print(f\"[WARN] Bỏ {n_bad_date} dòng có date không hợp lệ.\")\n",
    "    df_price = df_price[df_price[\"date\"].notna()].copy()\n",
    "\n",
    "# --- Kiểm tra presence của OHLC ---\n",
    "required = [\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\"]\n",
    "miss = [c for c in required if c not in df_price.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"Price Data phải chứa các cột: {', '.join(required)}. Thiếu: {', '.join(miss)}\")\n",
    "\n",
    "# --- Ép kiểu số cho giá (loại bỏ giá không hợp lệ) ---\n",
    "for c in [\"open\",\"high\",\"low\",\"close\"]:\n",
    "    # replace commas, whitespace nếu có rồi to_numeric\n",
    "    df_price[c] = pd.to_numeric(df_price[c], errors=\"coerce\")\n",
    "\n",
    "# Remove rows where all four prices are NaN (bad rows)\n",
    "keep_mask = ~(df_price[[\"open\",\"high\",\"low\",\"close\"]].isna().all(axis=1))\n",
    "n_removed = (~keep_mask).sum()\n",
    "if n_removed > 0:\n",
    "    print(f\"[WARN] Loại {n_removed} dòng không có giá hợp lệ trong open/high/low/close.\")\n",
    "    df_price = df_price[keep_mask].copy()\n",
    "\n",
    "# --- Giữ các cột cần thiết và sort ---\n",
    "cols_needed = [\"ticker\",\"date\",\"open\",\"high\",\"low\",\"close\"]\n",
    "df_backtest = df_price[cols_needed].copy()\n",
    "df_backtest = df_backtest.sort_values([\"date\",\"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "# --- Optional: forward/backfill small gaps per ticker (commented out, bật nếu muốn) ---\n",
    "# df_backtest[['open','high','low','close']] = df_backtest.groupby('ticker')[['open','high','low','close']].apply(lambda g: g.ffill().bfill())\n",
    "\n",
    "# --- Summary & save ---\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "df_backtest.to_csv(OUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Done Block 7.5: df_backtest sẵn sàng cho reward/SL/TP.\")\n",
    "print(\"  Saved to:\", OUT_FILE)\n",
    "print(\"  Shape:\", df_backtest.shape)\n",
    "print(\"  Unique tickers:\", int(df_backtest['ticker'].nunique()))\n",
    "print(\"  Timeline:\", df_backtest['date'].min().date(), \"→\", df_backtest['date'].max().date())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7aa4e",
   "metadata": {},
   "source": [
    "**Block 8: Huấn luyện A3C theo từng cụm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "978efcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "df_backtest loaded: dates 2020-01-02 00:00:00 -> 2025-10-02 00:00:00\n",
      "\n",
      "--- TRAIN cluster 0 (split=train) ---\n",
      "  windows=193, LOOKBACK=20, N=350, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 0] Ep 1/6 | loss=-0.015854 actor=-0.005562 critic=0.000217 ent=1.050907\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "  ✅ New best checkpoint saved for cluster 0 (val=0.000183)\n",
      "[Cluster 0] Ep 2/6 | loss=-0.014637 actor=-0.011404 critic=0.000258 ent=0.349111\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 3/6 | loss=-0.008478 actor=-0.007185 critic=0.000267 ent=0.155960\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 4/6 | loss=-0.005977 actor=-0.005140 critic=0.000269 ent=0.110622\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 5/6 | loss=-0.005695 actor=-0.004631 critic=0.000267 ent=0.133128\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 6/6 | loss=-0.005296 actor=-0.003683 critic=0.000261 ent=0.187431\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "--- Finished training cluster 0. Best val score = 0.000183 ---\n",
      "\n",
      "--- TRAIN cluster 1 (split=train) ---\n",
      "  windows=152, LOOKBACK=20, N=367, F=16\n",
      "  Found val windows=125\n",
      "[Cluster 1] Ep 1/6 | loss=-0.011734 actor=-0.001044 critic=0.000267 ent=1.095655\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "  ✅ New best checkpoint saved for cluster 1 (val=0.000000)\n",
      "[Cluster 1] Ep 2/6 | loss=-0.014205 actor=-0.003572 critic=0.000183 ent=1.081557\n",
      "  [VAL] deterministic mean net reward = 0.000176\n",
      "  ✅ New best checkpoint saved for cluster 1 (val=0.000176)\n",
      "[Cluster 1] Ep 3/6 | loss=-0.018261 actor=-0.011722 critic=0.000228 ent=0.676623\n",
      "  [VAL] deterministic mean net reward = 0.000157\n",
      "[Cluster 1] Ep 4/6 | loss=-0.011775 actor=-0.006520 critic=0.000233 ent=0.548828\n",
      "  [VAL] deterministic mean net reward = 0.000172\n",
      "[Cluster 1] Ep 5/6 | loss=-0.011506 actor=-0.006379 critic=0.000235 ent=0.536151\n",
      "  [VAL] deterministic mean net reward = 0.000113\n",
      "[Cluster 1] Ep 6/6 | loss=-0.013885 actor=-0.008528 critic=0.000231 ent=0.558884\n",
      "  [VAL] deterministic mean net reward = 0.000090\n",
      "--- Finished training cluster 1. Best val score = 0.000176 ---\n",
      "\n",
      "--- TRAIN cluster 2 (split=train) ---\n",
      "  windows=169, LOOKBACK=20, N=326, F=16\n",
      "  Found val windows=180\n",
      "[Cluster 2] Ep 1/6 | loss=-0.018738 actor=-0.008425 critic=0.000191 ent=1.050429\n",
      "  [VAL] deterministic mean net reward = 0.000468\n",
      "  ✅ New best checkpoint saved for cluster 2 (val=0.000468)\n",
      "[Cluster 2] Ep 2/6 | loss=-0.013844 actor=-0.011358 critic=0.000261 ent=0.274751\n",
      "  [VAL] deterministic mean net reward = 0.000465\n",
      "[Cluster 2] Ep 3/6 | loss=-0.009350 actor=-0.007598 critic=0.000261 ent=0.201292\n",
      "  [VAL] deterministic mean net reward = 0.000450\n",
      "[Cluster 2] Ep 4/6 | loss=-0.003806 actor=-0.002455 critic=0.000258 ent=0.160889\n",
      "  [VAL] deterministic mean net reward = 0.000445\n",
      "[Cluster 2] Ep 5/6 | loss=-0.004603 actor=-0.003191 critic=0.000256 ent=0.166806\n",
      "  [VAL] deterministic mean net reward = 0.000439\n",
      "[Cluster 2] Ep 6/6 | loss=-0.003231 actor=-0.002046 critic=0.000256 ent=0.144069\n",
      "  [VAL] deterministic mean net reward = 0.000435\n",
      "--- Finished training cluster 2. Best val score = 0.000468 ---\n",
      "\n",
      "--- TRAIN cluster 3 (split=train) ---\n",
      "  windows=209, LOOKBACK=20, N=372, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 3] Ep 1/6 | loss=-0.018508 actor=-0.009636 critic=0.000246 ent=0.911656\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "  ✅ New best checkpoint saved for cluster 3 (val=0.000327)\n",
      "[Cluster 3] Ep 2/6 | loss=-0.012135 actor=-0.010276 critic=0.000269 ent=0.212799\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 3/6 | loss=-0.005502 actor=-0.004333 critic=0.000266 ent=0.143561\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 4/6 | loss=-0.006627 actor=-0.005327 critic=0.000265 ent=0.156387\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 5/6 | loss=-0.005941 actor=-0.005037 critic=0.000266 ent=0.117024\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 6/6 | loss=-0.006652 actor=-0.005456 critic=0.000265 ent=0.146089\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "--- Finished training cluster 3. Best val score = 0.000327 ---\n",
      "\n",
      "--- TRAIN cluster 4 (split=train) ---\n",
      "  windows=66, LOOKBACK=20, N=356, F=16\n",
      "  Found val windows=205\n",
      "[Cluster 4] Ep 1/6 | loss=-0.010954 actor=-0.000232 critic=0.000260 ent=1.098188\n",
      "  [VAL] deterministic mean net reward = 0.000060\n",
      "  ✅ New best checkpoint saved for cluster 4 (val=0.000060)\n",
      "[Cluster 4] Ep 2/6 | loss=-0.011676 actor=-0.000903 critic=0.000206 ent=1.097968\n",
      "  [VAL] deterministic mean net reward = 0.000120\n",
      "  ✅ New best checkpoint saved for cluster 4 (val=0.000120)\n",
      "[Cluster 4] Ep 3/6 | loss=-0.012229 actor=-0.001459 critic=0.000202 ent=1.097259\n",
      "  [VAL] deterministic mean net reward = 0.000120\n",
      "[Cluster 4] Ep 4/6 | loss=-0.013923 actor=-0.003196 critic=0.000206 ent=1.093322\n",
      "  [VAL] deterministic mean net reward = 0.000166\n",
      "  ✅ New best checkpoint saved for cluster 4 (val=0.000166)\n",
      "[Cluster 4] Ep 5/6 | loss=-0.016162 actor=-0.005647 critic=0.000206 ent=1.072116\n",
      "  [VAL] deterministic mean net reward = 0.000098\n",
      "[Cluster 4] Ep 6/6 | loss=-0.016095 actor=-0.006147 critic=0.000224 ent=1.017295\n",
      "  [VAL] deterministic mean net reward = 0.000095\n",
      "--- Finished training cluster 4. Best val score = 0.000166 ---\n",
      "\n",
      "--- TRAIN cluster 5 (split=train) ---\n",
      "  windows=196, LOOKBACK=20, N=362, F=16\n",
      "  Found val windows=130\n",
      "[Cluster 5] Ep 1/6 | loss=-0.013367 actor=-0.002959 critic=0.000473 ent=1.088034\n",
      "  [VAL] deterministic mean net reward = 0.000490\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000490)\n",
      "[Cluster 5] Ep 2/6 | loss=-0.013674 actor=-0.010076 critic=0.000273 ent=0.387061\n",
      "  [VAL] deterministic mean net reward = 0.000493\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000493)\n",
      "[Cluster 5] Ep 3/6 | loss=-0.002845 actor=0.000192 critic=0.000273 ent=0.330968\n",
      "  [VAL] deterministic mean net reward = 0.000498\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000498)\n",
      "[Cluster 5] Ep 4/6 | loss=-0.005307 actor=-0.000379 critic=0.000254 ent=0.518211\n",
      "  [VAL] deterministic mean net reward = 0.000501\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000501)\n",
      "[Cluster 5] Ep 5/6 | loss=-0.009541 actor=-0.005085 critic=0.000258 ent=0.471422\n",
      "  [VAL] deterministic mean net reward = 0.000497\n",
      "[Cluster 5] Ep 6/6 | loss=-0.008438 actor=-0.004905 critic=0.000262 ent=0.379530\n",
      "  [VAL] deterministic mean net reward = 0.000490\n",
      "--- Finished training cluster 5. Best val score = 0.000501 ---\n",
      "\n",
      "--- TRAIN cluster 6 (split=train) ---\n",
      "  windows=212, LOOKBACK=20, N=317, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 6] Ep 1/6 | loss=-0.010721 actor=-0.000248 critic=0.000479 ent=1.095215\n",
      "  [VAL] deterministic mean net reward = 0.000269\n",
      "  ✅ New best checkpoint saved for cluster 6 (val=0.000269)\n",
      "[Cluster 6] Ep 2/6 | loss=-0.019039 actor=-0.010944 critic=0.000230 ent=0.832478\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "  ✅ New best checkpoint saved for cluster 6 (val=0.000304)\n",
      "[Cluster 6] Ep 3/6 | loss=-0.013402 actor=-0.009763 critic=0.000266 ent=0.390501\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "[Cluster 6] Ep 4/6 | loss=-0.011187 actor=-0.008578 critic=0.000272 ent=0.288068\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "[Cluster 6] Ep 5/6 | loss=-0.012215 actor=-0.009773 critic=0.000271 ent=0.271303\n",
      "  [VAL] deterministic mean net reward = 0.000301\n",
      "[Cluster 6] Ep 6/6 | loss=-0.008733 actor=-0.006716 critic=0.000274 ent=0.229057\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "--- Finished training cluster 6. Best val score = 0.000304 ---\n",
      "\n",
      "--- TRAIN cluster 7 (split=train) ---\n",
      "  windows=205, LOOKBACK=20, N=373, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 7] Ep 1/6 | loss=-0.018267 actor=-0.009581 critic=0.000222 ent=0.890817\n",
      "  [VAL] deterministic mean net reward = 0.000298\n",
      "  ✅ New best checkpoint saved for cluster 7 (val=0.000298)\n",
      "[Cluster 7] Ep 2/6 | loss=-0.014787 actor=-0.009948 critic=0.000252 ent=0.509092\n",
      "  [VAL] deterministic mean net reward = 0.000298\n",
      "[Cluster 7] Ep 3/6 | loss=-0.012967 actor=-0.008810 critic=0.000254 ent=0.441055\n",
      "  [VAL] deterministic mean net reward = 0.000298\n",
      "[Cluster 7] Ep 4/6 | loss=-0.008970 actor=-0.004730 critic=0.000248 ent=0.448826\n",
      "  [VAL] deterministic mean net reward = 0.000277\n",
      "[Cluster 7] Ep 5/6 | loss=-0.006311 actor=-0.003435 critic=0.000257 ent=0.313407\n",
      "  [VAL] deterministic mean net reward = 0.000303\n",
      "  ✅ New best checkpoint saved for cluster 7 (val=0.000303)\n",
      "[Cluster 7] Ep 6/6 | loss=-0.005459 actor=-0.002226 critic=0.000258 ent=0.349089\n",
      "  [VAL] deterministic mean net reward = 0.000178\n",
      "--- Finished training cluster 7. Best val score = 0.000303 ---\n",
      "\n",
      "--- TRAIN cluster 8 (split=train) ---\n",
      "  windows=228, LOOKBACK=20, N=23, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 8] Ep 1/6 | loss=-0.010176 actor=0.000608 critic=0.000176 ent=1.095989\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "  ✅ New best checkpoint saved for cluster 8 (val=0.000000)\n",
      "[Cluster 8] Ep 2/6 | loss=-0.012097 actor=-0.001272 critic=0.000135 ent=1.096055\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "[Cluster 8] Ep 3/6 | loss=-0.012550 actor=-0.001719 critic=0.000127 ent=1.095829\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "[Cluster 8] Ep 4/6 | loss=-0.012501 actor=-0.001665 critic=0.000118 ent=1.095394\n",
      "  [VAL] deterministic mean net reward = 0.000017\n",
      "  ✅ New best checkpoint saved for cluster 8 (val=0.000017)\n",
      "[Cluster 8] Ep 5/6 | loss=-0.013428 actor=-0.002595 critic=0.000117 ent=1.094985\n",
      "  [VAL] deterministic mean net reward = 0.000004\n",
      "[Cluster 8] Ep 6/6 | loss=-0.013645 actor=-0.002815 critic=0.000113 ent=1.094250\n",
      "  [VAL] deterministic mean net reward = 0.000074\n",
      "  ✅ New best checkpoint saved for cluster 8 (val=0.000074)\n",
      "--- Finished training cluster 8. Best val score = 0.000074 ---\n",
      "\n",
      "--- TRAIN cluster 9 (split=train) ---\n",
      "  windows=206, LOOKBACK=20, N=366, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 9] Ep 1/6 | loss=-0.018577 actor=-0.009849 critic=0.000264 ent=0.899189\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "  ✅ New best checkpoint saved for cluster 9 (val=0.000334)\n",
      "[Cluster 9] Ep 2/6 | loss=-0.010796 actor=-0.008579 critic=0.000266 ent=0.248318\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 3/6 | loss=-0.009362 actor=-0.007094 critic=0.000267 ent=0.253503\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 4/6 | loss=-0.013233 actor=-0.010353 critic=0.000261 ent=0.314050\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 5/6 | loss=-0.008836 actor=-0.006354 critic=0.000263 ent=0.274492\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 6/6 | loss=-0.008066 actor=-0.006306 critic=0.000264 ent=0.202367\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "--- Finished training cluster 9. Best val score = 0.000334 ---\n",
      "\n",
      "=== Running inference on split=test and writing to signals\\a3c_signals_infer.csv ===\n",
      "[INF] cluster=0 split=test\n",
      "  [INF] cluster 0 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=1 split=test\n",
      "  [INF] cluster 1 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=2 split=test\n",
      "  [INF] cluster 2 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=3 split=test\n",
      "  [INF] cluster 3 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=4 split=test\n",
      "  [INF] cluster 4 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=5 split=test\n",
      "  [INF] cluster 5 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=6 split=test\n",
      "  [INF] cluster 6 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=7 split=test\n",
      "  [INF] cluster 7 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=8 split=test\n",
      "  [INF] cluster 8 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=9 split=test\n",
      "  [INF] cluster 9 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "\n",
      "✅ Block 8 complete: training done and inference saved to signals\\a3c_signals_infer.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 8 — A3C per-cluster (full, stable, val-checkpoint, inference)\n",
    "import os, gc, json, csv, math, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========== CONFIG / PATH ===========\n",
    "BASE_TENSOR_DIR = Path(\"C:/tensors_out\")         # nơi có train/val/test subfolders + tensor_index.json\n",
    "TENSOR_INDEX = BASE_TENSOR_DIR / \"tensor_index.json\"\n",
    "DF_BACKTEST = Path(\"./backtest_ddpg/df_backtest.csv\")   # nếu có -> dùng để tính reward next-day\n",
    "SIG_DIR = Path(\"./signals/\")\n",
    "MODEL_DIR = Path(\"./models_a3c/\")\n",
    "SIG_TRAIN_FILE = SIG_DIR / \"a3c_signals_train.csv\"\n",
    "SIG_INFER_FILE = SIG_DIR / \"a3c_signals_infer.csv\"\n",
    "\n",
    "SIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========== HYPER (tune) ===========\n",
    "EPOCHS = 6               # epochs per cluster (increase later)\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-4\n",
    "HIDDEN = 128\n",
    "ENTROPY_BETA = 0.01\n",
    "CRITIC_COEF = 0.5\n",
    "GRAD_CLIP = 1.0\n",
    "REWARD_SCALE = 1.0\n",
    "SEED = 42\n",
    "LOOKBACK_EXPECT = None   # None = inferred per-cluster\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# =========== DEVICE ===========\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# =========== SAFETY helpers ===========\n",
    "EPS = 1e-8\n",
    "\n",
    "def safe_tensor_np(x, dtype=np.float32):\n",
    "    a = np.array(x, dtype=dtype)\n",
    "    # replace inf/nan\n",
    "    a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return a\n",
    "\n",
    "def safe_tensor(x, dtype=torch.float32):\n",
    "    t = torch.tensor(np.array(x), dtype=dtype, device=DEVICE)\n",
    "    t = torch.where(torch.isfinite(t), t, torch.zeros_like(t))\n",
    "    return t\n",
    "\n",
    "def is_bad(t: torch.Tensor):\n",
    "    return torch.isnan(t).any().item() or torch.isinf(t).any().item()\n",
    "\n",
    "# =========== MODEL ===========\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden, max(32, hidden//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(32, hidden//2), 3)   # logits: short(0), flat(1), long(2)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden, max(32, hidden//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(32, hidden//2), 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]                          # (B, hidden)\n",
    "        logits = self.actor(h)                     # (B, 3)\n",
    "        value = self.critic(h).squeeze(-1)         # (B,)\n",
    "        return logits, value\n",
    "\n",
    "# =========== IO helpers (split-aware) ===========\n",
    "def resolve_path_rel_to_split(path_str: str, split: str):\n",
    "    \"\"\"\n",
    "    If given path_str exists -> return Path(path_str).\n",
    "    Else, attempt BASE_TENSOR_DIR / split / basename(path_str).\n",
    "    \"\"\"\n",
    "    p = Path(path_str)\n",
    "    if p.exists():\n",
    "        return p\n",
    "    candidate = BASE_TENSOR_DIR / split / p.name\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    # fallback: try under base dir directly\n",
    "    candidate2 = BASE_TENSOR_DIR / p.name\n",
    "    if candidate2.exists():\n",
    "        return candidate2\n",
    "    # final fallback: return expected candidate (may raise later)\n",
    "    return candidate\n",
    "\n",
    "# =========== LOAD tensor_index ===========\n",
    "if not TENSOR_INDEX.exists():\n",
    "    raise FileNotFoundError(f\"tensor_index.json not found at {TENSOR_INDEX}. Run Block 7 first.\")\n",
    "with open(TENSOR_INDEX, \"r\", encoding=\"utf-8\") as fh:\n",
    "    tensor_index_all = json.load(fh)\n",
    "\n",
    "# =========== LOAD price (backtest) for reward if available ===========\n",
    "USE_PRICE_REWARD = DF_BACKTEST.exists()\n",
    "if USE_PRICE_REWARD:\n",
    "    df_back = pd.read_csv(DF_BACKTEST, parse_dates=[\"date\"])\n",
    "    df_back = df_back.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "    px_close = df_back.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "    print(\"df_backtest loaded: dates\", px_close.index.min(), \"->\", px_close.index.max())\n",
    "else:\n",
    "    px_close = None\n",
    "    print(\"No df_backtest.csv found -> training with synthetic/zero reward unless user supplies df_backtest.csv\")\n",
    "\n",
    "# ---------- helper: compute next-day log return for arrays of dates & tickers ----------\n",
    "def compute_nextday_logret(dates_iso: List[str], tickers: List[str]):\n",
    "    \"\"\"\n",
    "    dates_iso: list of window-end dates (ISO strings). returns array shape (B, N)\n",
    "    For each date d, compute log(close[next_trading_day, ticker] / close[d, ticker])\n",
    "    If px_close missing -> return zeros.\n",
    "    \"\"\"\n",
    "    if px_close is None:\n",
    "        return np.zeros((len(dates_iso), len(tickers)), dtype=np.float32)\n",
    "\n",
    "    d_idx = pd.to_datetime(dates_iso)\n",
    "    # find nearest index positions (<= d), then pos+1 -> next trading day if exists\n",
    "    all_index = px_close.index\n",
    "    res = np.zeros((len(d_idx), len(tickers)), dtype=np.float32)\n",
    "    for i, d in enumerate(d_idx):\n",
    "        # get location of nearest index <= d\n",
    "        pos = all_index.get_indexer([d], method='pad')[0]\n",
    "        pos_next = pos + 1 if (pos + 1) < len(all_index) else pos\n",
    "        date_curr = all_index[pos]\n",
    "        date_next = all_index[pos_next]\n",
    "        # slice\n",
    "        v_curr = px_close.loc[date_curr, tickers].values.astype(np.float32)\n",
    "        v_next = px_close.loc[date_next, tickers].values.astype(np.float32)\n",
    "        # safe division\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            r = np.log(np.divide(v_next, np.maximum(v_curr, 1e-9)))\n",
    "        r = np.nan_to_num(r, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        res[i, :] = r\n",
    "    return res\n",
    "\n",
    "# =========== safe loader for tensor meta ===========\n",
    "def load_tensor_meta(meta: Dict):\n",
    "    \"\"\"\n",
    "    meta expected keys: split, cluster, tensor_file, mask_file, tickers, dates (list), dates_shifted (list optional)\n",
    "    returns: X (B, LOOKBACK, N, F), M (B, LOOKBACK, N, F), tickers(list), dates(list), split(str)\n",
    "    \"\"\"\n",
    "    split = meta.get(\"split\", \"train\")\n",
    "    tpath = resolve_path_rel_to_split(meta[\"tensor_file\"], split)\n",
    "    mpath = resolve_path_rel_to_split(meta[\"mask_file\"], split)\n",
    "    if not tpath.exists() or not mpath.exists():\n",
    "        raise FileNotFoundError(f\"Tensor/Mask not found for cluster {meta.get('cluster')} split={split}: {tpath}, {mpath}\")\n",
    "    X = np.load(tpath, mmap_mode=\"r\")\n",
    "    M = np.load(mpath, mmap_mode=\"r\")\n",
    "    tickers = list(meta.get(\"tickers\", []))\n",
    "    dates = [pd.to_datetime(d) for d in meta.get(\"dates\", [])]\n",
    "    dates_shifted = [pd.to_datetime(d) for d in meta.get(\"dates_shifted\", [])] if meta.get(\"dates_shifted\") else None\n",
    "    return X, M, tickers, dates, dates_shifted, split\n",
    "\n",
    "# =========== Training per-cluster (with val checkpointing) ===========\n",
    "def train_cluster(meta, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, save_dir=MODEL_DIR):\n",
    "    cluster_id = int(meta.get(\"cluster\"))\n",
    "    split = meta.get(\"split\", \"train\")\n",
    "    if split != \"train\":\n",
    "        print(f\"Skipping non-train meta cluster {cluster_id} (split={split})\")\n",
    "        return None\n",
    "    print(f\"\\n--- TRAIN cluster {cluster_id} (split=train) ---\")\n",
    "    # load train meta\n",
    "    try:\n",
    "        X_tr, M_tr, tickers_tr, dates_tr, dates_shifted_tr, _ = load_tensor_meta(meta)\n",
    "    except Exception as e:\n",
    "        print(\"  [ERROR] load tensor:\", e)\n",
    "        return None\n",
    "    B, LOOKBACK, N, F = X_tr.shape\n",
    "    print(f\"  windows={B}, LOOKBACK={LOOKBACK}, N={N}, F={F}\")\n",
    "\n",
    "    # find val meta for same cluster if exists\n",
    "    val_meta = None\n",
    "    for m in tensor_index_all:\n",
    "        if int(m.get(\"cluster\"))==cluster_id and m.get(\"split\",\"\")==\"val\":\n",
    "            val_meta = m; break\n",
    "    if val_meta:\n",
    "        X_val, M_val, tickers_val, dates_val, dates_shifted_val, _ = load_tensor_meta(val_meta)\n",
    "        print(f\"  Found val windows={X_val.shape[0]}\")\n",
    "        # precompute val rewards\n",
    "        r_val = compute_nextday_logret([d.isoformat() for d in dates_val], tickers_val) * REWARD_SCALE\n",
    "    else:\n",
    "        X_val = M_val = tickers_val = dates_val = dates_shifted_val = None\n",
    "        r_val = None\n",
    "\n",
    "    # precompute train rewards (B, N)\n",
    "    r_tr = compute_nextday_logret([d.isoformat() for d in dates_tr], tickers_tr) * REWARD_SCALE\n",
    "\n",
    "    # safe conversions\n",
    "    X_tr = np.nan_to_num(X_tr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    M_tr = np.nan_to_num(M_tr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    # model & optimizer\n",
    "    model = A3CNet(n_features=F, hidden=HIDDEN).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_score = -1e9\n",
    "    best_path = save_dir / f\"a3c_cluster_{cluster_id}_best.pt\"\n",
    "    last_path = save_dir / f\"a3c_cluster_{cluster_id}_last.pt\"\n",
    "\n",
    "    total_samples = B * N\n",
    "    indices = np.arange(total_samples)\n",
    "\n",
    "    def batch_iter(shuffle=True):\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for start in range(0, total_samples, batch_size):\n",
    "            end = min(total_samples, start + batch_size)\n",
    "            batch_idx = indices[start:end]\n",
    "            xb_list, mb_list, rb_list = [], [], []\n",
    "            for s in batch_idx:\n",
    "                b, n = divmod(int(s), N)\n",
    "                seq = X_tr[b, :, n, :]          # (LOOKBACK, F)\n",
    "                mask = M_tr[b, :, n, :]         # (LOOKBACK, F)\n",
    "                xb_list.append(seq)\n",
    "                mb_list.append(mask)\n",
    "                rb_list.append(r_tr[b, n])\n",
    "            yield np.stack(xb_list, axis=0), np.stack(mb_list, axis=0), np.array(rb_list, dtype=np.float32), batch_idx\n",
    "\n",
    "    # training loop\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        ep_loss = 0.0; ep_actor = 0.0; ep_critic = 0.0; ep_ent = 0.0; ep_batches = 0\n",
    "        for xb_np, mb_np, rb_np, batch_idx in batch_iter(shuffle=True):\n",
    "            # to tensors\n",
    "            xb = safe_tensor(xb_np); mb = safe_tensor(mb_np); rb = safe_tensor(rb_np)\n",
    "            # apply mask to inputs\n",
    "            xb = xb * mb\n",
    "            # forward\n",
    "            logits, vals = model(xb)                 # logits (B,3), vals (B,)\n",
    "            if is_bad(logits) or is_bad(vals) or is_bad(rb):\n",
    "                print(\"  [WARN] bad numerics in forward -> skip batch\")\n",
    "                continue\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            acts = dist.sample()                     # (B,)\n",
    "            logp = torch.log_softmax(logits, dim=-1)\n",
    "            logp_act = logp.gather(1, acts.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # map action -> reward: long(2) => +r, short(0) => -r, flat => 0\n",
    "            reward_t = torch.where(acts==2, rb, torch.where(acts==0, -rb, torch.zeros_like(rb)))\n",
    "\n",
    "            # advantage\n",
    "            adv = reward_t - vals\n",
    "            adv_mean = adv.mean()\n",
    "            adv_std = adv.std(unbiased=False) + EPS\n",
    "            adv_norm = (adv - adv_mean) / adv_std\n",
    "\n",
    "            actor_loss = -(logp_act * adv_norm.detach()).mean()\n",
    "            critic_loss = CRITIC_COEF * (adv.pow(2).mean())\n",
    "            entropy = - (torch.softmax(logits, dim=-1) * logp).sum(-1).mean()\n",
    "            loss = actor_loss + critic_loss - ENTROPY_BETA * entropy\n",
    "\n",
    "            if not math.isfinite(float(loss.item())):\n",
    "                print(\"  [WARN] loss not finite -> skip batch\")\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            opt.step()\n",
    "\n",
    "            ep_loss += float(loss.item()); ep_actor += float(actor_loss.item())\n",
    "            ep_critic += float(critic_loss.item()); ep_ent += float(entropy.item()); ep_batches += 1\n",
    "\n",
    "            # free small\n",
    "            del xb, mb, rb, logits, vals, dist, acts, logp, logp_act, reward_t, adv, adv_norm\n",
    "            gc.collect()\n",
    "\n",
    "        if ep_batches == 0:\n",
    "            print(f\"  [WARN] no batches processed in epoch {ep}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[Cluster {cluster_id}] Ep {ep}/{epochs} | loss={ep_loss/ep_batches:.6f} actor={ep_actor/ep_batches:.6f} critic={ep_critic/ep_batches:.6f} ent={ep_ent/ep_batches:.6f}\")\n",
    "\n",
    "        # validation evaluation (deterministic argmax if val exists)\n",
    "        if X_val is not None:\n",
    "            model.eval()\n",
    "            # build val inference in batches\n",
    "            val_total = X_val.shape[0] * X_val.shape[2]\n",
    "            val_batchsize = batch_size\n",
    "            val_rewards = []\n",
    "            for start in range(0, val_total, val_batchsize):\n",
    "                xb_list = []; rb_list = []\n",
    "                for s in range(start, min(val_total, start+val_batchsize)):\n",
    "                    b, n = divmod(int(s), X_val.shape[2])\n",
    "                    seq = X_val[b, :, n, :]\n",
    "                    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                    xb_list.append(seq)\n",
    "                    rb_list.append(r_val[b, n])\n",
    "                if not xb_list:\n",
    "                    continue\n",
    "                xb_t = safe_tensor(np.stack(xb_list, axis=0))\n",
    "                logits_v, vals_v = model(xb_t)\n",
    "                acts_v = torch.argmax(logits_v, dim=-1)\n",
    "                rb_t = safe_tensor(np.array(rb_list, dtype=np.float32))\n",
    "                rdet = torch.where(acts_v==2, rb_t, torch.where(acts_v==0, -rb_t, torch.zeros_like(rb_t)))\n",
    "                val_rewards.append(rdet.cpu().numpy())\n",
    "                del xb_t, logits_v, vals_v, acts_v, rb_t, rdet\n",
    "            if val_rewards:\n",
    "                val_arr = np.concatenate(val_rewards, axis=0)\n",
    "                val_score = float(val_arr.mean())\n",
    "            else:\n",
    "                val_score = -1e9\n",
    "            print(f\"  [VAL] deterministic mean net reward = {val_score:.6f}\")\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                torch.save(model.state_dict(), best_path)\n",
    "                print(f\"  ✅ New best checkpoint saved for cluster {cluster_id} (val={val_score:.6f})\")\n",
    "\n",
    "        # always save last\n",
    "        torch.save(model.state_dict(), last_path)\n",
    "\n",
    "    # end epochs\n",
    "    print(f\"--- Finished training cluster {cluster_id}. Best val score = {best_val_score:.6f} ---\")\n",
    "    ckpt = best_path if best_path.exists() else last_path\n",
    "    return str(ckpt)\n",
    "\n",
    "# =========== Inference (deterministic argmax) ===========\n",
    "def infer_on_split(output_csv: str, split_name: str = \"test\"):\n",
    "    # reset file\n",
    "    if os.path.exists(output_csv):\n",
    "        os.remove(output_csv)\n",
    "    with open(output_csv, \"w\", newline=\"\") as fh:\n",
    "        csv.writer(fh).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "    for meta in tensor_index_all:\n",
    "        if meta.get(\"split\") != split_name:\n",
    "            continue\n",
    "        cluster_id = int(meta.get(\"cluster\"))\n",
    "        print(f\"[INF] cluster={cluster_id} split={split_name}\")\n",
    "        try:\n",
    "            X, M, tickers, dates, dates_shifted, split = load_tensor_meta(meta)\n",
    "        except Exception as e:\n",
    "            print(\"  [WARN] load tensor failed:\", e); continue\n",
    "        B, LOOKBACK, N, F = X.shape\n",
    "        # choose checkpoint: best else last\n",
    "        best_path = MODEL_DIR / f\"a3c_cluster_{cluster_id}_best.pt\"\n",
    "        last_path = MODEL_DIR / f\"a3c_cluster_{cluster_id}_last.pt\"\n",
    "        model_path = best_path if best_path.exists() else last_path if last_path.exists() else None\n",
    "        if model_path is None:\n",
    "            print(f\"  [WARN] no checkpoint for cluster {cluster_id}, skipping\")\n",
    "            continue\n",
    "        model = A3CNet(n_features=F, hidden=HIDDEN).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.eval()\n",
    "        total = B * N\n",
    "        with open(output_csv, \"a\", newline=\"\") as fh:\n",
    "            w = csv.writer(fh)\n",
    "            for start in range(0, total, BATCH_SIZE):\n",
    "                xb_list = []; idx_list = []\n",
    "                for s in range(start, min(total, start+BATCH_SIZE)):\n",
    "                    b, n = divmod(int(s), N)\n",
    "                    seq = X[b, :, n, :]\n",
    "                    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                    xb_list.append(seq); idx_list.append((b,n))\n",
    "                if not xb_list:\n",
    "                    continue\n",
    "                xb_t = safe_tensor(np.stack(xb_list, axis=0))\n",
    "                logits, vals = model(xb_t)\n",
    "                acts = torch.argmax(logits, dim=-1).cpu().numpy()    # 0,1,2\n",
    "                acts_mapped = np.where(acts==2, 1, np.where(acts==0, -1, 0))\n",
    "                for k, (b,n) in enumerate(idx_list):\n",
    "                    out_date = dates_shifted[b].isoformat() if dates_shifted is not None and len(dates_shifted)>b else pd.to_datetime(dates[b]).date().isoformat()\n",
    "                    w.writerow([out_date, tickers[n], int(acts_mapped[k])])\n",
    "                del xb_t, logits, vals, acts\n",
    "                gc.collect()\n",
    "        print(f\"  [INF] cluster {cluster_id} -> appended signals to {output_csv}\")\n",
    "\n",
    "# =========== MAIN flow ===========\n",
    "# 1) Train clusters that have split=train entries (iterate over tensor_index_all)\n",
    "trained = {}\n",
    "for meta in tensor_index_all:\n",
    "    if meta.get(\"split\") != \"train\":\n",
    "        continue\n",
    "    try:\n",
    "        ckpt = train_cluster(meta, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)\n",
    "        if ckpt:\n",
    "            trained[int(meta.get(\"cluster\"))] = ckpt\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] training cluster {meta.get('cluster')}: {e}\")\n",
    "\n",
    "# 2) After training, run inference on split=test and save CSV\n",
    "print(\"\\n=== Running inference on split=test and writing to\", SIG_INFER_FILE, \"===\")\n",
    "infer_on_split(str(SIG_INFER_FILE), split_name=\"test\")\n",
    "print(\"\\n✅ Block 8 complete: training done and inference saved to\", SIG_INFER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915b1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "df_backtest loaded: dates 2020-01-02 00:00:00 -> 2025-10-02 00:00:00\n",
      "\n",
      "--- TRAIN cluster 0 (split=train) ---\n",
      "  windows=193, LOOKBACK=20, N=350, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 0] Ep 1/6 | loss=-0.015854 actor=-0.005562 critic=0.000217 ent=1.050907\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "  ✅ New best checkpoint saved for cluster 0 (val=0.000183)\n",
      "[Cluster 0] Ep 2/6 | loss=-0.014637 actor=-0.011404 critic=0.000258 ent=0.349111\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 3/6 | loss=-0.008478 actor=-0.007185 critic=0.000267 ent=0.155960\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 4/6 | loss=-0.005977 actor=-0.005140 critic=0.000269 ent=0.110622\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 5/6 | loss=-0.005695 actor=-0.004631 critic=0.000267 ent=0.133128\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "[Cluster 0] Ep 6/6 | loss=-0.005296 actor=-0.003683 critic=0.000261 ent=0.187431\n",
      "  [VAL] deterministic mean net reward = 0.000183\n",
      "--- Finished training cluster 0. Best val score = 0.000183 ---\n",
      "\n",
      "--- TRAIN cluster 1 (split=train) ---\n",
      "  windows=152, LOOKBACK=20, N=367, F=16\n",
      "  Found val windows=125\n",
      "[Cluster 1] Ep 1/6 | loss=-0.011734 actor=-0.001044 critic=0.000267 ent=1.095655\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "  ✅ New best checkpoint saved for cluster 1 (val=0.000000)\n",
      "[Cluster 1] Ep 2/6 | loss=-0.014205 actor=-0.003572 critic=0.000183 ent=1.081557\n",
      "  [VAL] deterministic mean net reward = 0.000176\n",
      "  ✅ New best checkpoint saved for cluster 1 (val=0.000176)\n",
      "[Cluster 1] Ep 3/6 | loss=-0.018261 actor=-0.011722 critic=0.000228 ent=0.676623\n",
      "  [VAL] deterministic mean net reward = 0.000157\n",
      "[Cluster 1] Ep 4/6 | loss=-0.011775 actor=-0.006520 critic=0.000233 ent=0.548828\n",
      "  [VAL] deterministic mean net reward = 0.000172\n",
      "[Cluster 1] Ep 5/6 | loss=-0.011506 actor=-0.006379 critic=0.000235 ent=0.536151\n",
      "  [VAL] deterministic mean net reward = 0.000113\n",
      "[Cluster 1] Ep 6/6 | loss=-0.013885 actor=-0.008528 critic=0.000231 ent=0.558884\n",
      "  [VAL] deterministic mean net reward = 0.000090\n",
      "--- Finished training cluster 1. Best val score = 0.000176 ---\n",
      "\n",
      "--- TRAIN cluster 2 (split=train) ---\n",
      "  windows=169, LOOKBACK=20, N=326, F=16\n",
      "  Found val windows=180\n",
      "[Cluster 2] Ep 1/6 | loss=-0.018738 actor=-0.008425 critic=0.000191 ent=1.050429\n",
      "  [VAL] deterministic mean net reward = 0.000468\n",
      "  ✅ New best checkpoint saved for cluster 2 (val=0.000468)\n",
      "[Cluster 2] Ep 2/6 | loss=-0.013844 actor=-0.011358 critic=0.000261 ent=0.274751\n",
      "  [VAL] deterministic mean net reward = 0.000465\n",
      "[Cluster 2] Ep 3/6 | loss=-0.009350 actor=-0.007598 critic=0.000261 ent=0.201292\n",
      "  [VAL] deterministic mean net reward = 0.000450\n",
      "[Cluster 2] Ep 4/6 | loss=-0.003806 actor=-0.002455 critic=0.000258 ent=0.160889\n",
      "  [VAL] deterministic mean net reward = 0.000445\n",
      "[Cluster 2] Ep 5/6 | loss=-0.004603 actor=-0.003191 critic=0.000256 ent=0.166806\n",
      "  [VAL] deterministic mean net reward = 0.000439\n",
      "[Cluster 2] Ep 6/6 | loss=-0.003231 actor=-0.002046 critic=0.000256 ent=0.144069\n",
      "  [VAL] deterministic mean net reward = 0.000435\n",
      "--- Finished training cluster 2. Best val score = 0.000468 ---\n",
      "\n",
      "--- TRAIN cluster 3 (split=train) ---\n",
      "  windows=209, LOOKBACK=20, N=372, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 3] Ep 1/6 | loss=-0.018508 actor=-0.009636 critic=0.000246 ent=0.911656\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "  ✅ New best checkpoint saved for cluster 3 (val=0.000327)\n",
      "[Cluster 3] Ep 2/6 | loss=-0.012135 actor=-0.010276 critic=0.000269 ent=0.212799\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 3/6 | loss=-0.005502 actor=-0.004333 critic=0.000266 ent=0.143561\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 4/6 | loss=-0.006627 actor=-0.005327 critic=0.000265 ent=0.156387\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 5/6 | loss=-0.005941 actor=-0.005037 critic=0.000266 ent=0.117024\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "[Cluster 3] Ep 6/6 | loss=-0.006652 actor=-0.005456 critic=0.000265 ent=0.146089\n",
      "  [VAL] deterministic mean net reward = 0.000327\n",
      "--- Finished training cluster 3. Best val score = 0.000327 ---\n",
      "\n",
      "--- TRAIN cluster 4 (split=train) ---\n",
      "  windows=66, LOOKBACK=20, N=356, F=16\n",
      "  Found val windows=205\n",
      "[Cluster 4] Ep 1/6 | loss=-0.010954 actor=-0.000232 critic=0.000260 ent=1.098188\n",
      "  [VAL] deterministic mean net reward = 0.000060\n",
      "  ✅ New best checkpoint saved for cluster 4 (val=0.000060)\n",
      "[Cluster 4] Ep 2/6 | loss=-0.011676 actor=-0.000903 critic=0.000206 ent=1.097968\n",
      "  [VAL] deterministic mean net reward = 0.000120\n",
      "  ✅ New best checkpoint saved for cluster 4 (val=0.000120)\n",
      "[Cluster 4] Ep 3/6 | loss=-0.012229 actor=-0.001459 critic=0.000202 ent=1.097259\n",
      "  [VAL] deterministic mean net reward = 0.000120\n",
      "[Cluster 4] Ep 4/6 | loss=-0.013923 actor=-0.003196 critic=0.000206 ent=1.093322\n",
      "  [VAL] deterministic mean net reward = 0.000166\n",
      "  ✅ New best checkpoint saved for cluster 4 (val=0.000166)\n",
      "[Cluster 4] Ep 5/6 | loss=-0.016162 actor=-0.005647 critic=0.000206 ent=1.072116\n",
      "  [VAL] deterministic mean net reward = 0.000098\n",
      "[Cluster 4] Ep 6/6 | loss=-0.016095 actor=-0.006147 critic=0.000224 ent=1.017295\n",
      "  [VAL] deterministic mean net reward = 0.000095\n",
      "--- Finished training cluster 4. Best val score = 0.000166 ---\n",
      "\n",
      "--- TRAIN cluster 5 (split=train) ---\n",
      "  windows=196, LOOKBACK=20, N=362, F=16\n",
      "  Found val windows=130\n",
      "[Cluster 5] Ep 1/6 | loss=-0.013367 actor=-0.002959 critic=0.000473 ent=1.088034\n",
      "  [VAL] deterministic mean net reward = 0.000490\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000490)\n",
      "[Cluster 5] Ep 2/6 | loss=-0.013674 actor=-0.010076 critic=0.000273 ent=0.387061\n",
      "  [VAL] deterministic mean net reward = 0.000493\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000493)\n",
      "[Cluster 5] Ep 3/6 | loss=-0.002845 actor=0.000192 critic=0.000273 ent=0.330968\n",
      "  [VAL] deterministic mean net reward = 0.000498\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000498)\n",
      "[Cluster 5] Ep 4/6 | loss=-0.005307 actor=-0.000379 critic=0.000254 ent=0.518211\n",
      "  [VAL] deterministic mean net reward = 0.000501\n",
      "  ✅ New best checkpoint saved for cluster 5 (val=0.000501)\n",
      "[Cluster 5] Ep 5/6 | loss=-0.009541 actor=-0.005085 critic=0.000258 ent=0.471422\n",
      "  [VAL] deterministic mean net reward = 0.000497\n",
      "[Cluster 5] Ep 6/6 | loss=-0.008438 actor=-0.004905 critic=0.000262 ent=0.379530\n",
      "  [VAL] deterministic mean net reward = 0.000490\n",
      "--- Finished training cluster 5. Best val score = 0.000501 ---\n",
      "\n",
      "--- TRAIN cluster 6 (split=train) ---\n",
      "  windows=212, LOOKBACK=20, N=317, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 6] Ep 1/6 | loss=-0.010721 actor=-0.000248 critic=0.000479 ent=1.095215\n",
      "  [VAL] deterministic mean net reward = 0.000269\n",
      "  ✅ New best checkpoint saved for cluster 6 (val=0.000269)\n",
      "[Cluster 6] Ep 2/6 | loss=-0.019039 actor=-0.010944 critic=0.000230 ent=0.832478\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "  ✅ New best checkpoint saved for cluster 6 (val=0.000304)\n",
      "[Cluster 6] Ep 3/6 | loss=-0.013402 actor=-0.009763 critic=0.000266 ent=0.390501\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "[Cluster 6] Ep 4/6 | loss=-0.011187 actor=-0.008578 critic=0.000272 ent=0.288068\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "[Cluster 6] Ep 5/6 | loss=-0.012215 actor=-0.009773 critic=0.000271 ent=0.271303\n",
      "  [VAL] deterministic mean net reward = 0.000301\n",
      "[Cluster 6] Ep 6/6 | loss=-0.008733 actor=-0.006716 critic=0.000274 ent=0.229057\n",
      "  [VAL] deterministic mean net reward = 0.000304\n",
      "--- Finished training cluster 6. Best val score = 0.000304 ---\n",
      "\n",
      "--- TRAIN cluster 7 (split=train) ---\n",
      "  windows=205, LOOKBACK=20, N=373, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 7] Ep 1/6 | loss=-0.018267 actor=-0.009581 critic=0.000222 ent=0.890817\n",
      "  [VAL] deterministic mean net reward = 0.000298\n",
      "  ✅ New best checkpoint saved for cluster 7 (val=0.000298)\n",
      "[Cluster 7] Ep 2/6 | loss=-0.014787 actor=-0.009948 critic=0.000252 ent=0.509092\n",
      "  [VAL] deterministic mean net reward = 0.000298\n",
      "[Cluster 7] Ep 3/6 | loss=-0.012967 actor=-0.008810 critic=0.000254 ent=0.441055\n",
      "  [VAL] deterministic mean net reward = 0.000298\n",
      "[Cluster 7] Ep 4/6 | loss=-0.008970 actor=-0.004730 critic=0.000248 ent=0.448826\n",
      "  [VAL] deterministic mean net reward = 0.000277\n",
      "[Cluster 7] Ep 5/6 | loss=-0.006311 actor=-0.003435 critic=0.000257 ent=0.313407\n",
      "  [VAL] deterministic mean net reward = 0.000303\n",
      "  ✅ New best checkpoint saved for cluster 7 (val=0.000303)\n",
      "[Cluster 7] Ep 6/6 | loss=-0.005459 actor=-0.002226 critic=0.000258 ent=0.349089\n",
      "  [VAL] deterministic mean net reward = 0.000178\n",
      "--- Finished training cluster 7. Best val score = 0.000303 ---\n",
      "\n",
      "--- TRAIN cluster 8 (split=train) ---\n",
      "  windows=228, LOOKBACK=20, N=23, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 8] Ep 1/6 | loss=-0.010176 actor=0.000608 critic=0.000176 ent=1.095989\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "  ✅ New best checkpoint saved for cluster 8 (val=0.000000)\n",
      "[Cluster 8] Ep 2/6 | loss=-0.012097 actor=-0.001272 critic=0.000135 ent=1.096055\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "[Cluster 8] Ep 3/6 | loss=-0.012550 actor=-0.001719 critic=0.000127 ent=1.095829\n",
      "  [VAL] deterministic mean net reward = 0.000000\n",
      "[Cluster 8] Ep 4/6 | loss=-0.012501 actor=-0.001665 critic=0.000118 ent=1.095394\n",
      "  [VAL] deterministic mean net reward = 0.000017\n",
      "  ✅ New best checkpoint saved for cluster 8 (val=0.000017)\n",
      "[Cluster 8] Ep 5/6 | loss=-0.013428 actor=-0.002595 critic=0.000117 ent=1.094985\n",
      "  [VAL] deterministic mean net reward = 0.000004\n",
      "[Cluster 8] Ep 6/6 | loss=-0.013645 actor=-0.002815 critic=0.000113 ent=1.094250\n",
      "  [VAL] deterministic mean net reward = 0.000074\n",
      "  ✅ New best checkpoint saved for cluster 8 (val=0.000074)\n",
      "--- Finished training cluster 8. Best val score = 0.000074 ---\n",
      "\n",
      "--- TRAIN cluster 9 (split=train) ---\n",
      "  windows=206, LOOKBACK=20, N=366, F=16\n",
      "  Found val windows=230\n",
      "[Cluster 9] Ep 1/6 | loss=-0.018577 actor=-0.009849 critic=0.000264 ent=0.899189\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "  ✅ New best checkpoint saved for cluster 9 (val=0.000334)\n",
      "[Cluster 9] Ep 2/6 | loss=-0.010796 actor=-0.008579 critic=0.000266 ent=0.248318\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 3/6 | loss=-0.009362 actor=-0.007094 critic=0.000267 ent=0.253503\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 4/6 | loss=-0.013233 actor=-0.010353 critic=0.000261 ent=0.314050\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 5/6 | loss=-0.008836 actor=-0.006354 critic=0.000263 ent=0.274492\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "[Cluster 9] Ep 6/6 | loss=-0.008066 actor=-0.006306 critic=0.000264 ent=0.202367\n",
      "  [VAL] deterministic mean net reward = 0.000334\n",
      "--- Finished training cluster 9. Best val score = 0.000334 ---\n",
      "\n",
      "=== Running inference on split=test and writing to signals\\a3c_signals_infer.csv ===\n",
      "[INF] cluster=0 split=test\n",
      "  [INF] cluster 0 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=1 split=test\n",
      "  [INF] cluster 1 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=2 split=test\n",
      "  [INF] cluster 2 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=3 split=test\n",
      "  [INF] cluster 3 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=4 split=test\n",
      "  [INF] cluster 4 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=5 split=test\n",
      "  [INF] cluster 5 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=6 split=test\n",
      "  [INF] cluster 6 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=7 split=test\n",
      "  [INF] cluster 7 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=8 split=test\n",
      "  [INF] cluster 8 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "[INF] cluster=9 split=test\n",
      "  [INF] cluster 9 -> appended signals to signals\\a3c_signals_infer.csv\n",
      "\n",
      "✅ Block 8 complete: training done and inference saved to signals\\a3c_signals_infer.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 8 — A3C per-cluster (full, stable, val-checkpoint, inference)\n",
    "import os, gc, json, csv, math, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========== CONFIG / PATH ===========\n",
    "BASE_TENSOR_DIR = Path(\"C:/tensors_out\")         # nơi có train/val/test subfolders + tensor_index.json\n",
    "TENSOR_INDEX = BASE_TENSOR_DIR / \"tensor_index.json\"\n",
    "DF_BACKTEST = Path(\"./backtest_ddpg/df_backtest.csv\")   # nếu có -> dùng để tính reward next-day\n",
    "SIG_DIR = Path(\"./signals/\")\n",
    "MODEL_DIR = Path(\"./models_a3c/\")\n",
    "SIG_TRAIN_FILE = SIG_DIR / \"a3c_signals_train.csv\"\n",
    "SIG_INFER_FILE = SIG_DIR / \"a3c_signals_infer.csv\"\n",
    "\n",
    "SIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========== HYPER (tune) ===========\n",
    "EPOCHS = 3               # epochs per cluster (increase later)\n",
    "BATCH_SIZE = 256\n",
    "LR = 1e-4\n",
    "HIDDEN = 128\n",
    "ENTROPY_BETA = 0.01\n",
    "CRITIC_COEF = 0.5\n",
    "GRAD_CLIP = 1.0\n",
    "REWARD_SCALE = 1.0\n",
    "SEED = 42\n",
    "LOOKBACK_EXPECT = None   # None = inferred per-cluster\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# =========== DEVICE ===========\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# =========== SAFETY helpers ===========\n",
    "EPS = 1e-8\n",
    "\n",
    "def safe_tensor_np(x, dtype=np.float32):\n",
    "    a = np.array(x, dtype=dtype)\n",
    "    # replace inf/nan\n",
    "    a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return a\n",
    "\n",
    "def safe_tensor(x, dtype=torch.float32):\n",
    "    t = torch.tensor(np.array(x), dtype=dtype, device=DEVICE)\n",
    "    t = torch.where(torch.isfinite(t), t, torch.zeros_like(t))\n",
    "    return t\n",
    "\n",
    "def is_bad(t: torch.Tensor):\n",
    "    return torch.isnan(t).any().item() or torch.isinf(t).any().item()\n",
    "\n",
    "# =========== MODEL ===========\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden, max(32, hidden//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(32, hidden//2), 3)   # logits: short(0), flat(1), long(2)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden, max(32, hidden//2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(32, hidden//2), 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]                          # (B, hidden)\n",
    "        logits = self.actor(h)                     # (B, 3)\n",
    "        value = self.critic(h).squeeze(-1)         # (B,)\n",
    "        return logits, value\n",
    "\n",
    "# =========== IO helpers (split-aware) ===========\n",
    "def resolve_path_rel_to_split(path_str: str, split: str):\n",
    "    \"\"\"\n",
    "    If given path_str exists -> return Path(path_str).\n",
    "    Else, attempt BASE_TENSOR_DIR / split / basename(path_str).\n",
    "    \"\"\"\n",
    "    p = Path(path_str)\n",
    "    if p.exists():\n",
    "        return p\n",
    "    candidate = BASE_TENSOR_DIR / split / p.name\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    # fallback: try under base dir directly\n",
    "    candidate2 = BASE_TENSOR_DIR / p.name\n",
    "    if candidate2.exists():\n",
    "        return candidate2\n",
    "    # final fallback: return expected candidate (may raise later)\n",
    "    return candidate\n",
    "\n",
    "# =========== LOAD tensor_index ===========\n",
    "if not TENSOR_INDEX.exists():\n",
    "    raise FileNotFoundError(f\"tensor_index.json not found at {TENSOR_INDEX}. Run Block 7 first.\")\n",
    "with open(TENSOR_INDEX, \"r\", encoding=\"utf-8\") as fh:\n",
    "    tensor_index_all = json.load(fh)\n",
    "\n",
    "# =========== LOAD price (backtest) for reward if available ===========\n",
    "USE_PRICE_REWARD = DF_BACKTEST.exists()\n",
    "if USE_PRICE_REWARD:\n",
    "    df_back = pd.read_csv(DF_BACKTEST, parse_dates=[\"date\"])\n",
    "    df_back = df_back.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n",
    "    px_close = df_back.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "    print(\"df_backtest loaded: dates\", px_close.index.min(), \"->\", px_close.index.max())\n",
    "else:\n",
    "    px_close = None\n",
    "    print(\"No df_backtest.csv found -> training with synthetic/zero reward unless user supplies df_backtest.csv\")\n",
    "\n",
    "# ---------- helper: compute next-day log return for arrays of dates & tickers ----------\n",
    "def compute_nextday_logret(dates_iso: List[str], tickers: List[str]):\n",
    "    \"\"\"\n",
    "    dates_iso: list of window-end dates (ISO strings). returns array shape (B, N)\n",
    "    For each date d, compute log(close[next_trading_day, ticker] / close[d, ticker])\n",
    "    If px_close missing -> return zeros.\n",
    "    \"\"\"\n",
    "    if px_close is None:\n",
    "        return np.zeros((len(dates_iso), len(tickers)), dtype=np.float32)\n",
    "\n",
    "    d_idx = pd.to_datetime(dates_iso)\n",
    "    # find nearest index positions (<= d), then pos+1 -> next trading day if exists\n",
    "    all_index = px_close.index\n",
    "    res = np.zeros((len(d_idx), len(tickers)), dtype=np.float32)\n",
    "    for i, d in enumerate(d_idx):\n",
    "        # get location of nearest index <= d\n",
    "        pos = all_index.get_indexer([d], method='pad')[0]\n",
    "        pos_next = pos + 1 if (pos + 1) < len(all_index) else pos\n",
    "        date_curr = all_index[pos]\n",
    "        date_next = all_index[pos_next]\n",
    "        # slice\n",
    "        v_curr = px_close.loc[date_curr, tickers].values.astype(np.float32)\n",
    "        v_next = px_close.loc[date_next, tickers].values.astype(np.float32)\n",
    "        # safe division\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            r = np.log(np.divide(v_next, np.maximum(v_curr, 1e-9)))\n",
    "        r = np.nan_to_num(r, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        res[i, :] = r\n",
    "    return res\n",
    "\n",
    "# =========== safe loader for tensor meta ===========\n",
    "def load_tensor_meta(meta: Dict):\n",
    "    \"\"\"\n",
    "    meta expected keys: split, cluster, tensor_file, mask_file, tickers, dates (list), dates_shifted (list optional)\n",
    "    returns: X (B, LOOKBACK, N, F), M (B, LOOKBACK, N, F), tickers(list), dates(list), split(str)\n",
    "    \"\"\"\n",
    "    split = meta.get(\"split\", \"train\")\n",
    "    tpath = resolve_path_rel_to_split(meta[\"tensor_file\"], split)\n",
    "    mpath = resolve_path_rel_to_split(meta[\"mask_file\"], split)\n",
    "    if not tpath.exists() or not mpath.exists():\n",
    "        raise FileNotFoundError(f\"Tensor/Mask not found for cluster {meta.get('cluster')} split={split}: {tpath}, {mpath}\")\n",
    "    X = np.load(tpath, mmap_mode=\"r\")\n",
    "    M = np.load(mpath, mmap_mode=\"r\")\n",
    "    tickers = list(meta.get(\"tickers\", []))\n",
    "    dates = [pd.to_datetime(d) for d in meta.get(\"dates\", [])]\n",
    "    dates_shifted = [pd.to_datetime(d) for d in meta.get(\"dates_shifted\", [])] if meta.get(\"dates_shifted\") else None\n",
    "    return X, M, tickers, dates, dates_shifted, split\n",
    "\n",
    "# =========== Training per-cluster (with val checkpointing) ===========\n",
    "def train_cluster(meta, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, save_dir=MODEL_DIR):\n",
    "    cluster_id = int(meta.get(\"cluster\"))\n",
    "    split = meta.get(\"split\", \"train\")\n",
    "    if split != \"train\":\n",
    "        print(f\"Skipping non-train meta cluster {cluster_id} (split={split})\")\n",
    "        return None\n",
    "    print(f\"\\n--- TRAIN cluster {cluster_id} (split=train) ---\")\n",
    "    # load train meta\n",
    "    try:\n",
    "        X_tr, M_tr, tickers_tr, dates_tr, dates_shifted_tr, _ = load_tensor_meta(meta)\n",
    "    except Exception as e:\n",
    "        print(\"  [ERROR] load tensor:\", e)\n",
    "        return None\n",
    "    B, LOOKBACK, N, F = X_tr.shape\n",
    "    print(f\"  windows={B}, LOOKBACK={LOOKBACK}, N={N}, F={F}\")\n",
    "\n",
    "    # find val meta for same cluster if exists\n",
    "    val_meta = None\n",
    "    for m in tensor_index_all:\n",
    "        if int(m.get(\"cluster\"))==cluster_id and m.get(\"split\",\"\")==\"val\":\n",
    "            val_meta = m; break\n",
    "    if val_meta:\n",
    "        X_val, M_val, tickers_val, dates_val, dates_shifted_val, _ = load_tensor_meta(val_meta)\n",
    "        print(f\"  Found val windows={X_val.shape[0]}\")\n",
    "        # precompute val rewards\n",
    "        r_val = compute_nextday_logret([d.isoformat() for d in dates_val], tickers_val) * REWARD_SCALE\n",
    "    else:\n",
    "        X_val = M_val = tickers_val = dates_val = dates_shifted_val = None\n",
    "        r_val = None\n",
    "\n",
    "    # precompute train rewards (B, N)\n",
    "    r_tr = compute_nextday_logret([d.isoformat() for d in dates_tr], tickers_tr) * REWARD_SCALE\n",
    "\n",
    "    # safe conversions\n",
    "    X_tr = np.nan_to_num(X_tr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    M_tr = np.nan_to_num(M_tr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "    # model & optimizer\n",
    "    model = A3CNet(n_features=F, hidden=HIDDEN).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_score = -1e9\n",
    "    best_path = save_dir / f\"a3c_cluster_{cluster_id}_best.pt\"\n",
    "    last_path = save_dir / f\"a3c_cluster_{cluster_id}_last.pt\"\n",
    "\n",
    "    total_samples = B * N\n",
    "    indices = np.arange(total_samples)\n",
    "\n",
    "    def batch_iter(shuffle=True):\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for start in range(0, total_samples, batch_size):\n",
    "            end = min(total_samples, start + batch_size)\n",
    "            batch_idx = indices[start:end]\n",
    "            xb_list, mb_list, rb_list = [], [], []\n",
    "            for s in batch_idx:\n",
    "                b, n = divmod(int(s), N)\n",
    "                seq = X_tr[b, :, n, :]          # (LOOKBACK, F)\n",
    "                mask = M_tr[b, :, n, :]         # (LOOKBACK, F)\n",
    "                xb_list.append(seq)\n",
    "                mb_list.append(mask)\n",
    "                rb_list.append(r_tr[b, n])\n",
    "            yield np.stack(xb_list, axis=0), np.stack(mb_list, axis=0), np.array(rb_list, dtype=np.float32), batch_idx\n",
    "\n",
    "    # training loop\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        ep_loss = 0.0; ep_actor = 0.0; ep_critic = 0.0; ep_ent = 0.0; ep_batches = 0\n",
    "        for xb_np, mb_np, rb_np, batch_idx in batch_iter(shuffle=True):\n",
    "            # to tensors\n",
    "            xb = safe_tensor(xb_np); mb = safe_tensor(mb_np); rb = safe_tensor(rb_np)\n",
    "            # apply mask to inputs\n",
    "            xb = xb * mb\n",
    "            # forward\n",
    "            logits, vals = model(xb)                 # logits (B,3), vals (B,)\n",
    "            if is_bad(logits) or is_bad(vals) or is_bad(rb):\n",
    "                print(\"  [WARN] bad numerics in forward -> skip batch\")\n",
    "                continue\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            acts = dist.sample()                     # (B,)\n",
    "            logp = torch.log_softmax(logits, dim=-1)\n",
    "            logp_act = logp.gather(1, acts.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # map action -> reward: long(2) => +r, short(0) => -r, flat => 0\n",
    "            reward_t = torch.where(acts==2, rb, torch.where(acts==0, -rb, torch.zeros_like(rb)))\n",
    "\n",
    "            # advantage\n",
    "            adv = reward_t - vals\n",
    "            adv_mean = adv.mean()\n",
    "            adv_std = adv.std(unbiased=False) + EPS\n",
    "            adv_norm = (adv - adv_mean) / adv_std\n",
    "\n",
    "            actor_loss = -(logp_act * adv_norm.detach()).mean()\n",
    "            critic_loss = CRITIC_COEF * (adv.pow(2).mean())\n",
    "            entropy = - (torch.softmax(logits, dim=-1) * logp).sum(-1).mean()\n",
    "            loss = actor_loss + critic_loss - ENTROPY_BETA * entropy\n",
    "\n",
    "            if not math.isfinite(float(loss.item())):\n",
    "                print(\"  [WARN] loss not finite -> skip batch\")\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            opt.step()\n",
    "\n",
    "            ep_loss += float(loss.item()); ep_actor += float(actor_loss.item())\n",
    "            ep_critic += float(critic_loss.item()); ep_ent += float(entropy.item()); ep_batches += 1\n",
    "\n",
    "            # free small\n",
    "            del xb, mb, rb, logits, vals, dist, acts, logp, logp_act, reward_t, adv, adv_norm\n",
    "            gc.collect()\n",
    "\n",
    "        if ep_batches == 0:\n",
    "            print(f\"  [WARN] no batches processed in epoch {ep}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[Cluster {cluster_id}] Ep {ep}/{epochs} | loss={ep_loss/ep_batches:.6f} actor={ep_actor/ep_batches:.6f} critic={ep_critic/ep_batches:.6f} ent={ep_ent/ep_batches:.6f}\")\n",
    "\n",
    "        # validation evaluation (deterministic argmax if val exists)\n",
    "        if X_val is not None:\n",
    "            model.eval()\n",
    "            # build val inference in batches\n",
    "            val_total = X_val.shape[0] * X_val.shape[2]\n",
    "            val_batchsize = batch_size\n",
    "            val_rewards = []\n",
    "            for start in range(0, val_total, val_batchsize):\n",
    "                xb_list = []; rb_list = []\n",
    "                for s in range(start, min(val_total, start+val_batchsize)):\n",
    "                    b, n = divmod(int(s), X_val.shape[2])\n",
    "                    seq = X_val[b, :, n, :]\n",
    "                    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                    xb_list.append(seq)\n",
    "                    rb_list.append(r_val[b, n])\n",
    "                if not xb_list:\n",
    "                    continue\n",
    "                xb_t = safe_tensor(np.stack(xb_list, axis=0))\n",
    "                logits_v, vals_v = model(xb_t)\n",
    "                acts_v = torch.argmax(logits_v, dim=-1)\n",
    "                rb_t = safe_tensor(np.array(rb_list, dtype=np.float32))\n",
    "                rdet = torch.where(acts_v==2, rb_t, torch.where(acts_v==0, -rb_t, torch.zeros_like(rb_t)))\n",
    "                val_rewards.append(rdet.cpu().numpy())\n",
    "                del xb_t, logits_v, vals_v, acts_v, rb_t, rdet\n",
    "            if val_rewards:\n",
    "                val_arr = np.concatenate(val_rewards, axis=0)\n",
    "                val_score = float(val_arr.mean())\n",
    "            else:\n",
    "                val_score = -1e9\n",
    "            print(f\"  [VAL] deterministic mean net reward = {val_score:.6f}\")\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                torch.save(model.state_dict(), best_path)\n",
    "                print(f\"  ✅ New best checkpoint saved for cluster {cluster_id} (val={val_score:.6f})\")\n",
    "\n",
    "        # always save last\n",
    "        torch.save(model.state_dict(), last_path)\n",
    "\n",
    "    # end epochs\n",
    "    print(f\"--- Finished training cluster {cluster_id}. Best val score = {best_val_score:.6f} ---\")\n",
    "    ckpt = best_path if best_path.exists() else last_path\n",
    "    return str(ckpt)\n",
    "\n",
    "# =========== Inference (deterministic argmax) ===========\n",
    "def infer_on_split(output_csv: str, split_name: str = \"test\"):\n",
    "    # reset file\n",
    "    if os.path.exists(output_csv):\n",
    "        os.remove(output_csv)\n",
    "    with open(output_csv, \"w\", newline=\"\") as fh:\n",
    "        csv.writer(fh).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "    for meta in tensor_index_all:\n",
    "        if meta.get(\"split\") != split_name:\n",
    "            continue\n",
    "        cluster_id = int(meta.get(\"cluster\"))\n",
    "        print(f\"[INF] cluster={cluster_id} split={split_name}\")\n",
    "        try:\n",
    "            X, M, tickers, dates, dates_shifted, split = load_tensor_meta(meta)\n",
    "        except Exception as e:\n",
    "            print(\"  [WARN] load tensor failed:\", e); continue\n",
    "        B, LOOKBACK, N, F = X.shape\n",
    "        # choose checkpoint: best else last\n",
    "        best_path = MODEL_DIR / f\"a3c_cluster_{cluster_id}_best.pt\"\n",
    "        last_path = MODEL_DIR / f\"a3c_cluster_{cluster_id}_last.pt\"\n",
    "        model_path = best_path if best_path.exists() else last_path if last_path.exists() else None\n",
    "        if model_path is None:\n",
    "            print(f\"  [WARN] no checkpoint for cluster {cluster_id}, skipping\")\n",
    "            continue\n",
    "        model = A3CNet(n_features=F, hidden=HIDDEN).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.eval()\n",
    "        total = B * N\n",
    "        with open(output_csv, \"a\", newline=\"\") as fh:\n",
    "            w = csv.writer(fh)\n",
    "            for start in range(0, total, BATCH_SIZE):\n",
    "                xb_list = []; idx_list = []\n",
    "                for s in range(start, min(total, start+BATCH_SIZE)):\n",
    "                    b, n = divmod(int(s), N)\n",
    "                    seq = X[b, :, n, :]\n",
    "                    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                    xb_list.append(seq); idx_list.append((b,n))\n",
    "                if not xb_list:\n",
    "                    continue\n",
    "                xb_t = safe_tensor(np.stack(xb_list, axis=0))\n",
    "                logits, vals = model(xb_t)\n",
    "                acts = torch.argmax(logits, dim=-1).cpu().numpy()    # 0,1,2\n",
    "                acts_mapped = np.where(acts==2, 1, np.where(acts==0, -1, 0))\n",
    "                for k, (b,n) in enumerate(idx_list):\n",
    "                    out_date = dates_shifted[b].isoformat() if dates_shifted is not None and len(dates_shifted)>b else pd.to_datetime(dates[b]).date().isoformat()\n",
    "                    w.writerow([out_date, tickers[n], int(acts_mapped[k])])\n",
    "                del xb_t, logits, vals, acts\n",
    "                gc.collect()\n",
    "        print(f\"  [INF] cluster {cluster_id} -> appended signals to {output_csv}\")\n",
    "\n",
    "# =========== MAIN flow ===========\n",
    "# 1) Train clusters that have split=train entries (iterate over tensor_index_all)\n",
    "trained = {}\n",
    "for meta in tensor_index_all:\n",
    "    if meta.get(\"split\") != \"train\":\n",
    "        continue\n",
    "    try:\n",
    "        ckpt = train_cluster(meta, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)\n",
    "        if ckpt:\n",
    "            trained[int(meta.get(\"cluster\"))] = ckpt\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] training cluster {meta.get('cluster')}: {e}\")\n",
    "\n",
    "# 2) After training, run inference on split=test and save CSV\n",
    "print(\"\\n=== Running inference on split=test and writing to\", SIG_INFER_FILE, \"===\")\n",
    "infer_on_split(str(SIG_INFER_FILE), split_name=\"test\")\n",
    "print(\"\\n✅ Block 8 complete: training done and inference saved to\", SIG_INFER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "924c9e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running inference on split=val and writing to signals\\a3c_signals_infer_val.csv ===\n",
      "[INF] cluster=0 split=val\n",
      "  [INF] cluster 0 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=1 split=val\n",
      "  [INF] cluster 1 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=2 split=val\n",
      "  [INF] cluster 2 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=3 split=val\n",
      "  [INF] cluster 3 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=4 split=val\n",
      "  [INF] cluster 4 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=5 split=val\n",
      "  [INF] cluster 5 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=6 split=val\n",
      "  [INF] cluster 6 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=7 split=val\n",
      "  [INF] cluster 7 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=8 split=val\n",
      "  [INF] cluster 8 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "[INF] cluster=9 split=val\n",
      "  [INF] cluster 9 -> appended signals to signals\\a3c_signals_infer_val.csv\n",
      "\n",
      "✅ Block 9b complete: inference saved to signals\\a3c_signals_infer_val.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, csv\n",
    "\n",
    "SIG_DIR = Path(\"./signals/\")\n",
    "MODEL_DIR = Path(\"./models_a3c/\")   # thêm dòng này cho chắc chắn\n",
    "\n",
    "SIG_VAL_FILE = SIG_DIR / \"a3c_signals_infer_val.csv\"\n",
    "\n",
    "print(\"\\n=== Running inference on split=val and writing to\", SIG_VAL_FILE, \"===\")\n",
    "infer_on_split(str(SIG_VAL_FILE), split_name=\"val\")\n",
    "print(\"\\n✅ Block 9b complete: inference saved to\", SIG_VAL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f8af",
   "metadata": {},
   "source": [
    "**Block 10 : Huấn luyện Cluster DDPG (chỉ với trường hợp vị thế long)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeaf848",
   "metadata": {},
   "source": [
    "**Block 10 prep bản có chia train val test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6bf781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price timeline: 2020-01-02 00:00:00 → 2025-10-02 00:00:00\n",
      "Clusters in tensor_index (before remap): [5, 8, 9]\n",
      "Clusters in tensor_index (after remap): 3 -> [0, 1, 2]\n",
      "Tickers used: 391\n",
      "Building state matrices...\n",
      "Shapes:\n",
      " S_train: (991, 60)  R_train: (991, 3)\n",
      " S_val:   (241, 60)  R_val:   (241, 3)\n",
      " S_test:  (176, 60)  R_test:  (176, 3)\n",
      "✅ Block 10-prep done. Files in ddpg_prep\n"
     ]
    }
   ],
   "source": [
    "# Block 10-prep — Chuẩn bị dữ liệu DDPG (auto-remap clusters, split-aware)\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG / PATH ----------\n",
    "BASE_TENSOR_DIR = Path(\"C:/tensors_out\")          \n",
    "TENSOR_INDEX = BASE_TENSOR_DIR / \"tensor_index.json\"\n",
    "SIG_FILE = Path(\"./signals/a3c_signals_infer.csv\") \n",
    "DF_BACKTEST = Path(\"./backtest_ddpg/df_backtest.csv\")\n",
    "OUTPUT_DIR = Path(\"./ddpg_prep/\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXECUTION_LAG = 2        \n",
    "STATE_LKBK    = 10       \n",
    "MIN_NAMES_PER_CLUSTER = 5\n",
    "\n",
    "# ---------- Splits ----------\n",
    "TRAIN_START = pd.Timestamp(\"2020-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2023-12-31\")\n",
    "VAL_START   = pd.Timestamp(\"2024-01-01\")\n",
    "VAL_END     = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")\n",
    "\n",
    "# ---------- Sanity ----------\n",
    "if not TENSOR_INDEX.exists():\n",
    "    raise FileNotFoundError(f\"tensor_index.json không tìm thấy tại {TENSOR_INDEX}.\")\n",
    "if not SIG_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Signals file không tìm thấy: {SIG_FILE}.\")\n",
    "if not DF_BACKTEST.exists():\n",
    "    raise FileNotFoundError(f\"Price backtest file không tìm thấy: {DF_BACKTEST}.\")\n",
    "\n",
    "# ---------- Load price ----------\n",
    "df_px = pd.read_csv(DF_BACKTEST, parse_dates=[\"date\"])\n",
    "px_wide = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "px_wide = px_wide.ffill().bfill()\n",
    "TEST_END = px_wide.index.max()\n",
    "print(\"Price timeline:\", px_wide.index.min(), \"→\", px_wide.index.max())\n",
    "\n",
    "# ---------- Load signals ----------\n",
    "signals = pd.read_csv(SIG_FILE, parse_dates=[\"date\"])\n",
    "sig_wide_raw = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\").sort_index()\n",
    "idx_all = px_wide.index.union(sig_wide_raw.index).sort_values()\n",
    "sig_wide = sig_wide_raw.reindex(idx_all).fillna(0.0)\n",
    "sig_wide_lag = sig_wide.shift(EXECUTION_LAG).reindex(idx_all).fillna(0.0)\n",
    "\n",
    "# ---------- Load cluster mapping ----------\n",
    "with open(TENSOR_INDEX, \"r\", encoding=\"utf-8\") as fh:\n",
    "    tensor_index = json.load(fh)\n",
    "\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    for tk in meta.get(\"tickers\", []):\n",
    "        ticker2cluster[tk] = int(meta.get(\"cluster\", -1))\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "print(\"Clusters in tensor_index (before remap):\", sorted(ticker2cluster.unique().tolist()))\n",
    "\n",
    "# ---------- Restrict tickers ----------\n",
    "tickers = sorted(set(px_wide.columns).intersection(ticker2cluster.index))\n",
    "cluster_of = ticker2cluster.loc[tickers]\n",
    "\n",
    "# ---------- Remap clusters ----------\n",
    "active_clusters = sorted(cluster_of.unique().tolist())\n",
    "remap = {old:i for i, old in enumerate(active_clusters)}\n",
    "cluster_of = cluster_of.map(remap)\n",
    "\n",
    "clusters = sorted(remap.values())\n",
    "cluster_members = {c: cluster_of[cluster_of==c].index.tolist() for c in clusters}\n",
    "\n",
    "print(\"Clusters in tensor_index (after remap):\", len(clusters), \"->\", clusters)\n",
    "print(\"Tickers used:\", len(tickers))\n",
    "\n",
    "px_wide = px_wide.reindex(columns=tickers)\n",
    "sig_wide_lag = sig_wide_lag.reindex(columns=tickers).fillna(0.0)\n",
    "ret_wide = px_wide.pct_change().fillna(0.0)\n",
    "\n",
    "# ---------- Build state arrays ----------\n",
    "def build_state_arrays(ret_w, sig_lag_w, start, end, K=STATE_LKBK):\n",
    "    R = ret_w.loc[start:end]\n",
    "    S = sig_lag_w.loc[start:end]\n",
    "    dates = R.index\n",
    "\n",
    "    act_cols, ret_cols, ACTIVE_masks = [], [], {}\n",
    "    for c in clusters:\n",
    "        tks = cluster_members[c]\n",
    "        if not tks:\n",
    "            act_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ACTIVE_masks[c] = pd.DataFrame(0.0, index=dates, columns=[])\n",
    "        else:\n",
    "            S_c, R_c = S[tks], R[tks]\n",
    "            active_mask = (S_c != 0).astype(\"float32\")\n",
    "            ACTIVE_masks[c] = active_mask.copy()\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0).fillna(0.0)\n",
    "            act_c = active_mask.mean(axis=1).astype(\"float32\")\n",
    "            ret_c = (R_c * w).sum(axis=1).astype(\"float32\")\n",
    "        act_cols.append(act_c.rename(c))\n",
    "        ret_cols.append(ret_c.rename(c))\n",
    "\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "\n",
    "    def stack_lookback(df, K):\n",
    "        mats = []\n",
    "        for k in range(K):\n",
    "            mats.append(df.shift(k).fillna(0.0))\n",
    "        arr = np.stack([m.values for m in mats], axis=2)  \n",
    "        return arr\n",
    "\n",
    "    A3, R3 = stack_lookback(act_df, K), stack_lookback(cret_df, K)\n",
    "    valid_mask = np.arange(A3.shape[0]) >= (K - 1)\n",
    "    dates2 = dates[valid_mask]\n",
    "\n",
    "    A3_valid, R3_valid = A3[valid_mask], R3[valid_mask]\n",
    "    Tv = A3_valid.shape[0]\n",
    "    S_mat = np.concatenate([A3_valid.reshape(Tv,-1), R3_valid.reshape(Tv,-1)], axis=1).astype(\"float32\")\n",
    "    R_mat = cret_df.loc[dates2].values.astype(\"float32\")\n",
    "    ACTIVE_trim = {c: ACTIVE_masks[c].loc[dates2] for c in clusters}\n",
    "    return S_mat, R_mat, dates2, ACTIVE_trim\n",
    "\n",
    "print(\"Building state matrices...\")\n",
    "S_train, R_train, d_train, _ = build_state_arrays(ret_wide, sig_wide_lag, TRAIN_START, TRAIN_END)\n",
    "S_val,   R_val,   d_val,   _ = build_state_arrays(ret_wide, sig_wide_lag, VAL_START, VAL_END)\n",
    "S_test,  R_test,  d_test,  _ = build_state_arrays(ret_wide, sig_wide_lag, TEST_START, TEST_END)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\" S_train:\", S_train.shape, \" R_train:\", R_train.shape)\n",
    "print(\" S_val:  \", S_val.shape,   \" R_val:  \", R_val.shape)\n",
    "print(\" S_test: \", S_test.shape,  \" R_test: \", R_test.shape)\n",
    "\n",
    "# ---------- Save ----------\n",
    "np.save(OUTPUT_DIR/\"S_train.npy\", S_train)\n",
    "np.save(OUTPUT_DIR/\"R_train.npy\", R_train)\n",
    "np.save(OUTPUT_DIR/\"S_val.npy\", S_val)\n",
    "np.save(OUTPUT_DIR/\"R_val.npy\", R_val)\n",
    "np.save(OUTPUT_DIR/\"S_test.npy\", S_test)\n",
    "np.save(OUTPUT_DIR/\"R_test.npy\", R_test)\n",
    "\n",
    "pd.Series(d_train.astype(str)).to_csv(OUTPUT_DIR/\"dates_train.csv\", index=False)\n",
    "pd.Series(d_val.astype(str)).to_csv(OUTPUT_DIR/\"dates_val.csv\", index=False)\n",
    "pd.Series(d_test.astype(str)).to_csv(OUTPUT_DIR/\"dates_test.csv\", index=False)\n",
    "\n",
    "pd.Series(clusters).to_csv(OUTPUT_DIR/\"clusters_list.csv\", index=False)\n",
    "with open(OUTPUT_DIR/\"cluster_members.json\",\"w\",encoding=\"utf-8\") as fh:\n",
    "    json.dump({int(c): cluster_members[c] for c in clusters}, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Block 10-prep done. Files in\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b483a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5      1\n",
      "8     34\n",
      "9    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Đếm số mã mỗi cụm (sau khi giao cắt tickers)\n",
    "print(cluster_of.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e735e",
   "metadata": {},
   "source": [
    "**Block 10a mới bản có chia ra train val test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc9d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train shapes: (991, 60) (991, 3)\n",
      "Starting training...\n",
      "[Epoch 1/60] Critic=0.000000 | Actor=0.038839 | updates=927\n",
      "[Epoch 2/60] Critic=0.000000 | Actor=0.023845 | updates=990\n",
      "[Epoch 3/60] Critic=0.000000 | Actor=0.014240 | updates=990\n",
      "[Epoch 4/60] Critic=0.000000 | Actor=0.008509 | updates=990\n",
      "[Epoch 5/60] Critic=0.000000 | Actor=0.004987 | updates=990\n",
      "[Epoch 6/60] Critic=0.000000 | Actor=0.002926 | updates=990\n",
      "[Epoch 7/60] Critic=0.000000 | Actor=0.001760 | updates=990\n",
      "[Epoch 8/60] Critic=0.000000 | Actor=0.000930 | updates=990\n",
      "[Epoch 9/60] Critic=0.000000 | Actor=0.000314 | updates=990\n",
      "[Epoch 10/60] Critic=0.000000 | Actor=0.000017 | updates=990\n",
      "  -> Saved checkpoint at epoch 10\n",
      "[Epoch 11/60] Critic=0.000000 | Actor=-0.000455 | updates=990\n",
      "[Epoch 12/60] Critic=0.000000 | Actor=-0.000460 | updates=990\n",
      "[Epoch 13/60] Critic=0.000000 | Actor=-0.000368 | updates=990\n",
      "[Epoch 14/60] Critic=0.000000 | Actor=-0.000297 | updates=990\n",
      "[Epoch 15/60] Critic=0.000000 | Actor=-0.000223 | updates=990\n",
      "[Epoch 16/60] Critic=0.000000 | Actor=-0.000203 | updates=990\n",
      "[Epoch 17/60] Critic=0.000000 | Actor=-0.000222 | updates=990\n",
      "[Epoch 18/60] Critic=0.000000 | Actor=-0.000173 | updates=990\n",
      "[Epoch 19/60] Critic=0.000000 | Actor=-0.000144 | updates=990\n",
      "[Epoch 20/60] Critic=0.000000 | Actor=-0.000201 | updates=990\n",
      "  -> Saved checkpoint at epoch 20\n",
      "[Epoch 21/60] Critic=0.000000 | Actor=-0.000270 | updates=990\n",
      "[Epoch 22/60] Critic=0.000000 | Actor=-0.000249 | updates=990\n",
      "[Epoch 23/60] Critic=0.000000 | Actor=-0.000224 | updates=990\n",
      "[Epoch 24/60] Critic=0.000000 | Actor=-0.000184 | updates=990\n",
      "[Epoch 25/60] Critic=0.000000 | Actor=-0.000137 | updates=990\n",
      "[Epoch 26/60] Critic=0.000000 | Actor=-0.000147 | updates=990\n",
      "[Epoch 27/60] Critic=0.000000 | Actor=-0.000428 | updates=990\n",
      "[Epoch 28/60] Critic=0.000000 | Actor=-0.000603 | updates=990\n",
      "[Epoch 29/60] Critic=0.000000 | Actor=-0.000416 | updates=990\n",
      "[Epoch 30/60] Critic=0.000000 | Actor=-0.000299 | updates=990\n",
      "  -> Saved checkpoint at epoch 30\n",
      "[Epoch 31/60] Critic=0.000000 | Actor=-0.000249 | updates=990\n",
      "[Epoch 32/60] Critic=0.000000 | Actor=-0.000238 | updates=990\n",
      "[Epoch 33/60] Critic=0.000000 | Actor=-0.000267 | updates=990\n",
      "[Epoch 34/60] Critic=0.000000 | Actor=-0.000225 | updates=990\n",
      "[Epoch 35/60] Critic=0.000000 | Actor=-0.000229 | updates=990\n",
      "[Epoch 36/60] Critic=0.000000 | Actor=-0.000227 | updates=990\n",
      "[Epoch 37/60] Critic=0.000000 | Actor=-0.000160 | updates=990\n",
      "[Epoch 38/60] Critic=0.000000 | Actor=-0.000117 | updates=990\n",
      "[Epoch 39/60] Critic=0.000000 | Actor=-0.000146 | updates=990\n",
      "[Epoch 40/60] Critic=0.000000 | Actor=-0.000154 | updates=990\n",
      "  -> Saved checkpoint at epoch 40\n",
      "[Epoch 41/60] Critic=0.000000 | Actor=-0.000137 | updates=990\n",
      "[Epoch 42/60] Critic=0.000000 | Actor=-0.000096 | updates=990\n",
      "[Epoch 43/60] Critic=0.000000 | Actor=-0.000064 | updates=990\n",
      "[Epoch 44/60] Critic=0.000000 | Actor=-0.000058 | updates=990\n",
      "[Epoch 45/60] Critic=0.000000 | Actor=-0.000075 | updates=990\n",
      "[Epoch 46/60] Critic=0.000000 | Actor=-0.000080 | updates=990\n",
      "[Epoch 47/60] Critic=0.000000 | Actor=-0.000071 | updates=990\n",
      "[Epoch 48/60] Critic=0.000000 | Actor=-0.000069 | updates=990\n",
      "[Epoch 49/60] Critic=0.000000 | Actor=-0.000064 | updates=990\n",
      "[Epoch 50/60] Critic=0.000000 | Actor=-0.000101 | updates=990\n",
      "  -> Saved checkpoint at epoch 50\n",
      "[Epoch 51/60] Critic=0.000000 | Actor=-0.000167 | updates=990\n",
      "[Epoch 52/60] Critic=0.000000 | Actor=-0.000138 | updates=990\n",
      "[Epoch 53/60] Critic=0.000000 | Actor=-0.000103 | updates=990\n",
      "[Epoch 54/60] Critic=0.000000 | Actor=-0.000072 | updates=990\n",
      "[Epoch 55/60] Critic=0.000000 | Actor=-0.000045 | updates=990\n",
      "[Epoch 56/60] Critic=0.000000 | Actor=-0.000041 | updates=990\n",
      "[Epoch 57/60] Critic=0.000000 | Actor=-0.000027 | updates=990\n",
      "[Epoch 58/60] Critic=0.000000 | Actor=-0.000010 | updates=990\n",
      "[Epoch 59/60] Critic=0.000000 | Actor=-0.000008 | updates=990\n",
      "[Epoch 60/60] Critic=0.000000 | Actor=-0.000004 | updates=990\n",
      "  -> Saved checkpoint at epoch 60\n",
      "✅ Block 10a done. Models saved to ddpg_models Elapsed: 620.9910361766815\n"
     ]
    }
   ],
   "source": [
    "# Block 10a — Train DDPG-like actor–critic (fixed & stable)\n",
    "import os, gc, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "PREP_DIR   = Path(\"./ddpg_prep/\")\n",
    "MODEL_DIR  = Path(\"./ddpg_models/\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- HYPER ----------\n",
    "EPOCHS     = 60\n",
    "BATCH_SIZE = 64\n",
    "LR_ACTOR   = 1e-4\n",
    "LR_CRITIC  = 5e-4\n",
    "GAMMA      = 0.95\n",
    "TAU        = 1e-2\n",
    "NOISE_STD  = 0.05\n",
    "HIDDEN     = 128\n",
    "SEED       = 42\n",
    "REWARD_NORM = True    # normalize rewards (per-batch)\n",
    "CLIP_GRAD = 1.0\n",
    "\n",
    "np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------- Load prepared data ----------\n",
    "S_train = np.load(PREP_DIR / \"S_train.npy\")    # (T, s_dim)\n",
    "R_train = np.load(PREP_DIR / \"R_train.npy\")    # (T, a_dim)\n",
    "dates_train = pd.read_csv(PREP_DIR / \"dates_train.csv\", header=None).squeeze().astype(str).values\n",
    "\n",
    "# sanity\n",
    "assert S_train.ndim == 2, \"S_train should be 2D (T, s_dim)\"\n",
    "assert R_train.ndim == 2, \"R_train should be 2D (T, a_dim)\"\n",
    "T_total, s_dim = S_train.shape\n",
    "_, a_dim = R_train.shape\n",
    "print(\"Train shapes:\", S_train.shape, R_train.shape)\n",
    "\n",
    "# cast\n",
    "S_train = S_train.astype(np.float32)\n",
    "R_train = R_train.astype(np.float32)\n",
    "\n",
    "# optional: standardize states (fit on train)\n",
    "scaler = StandardScaler()\n",
    "S_train = scaler.fit_transform(S_train).astype(np.float32)\n",
    "\n",
    "# ---------- Networks ----------\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, a_dim)\n",
    "        )\n",
    "    def forward(self, s):\n",
    "        # return logits (pre-softmax) to allow stable noise injection outside\n",
    "        return self.net(s)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], dim=-1)\n",
    "        return self.net(x).squeeze(-1)  # return (batch,)\n",
    "\n",
    "# ---------- Replay Buffer ----------\n",
    "class Buffer:\n",
    "    def __init__(self, maxlen=20000):\n",
    "        self.buf=[]; self.maxlen=maxlen\n",
    "    def push(self, s,a,r,s2):\n",
    "        if len(self.buf)>=self.maxlen: self.buf.pop(0)\n",
    "        self.buf.append((s.astype(np.float32), a.astype(np.float32), float(r), s2.astype(np.float32)))\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "    def sample(self, bs):\n",
    "        n = min(bs, len(self.buf))\n",
    "        idx = np.random.choice(len(self.buf), n, replace=False)\n",
    "        s,a,r,s2 = zip(*[self.buf[i] for i in idx])\n",
    "        return (np.stack(s), np.stack(a), np.array(r, dtype=np.float32).reshape(-1,1), np.stack(s2))\n",
    "\n",
    "def soft_update(src, tgt, tau):\n",
    "    with torch.no_grad():\n",
    "        for p, tp in zip(src.parameters(), tgt.parameters()):\n",
    "            tp.data.mul_(1.0 - tau)\n",
    "            tp.data.add_(tau * p.data)\n",
    "\n",
    "def port_reward(w, r):\n",
    "    # w: (a_dim,) weights (sum=1), r: (a_dim,) returns for clusters; return scalar PnL\n",
    "    return float(np.dot(w, r))\n",
    "\n",
    "# ---------- Init models & optim ----------\n",
    "actor = Actor(s_dim, a_dim).to(device)\n",
    "critic = Critic(s_dim, a_dim).to(device)\n",
    "t_actor = Actor(s_dim, a_dim).to(device); t_actor.load_state_dict(actor.state_dict())\n",
    "t_critic = Critic(s_dim, a_dim).to(device); t_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "optA = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "optC = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "buf = Buffer(maxlen=50000)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    epoch_c_loss = 0.0\n",
    "    epoch_a_loss = 0.0\n",
    "    n_updates = 0\n",
    "\n",
    "    # iterate temporally: each step t -> transition (S[t], w_t, r_t, S[t+1])\n",
    "    for t in range(0, T_total - 1):\n",
    "        s = S_train[t]      # (s_dim,)\n",
    "        s2 = S_train[t+1]\n",
    "        # actor deterministic output (logits), then add noise in logit space\n",
    "        actor.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = actor(torch.from_numpy(s).float().unsqueeze(0).to(device)).cpu().numpy().squeeze(0)  # (a_dim,)\n",
    "        # safe log and noise\n",
    "        EPS = 1e-9\n",
    "        logits = np.clip(logits, -1e6, 1e6)   # numeric guard\n",
    "        # add gaussian noise in logit space\n",
    "        noisy = logits + np.random.normal(0, NOISE_STD, size=logits.shape)\n",
    "        # compute weights via softmax of noisy logits\n",
    "        expv = np.exp(noisy - noisy.max())\n",
    "        w_e = (expv / (expv.sum() + EPS)).astype(np.float32)\n",
    "\n",
    "        # immediate portfolio reward using cluster returns at t\n",
    "        r_scalar = port_reward(w_e, R_train[t])\n",
    "        buf.push(s, w_e, r_scalar, s2)\n",
    "\n",
    "        # update if buffer has enough\n",
    "        if len(buf) >= BATCH_SIZE:\n",
    "            sb, ab, rb, s2b = buf.sample(BATCH_SIZE)\n",
    "            sb_t = torch.from_numpy(sb).float().to(device)       # (B, s_dim)\n",
    "            ab_t = torch.from_numpy(ab).float().to(device)       # (B, a_dim)\n",
    "            rb_t = torch.from_numpy(rb).float().squeeze(-1).to(device)  # (B,)\n",
    "\n",
    "            s2b_t = torch.from_numpy(s2b).float().to(device)\n",
    "\n",
    "            # normalize rewards per-batch if desired\n",
    "            if REWARD_NORM:\n",
    "                rb_mean = rb_t.mean()\n",
    "                rb_std = rb_t.std(unbiased=False) + 1e-8\n",
    "                rb_t = (rb_t - rb_mean) / rb_std\n",
    "\n",
    "            # Critic target: y = r + gamma * Q_target(s2, a2)\n",
    "            with torch.no_grad():\n",
    "                a2_logits = t_actor(s2b_t)  # logits\n",
    "                a2_exp = torch.exp(a2_logits - a2_logits.max(dim=1, keepdim=True)[0])\n",
    "                a2 = a2_exp / (a2_exp.sum(dim=1, keepdim=True) + 1e-9)  # (B, a_dim)\n",
    "                q2 = t_critic(s2b_t, a2)   # (B,)\n",
    "                y = rb_t + GAMMA * q2\n",
    "\n",
    "            # Critic update\n",
    "            q = critic(sb_t, ab_t)\n",
    "            lc = mse(q, y)\n",
    "            optC.zero_grad(); lc.backward(); torch.nn.utils.clip_grad_norm_(critic.parameters(), CLIP_GRAD); optC.step()\n",
    "\n",
    "            # Actor update (maximize value predicted by critic)\n",
    "            ap_logits = actor(sb_t)\n",
    "            ap_exp = torch.exp(ap_logits - ap_logits.max(dim=1, keepdim=True)[0])\n",
    "            ap = ap_exp / (ap_exp.sum(dim=1, keepdim=True) + 1e-9)\n",
    "            la = - critic(sb_t, ap).mean()\n",
    "            optA.zero_grad(); la.backward(); torch.nn.utils.clip_grad_norm_(actor.parameters(), CLIP_GRAD); optA.step()\n",
    "\n",
    "            # soft update targets\n",
    "            soft_update(actor, t_actor, TAU)\n",
    "            soft_update(critic, t_critic, TAU)\n",
    "\n",
    "            epoch_c_loss += float(lc.item())\n",
    "            epoch_a_loss += float(la.item())\n",
    "            n_updates += 1\n",
    "\n",
    "            # free\n",
    "            del sb_t, ab_t, rb_t, s2b_t, a2_logits, a2_exp, a2, q2, y, q, lc, ap_logits, ap, la\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # epoch logging\n",
    "    if n_updates > 0:\n",
    "        print(f\"[Epoch {ep}/{EPOCHS}] Critic={epoch_c_loss / n_updates:.6f} | Actor={epoch_a_loss / n_updates:.6f} | updates={n_updates}\")\n",
    "    else:\n",
    "        print(f\"[Epoch {ep}/{EPOCHS}] no updates (buffer too small)\")\n",
    "\n",
    "    # periodic checkpoint\n",
    "    if ep % 10 == 0 or ep == EPOCHS:\n",
    "        torch.save(actor.state_dict(), MODEL_DIR / f\"actor_ep{ep}.pt\")\n",
    "        torch.save(critic.state_dict(), MODEL_DIR / f\"critic_ep{ep}.pt\")\n",
    "        print(\"  -> Saved checkpoint at epoch\", ep)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# final save\n",
    "torch.save(actor.state_dict(), MODEL_DIR / \"actor_final.pt\")\n",
    "torch.save(critic.state_dict(), MODEL_DIR / \"critic_final.pt\")\n",
    "print(\"✅ Block 10a done. Models saved to\", MODEL_DIR, \"Elapsed:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ea5e2",
   "metadata": {},
   "source": [
    "**code mới nhất nằm ngày trên**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9ce46",
   "metadata": {},
   "source": [
    "**BLock 10B bản có chia ra val train test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2989fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers: 390 | Clusters in-use: 4\n",
      "\n",
      "=== VAL_SL1_TP2 ===\n",
      "\n",
      "=== VAL_SL1_TP4 ===\n",
      "\n",
      "=== VAL_SL1_TP6 ===\n",
      "\n",
      "=== VAL_SL1_TP10 ===\n",
      "\n",
      "=== VAL_SL2_TP4 ===\n",
      "\n",
      "=== VAL_SL2_TP6 ===\n",
      "\n",
      "=== VAL_SL2_TP10 ===\n",
      "\n",
      "=== VAL_SL3_TP4 ===\n",
      "\n",
      "=== VAL_SL3_TP6 ===\n",
      "\n",
      "=== VAL_SL3_TP10 ===\n",
      "\n",
      "=== VAL_SL5_TP6 ===\n",
      "\n",
      "=== VAL_SL5_TP10 ===\n",
      "\n",
      "Grid done. Summary saved: backtest_grid_val\\grid_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 10B — Backtest (VAL-only or TEST) with trade_log, holdings and telegram payload\n",
    "import os, gc, json, csv, math, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "BASE_OUT      = Path(\"./backtest_grid_val/\")   # output root\n",
    "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR      = \"./tensors/\"\n",
    "SIG_DIR       = \"./signals/\"\n",
    "MODEL_DIR     = \"./ddpg_models/\"\n",
    "OUTPUT_ROOT   = BASE_OUT                        # will create per-config subfolders\n",
    "\n",
    "SIG_FILE      = os.path.join(SIG_DIR, \"a3c_signals_infer_val.csv\")   # ensure this contains VAL signals\n",
    "DF_PRICE      = \"./backtest_ddpg/df_backtest.csv\"                   # OHLC price (preferred)\n",
    "FALLBACK_PRICE= \"vnindex_price_fa_merged.csv\"\n",
    "ACTOR_PATH    = os.path.join(MODEL_DIR, \"actor_final.pt\")           # actor file (not used here but sanity)\n",
    "TENSOR_INDEX  = os.path.join(\"C:/tensors_out\", \"tensor_index.json\")  # mapping\n",
    "\n",
    "GRID_SL       = [0.01, 0.02, 0.03, 0.05]\n",
    "GRID_TP       = [0.02, 0.04, 0.06, 0.10]\n",
    "\n",
    "ENTRY_LAG     = 2     # signal T -> entry at open T+ENTRY_LAG\n",
    "INIT_CAP      = 10_000.0\n",
    "TOP_K_PER_CLUSTER = 5\n",
    "MIN_NAMES_PER_CLUSTER = 1\n",
    "MAX_HOLD_DAYS = 30\n",
    "COST_BPS      = 30   # per turnover\n",
    "STATE_LKBK    = 10\n",
    "\n",
    "# VAL range (no leakage); set to desired validation range\n",
    "VAL_START = pd.Timestamp(\"2024-01-01\")\n",
    "VAL_END   = pd.Timestamp(\"2024-12-31\")\n",
    "VAL_ONLY = True\n",
    "\n",
    "# ----------------- Helper funcs -----------------\n",
    "def intraday_check(entry_price, day_high, day_low, sl_pct, tp_pct):\n",
    "    if pd.isna(entry_price) or pd.isna(day_high) or pd.isna(day_low):\n",
    "        return None, \"hold\"\n",
    "    sl_price = entry_price * (1 - sl_pct)\n",
    "    tp_price = entry_price * (1 + tp_pct)\n",
    "    # conservative: treat SL as hit first if both satisfy (user choice)\n",
    "    if day_low <= sl_price and day_high >= tp_price:\n",
    "        # which first? choose SL-first conservative\n",
    "        return sl_price, \"sl\"\n",
    "    if day_low <= sl_price:\n",
    "        return sl_price, \"sl\"\n",
    "    if day_high >= tp_price:\n",
    "        return tp_price, \"tp\"\n",
    "    return None, \"hold\"\n",
    "\n",
    "def compute_stats_from_series(port_series):\n",
    "    port_series = port_series.dropna()\n",
    "    if len(port_series) < 2:\n",
    "        return {\"ROI\":0.0,\"Vol\":0.0,\"Sharpe\":0.0,\"MaxDD\":0.0,\"WinRate\":0.0}\n",
    "    ret = port_series.pct_change().dropna()\n",
    "    roi = port_series.iloc[-1]/port_series.iloc[0]-1.0\n",
    "    vol = ret.std() * math.sqrt(252) if len(ret)>1 else 0.0\n",
    "    sharpe = (ret.mean()/ret.std()) * math.sqrt(252) if ret.std()>0 else 0.0\n",
    "    dd = (port_series/port_series.cummax()-1.0).min()\n",
    "    winrate = float((ret>0).mean())\n",
    "    return {\"ROI\":roi, \"Vol\":vol, \"Sharpe\":sharpe, \"MaxDD\":dd, \"WinRate\":winrate}\n",
    "\n",
    "# ----------------- Load price -----------------\n",
    "if not os.path.exists(DF_PRICE):\n",
    "    print(f\"Warning: {DF_PRICE} not found, trying fallback {FALLBACK_PRICE}\")\n",
    "    if not os.path.exists(FALLBACK_PRICE):\n",
    "        raise FileNotFoundError(f\"Neither {DF_PRICE} nor fallback {FALLBACK_PRICE} found.\")\n",
    "    df_px = pd.read_csv(FALLBACK_PRICE)\n",
    "else:\n",
    "    df_px = pd.read_csv(DF_PRICE)\n",
    "\n",
    "# normalize date col\n",
    "if \"timestamp\" in df_px.columns:\n",
    "    df_px = df_px.rename(columns={\"timestamp\":\"date\"})\n",
    "if \"date\" not in df_px.columns:\n",
    "    raise ValueError(\"Price data needs 'date' or 'timestamp' column\")\n",
    "\n",
    "df_px[\"date\"] = pd.to_datetime(df_px[\"date\"])\n",
    "\n",
    "# require OHLC and ticker\n",
    "for c in [\"open\",\"high\",\"low\",\"close\",\"ticker\",\"date\"]:\n",
    "    if c not in df_px.columns:\n",
    "        raise ValueError(f\"Price data missing column '{c}'. Found: {df_px.columns.tolist()}\")\n",
    "\n",
    "# keep only VAL period (prevent leakage)\n",
    "df_px = df_px[(df_px[\"date\"] >= VAL_START) & (df_px[\"date\"] <= VAL_END)].copy()\n",
    "if df_px.empty:\n",
    "    raise ValueError(\"No price rows inside VAL range. Check DF_PRICE and VAL_START/VAL_END\")\n",
    "\n",
    "# pivot to wide OHLC\n",
    "px_open  = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"open\").sort_index()\n",
    "px_high  = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"high\").sort_index()\n",
    "px_low   = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"low\").sort_index()\n",
    "px_close = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "\n",
    "px_index = px_close.index.sort_values().unique()\n",
    "if len(px_index) == 0:\n",
    "    raise ValueError(\"No trading dates after pivot.\")\n",
    "\n",
    "# ----------------- Load signals & mapping -----------------\n",
    "if not os.path.exists(SIG_FILE):\n",
    "    raise FileNotFoundError(f\"Signals file not found: {SIG_FILE}\")\n",
    "signals = pd.read_csv(SIG_FILE)\n",
    "if \"date\" in signals.columns:\n",
    "    signals[\"date\"] = pd.to_datetime(signals[\"date\"])\n",
    "elif \"timestamp\" in signals.columns:\n",
    "    signals[\"date\"] = pd.to_datetime(signals[\"timestamp\"])\n",
    "else:\n",
    "    raise ValueError(\"Signals csv must contain 'date' or 'timestamp' column\")\n",
    "# restrict signals to VAL range\n",
    "signals = signals[(signals[\"date\"] >= VAL_START) & (signals[\"date\"] <= VAL_END)].copy()\n",
    "if signals.empty:\n",
    "    raise FileNotFoundError(\"No signals found in VAL range. Run inference for VAL first (Block 9b).\")\n",
    "\n",
    "if not os.path.exists(TENSOR_INDEX):\n",
    "    raise FileNotFoundError(\"tensor_index.json not found.\")\n",
    "with open(TENSOR_INDEX,\"r\",encoding=\"utf-8\") as fh:\n",
    "    tensor_index = json.load(fh)\n",
    "\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    c = int(meta.get(\"cluster\", -1))\n",
    "    for tk in meta.get(\"tickers\", []):\n",
    "        if tk not in ticker2cluster:\n",
    "            ticker2cluster[tk] = c\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "# intersection of tickers present in price and mapping\n",
    "tickers = sorted(list(set(px_close.columns).intersection(set(ticker2cluster.index))))\n",
    "if len(tickers) == 0:\n",
    "    raise ValueError(\"No tickers overlap between price and cluster mapping (within VAL).\")\n",
    "\n",
    "px_open  = px_open.reindex(columns=tickers)\n",
    "px_high  = px_high.reindex(columns=tickers)\n",
    "px_low   = px_low.reindex(columns=tickers)\n",
    "px_close = px_close.reindex(columns=tickers)\n",
    "\n",
    "# signals -> wide (last per day) and align to px_index\n",
    "sig_wide = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\")\n",
    "sig_wide = sig_wide.reindex(px_index).reindex(columns=tickers).fillna(0.0)\n",
    "\n",
    "# shift for execution lag\n",
    "sig_entry = sig_wide.shift(ENTRY_LAG).reindex(px_index).fillna(0.0)\n",
    "\n",
    "# clusters\n",
    "cluster_of = ticker2cluster.loc[tickers]\n",
    "clusters = sorted(cluster_of.unique().tolist())\n",
    "cluster_members = {c: cluster_of[cluster_of==c].index.tolist() for c in clusters}\n",
    "C = len(clusters)\n",
    "print(f\"Tickers: {len(tickers)} | Clusters in-use: {len(clusters)}\")\n",
    "\n",
    "# ----------------- Build safe valid_index (dates with next-day inside VAL) -----------------\n",
    "# For safety with next-day returns we require the next trading date to exist (we use close/prev_close).\n",
    "all_dates = list(px_index)\n",
    "valid_dates = []\n",
    "for i, d in enumerate(all_dates):\n",
    "    # we will need next day to compute prev_close for next day returns; but our logic uses same-day open->close and close/prev_close\n",
    "    if i+1 < len(all_dates):\n",
    "        # ensure next exists inside VAL (it is because px_index derived from VAL)\n",
    "        valid_dates.append(d)\n",
    "valid_index = pd.DatetimeIndex(valid_dates)\n",
    "if len(valid_index) == 0:\n",
    "    raise ValueError(\"No valid dates for backtest after geting valid_index.\")\n",
    "\n",
    "# reindex to valid_index\n",
    "px_open  = px_open.reindex(valid_index)\n",
    "px_high  = px_high.reindex(valid_index)\n",
    "px_low   = px_low.reindex(valid_index)\n",
    "px_close = px_close.reindex(valid_index)\n",
    "sig_entry = sig_entry.reindex(valid_index).fillna(0.0)\n",
    "\n",
    "# compute daily returns used at cluster-level (close/prev_close)\n",
    "ret_wide = px_close.pct_change().fillna(0.0)\n",
    "\n",
    "# ----------------- Build S_all/R_all (cluster-level state arrays) -----------------\n",
    "# We only need S_all to run actor inference — but for backtest we already precomputed actor weights externally.\n",
    "# Keep minimal: we will not run actor here; we assume cluster_weights will be provided or actor is available.\n",
    "# To keep compatibility, attempt to load precomputed cluster_weights file if exists (ddpg_prep),\n",
    "# else fall back to uniform cluster weights.\n",
    "PRECOMP_CLUSTER_WEIGHTS = None  # path if you have cluster weights precomputed per date (optional)\n",
    "\n",
    "# fallback function to compute cluster-level stats (act fraction and cluster return)\n",
    "def cluster_level_from_signals(sig_entry_df, ret_df, clusters, cluster_members, K=STATE_LKBK):\n",
    "    \"\"\"Return cluster-level A3 (activity fraction) and cret (cluster returns) per date (T x C)\"\"\"\n",
    "    dates = ret_df.index\n",
    "    act_cols, ret_cols = [], []\n",
    "    for c in clusters:\n",
    "        tks = cluster_members.get(c, [])\n",
    "        if len(tks)==0:\n",
    "            act_c = pd.Series(0.0, index=dates, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates, name=c)\n",
    "        else:\n",
    "            S_c = sig_entry_df.reindex(columns=tks).fillna(0.0)\n",
    "            R_c = ret_df.reindex(columns=tks).fillna(0.0)\n",
    "            active_mask = (S_c > 0).astype(\"float32\")\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0).fillna(0.0)\n",
    "            act_c = active_mask.mean(axis=1).astype(\"float32\")\n",
    "            ret_c = (R_c * w).sum(axis=1).astype(\"float32\")\n",
    "        act_cols.append(act_c.rename(c))\n",
    "        ret_cols.append(ret_c.rename(c))\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "    return act_df, cret_df\n",
    "\n",
    "# compute simple cluster_weights per date (uniform on active clusters) if actor not available\n",
    "# Here we try to load actor-based weights computed previously (S_all->actor). If not found we fallback uniform by cluster active fraction.\n",
    "cluster_weights_by_date = None\n",
    "if PRECOMP_CLUSTER_WEIGHTS and os.path.exists(PRECOMP_CLUSTER_WEIGHTS):\n",
    "    cluster_weights_by_date = pd.read_csv(PRECOMP_CLUSTER_WEIGHTS, parse_dates=[\"date\"], index_col=0)\n",
    "    # shape must be (dates x C)\n",
    "    cluster_weights = cluster_weights_by_date.values\n",
    "else:\n",
    "    # fallback: uniform weights across clusters proportional to act fraction (or uniform)\n",
    "    act_df, cret_df = cluster_level_from_signals(sig_entry, ret_wide, clusters, cluster_members, K=STATE_LKBK)\n",
    "    # avoid all-zero\n",
    "    act_sum = act_df.sum(axis=1)\n",
    "    cluster_weights = []\n",
    "    for idx, row in act_df.iterrows():\n",
    "        if row.sum() <= 1e-9:\n",
    "            w = np.ones(len(clusters), dtype=\"float32\") / len(clusters)\n",
    "        else:\n",
    "            # weight clusters proportionally to fraction active, but ensure sum=1\n",
    "            w = row.values.astype(\"float32\")\n",
    "            s = w.sum()\n",
    "            if s <= 0: w = np.ones_like(w) / len(w)\n",
    "            else: w = w / s\n",
    "        cluster_weights.append(w)\n",
    "    cluster_weights = np.stack(cluster_weights, axis=0)\n",
    "\n",
    "# ----------------- Grid search loop (per SL/TP) -----------------\n",
    "summary_rows = []\n",
    "\n",
    "def run_config(sl, tp):\n",
    "    cfg_name = f\"VAL_SL{int(sl*100)}_TP{int(tp*100)}\"\n",
    "    out_dir = OUTPUT_ROOT / cfg_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\n=== {cfg_name} ===\")\n",
    "\n",
    "    dates = list(valid_index)\n",
    "    n_days = len(dates)\n",
    "    capital = float(INIT_CAP)\n",
    "    port_vals = pd.Series(index=dates, dtype=\"float64\")\n",
    "    if n_days>0: port_vals.iloc[0] = capital\n",
    "\n",
    "    prev_weights = pd.Series(0.0, index=tickers)      # prev target ticker weights\n",
    "    pos_info = {}   # ticker -> {entry_price, entry_date, weight, hold_days, shares, notional}\n",
    "    trade_log = []\n",
    "    holding_log = []\n",
    "    turnover_series = pd.Series(index=dates, dtype=\"float64\")\n",
    "    daily_returns = pd.Series(index=dates, dtype=\"float64\")\n",
    "\n",
    "    # precomputed cluster weights aligned to dates length\n",
    "    cw = cluster_weights.copy()\n",
    "    if cw.shape[0] != len(dates):\n",
    "        # try align by index length; else repeat last\n",
    "        if cw.shape[0] > len(dates):\n",
    "            cw = cw[:len(dates)]\n",
    "        else:\n",
    "            last = cw[-1]\n",
    "            extra = np.tile(last, (len(dates)-cw.shape[0],1))\n",
    "            cw = np.vstack([cw, extra])\n",
    "\n",
    "    for i, dt in enumerate(dates):\n",
    "        capital_before = capital\n",
    "        w_c = cw[i]   # (C,)\n",
    "\n",
    "        # build target ticker weights from cluster weights and active candidates\n",
    "        target_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "        for j, c in enumerate(clusters):\n",
    "            members = cluster_members.get(c, [])\n",
    "            if not members: continue\n",
    "            # candidates where sig_entry>0 on this date\n",
    "            try:\n",
    "                row_sig = sig_entry.loc[dt]\n",
    "            except KeyError:\n",
    "                row_sig = pd.Series(0.0, index=tickers)\n",
    "            candidates = [tk for tk in members if (tk in sig_entry.columns and float(sig_entry.loc[dt, tk])>0)]\n",
    "            if len(candidates) < MIN_NAMES_PER_CLUSTER:\n",
    "                continue\n",
    "            # simple momentum ranking (past 5 days within valid_index)\n",
    "            mom_scores = {}\n",
    "            pos_dt = i\n",
    "            start_idx = max(0, pos_dt - 5)\n",
    "            dstart = dates[start_idx]\n",
    "            for tk in candidates:\n",
    "                try:\n",
    "                    vstart = px_close.loc[dstart, tk]\n",
    "                    vend = px_close.loc[dt, tk]\n",
    "                    r = (vend / vstart - 1.0) if (not pd.isna(vstart) and vstart>0) else 0.0\n",
    "                except Exception:\n",
    "                    r = 0.0\n",
    "                mom_scores[tk] = r\n",
    "            topk = sorted(mom_scores.keys(), key=lambda x: mom_scores[x], reverse=True)[:TOP_K_PER_CLUSTER]\n",
    "            if len(topk)==0: continue\n",
    "            share = float(w_c[j]) / len(topk)\n",
    "            for tk in topk:\n",
    "                target_w_ticker[tk] += share\n",
    "\n",
    "        # normalize and zero-out tickers with no price today\n",
    "        valid_price_mask = (~px_close.loc[dt].isna()).astype(float)\n",
    "        valid_tks = [tk for tk in tickers if valid_price_mask.get(tk, 0.0) > 0.0]\n",
    "        ssum = float(target_w_ticker.sum())\n",
    "        if ssum <= 1e-12:\n",
    "            # fallback uniform across available tickers\n",
    "            cnt = max(1, len(valid_tks))\n",
    "            target_w_ticker[:] = 0.0\n",
    "            for tk in valid_tks:\n",
    "                target_w_ticker[tk] = 1.0 / cnt\n",
    "        else:\n",
    "            # zero out missing price tickers\n",
    "            for tk in tickers:\n",
    "                if tk not in valid_tks:\n",
    "                    target_w_ticker[tk] = 0.0\n",
    "            s2 = float(target_w_ticker.sum())\n",
    "            if s2 <= 1e-12:\n",
    "                cnt = max(1, len(valid_tks))\n",
    "                for tk in tickers:\n",
    "                    target_w_ticker[tk] = (1.0/cnt) if tk in valid_tks else 0.0\n",
    "            else:\n",
    "                target_w_ticker = target_w_ticker / float(s2)\n",
    "\n",
    "        turnover = float(np.sum(np.abs(target_w_ticker.values - prev_weights.values)))\n",
    "        turnover_series.iloc[i] = turnover\n",
    "\n",
    "        # 1) Intraday exits (SL/TP) for currently held positions - BEFORE new entries (as requested)\n",
    "        realized_pnl_currency = 0.0\n",
    "        closed_tks = []\n",
    "        for tk, info in list(pos_info.items()):\n",
    "            entry_price = info[\"entry_price\"]\n",
    "            shares = info.get(\"shares\", 0.0)\n",
    "            day_high = px_high.loc[dt, tk] if (tk in px_high.columns and dt in px_high.index) else np.nan\n",
    "            day_low  = px_low.loc[dt, tk] if (tk in px_low.columns and dt in px_low.index) else np.nan\n",
    "            exit_price, exit_type = intraday_check(entry_price, day_high, day_low, sl, tp)\n",
    "            if exit_price is not None:\n",
    "                # compute realized pnl in currency (based on shares)\n",
    "                realized_pnl_curr = shares * (exit_price - entry_price)\n",
    "                realized_pnl_currency += float(realized_pnl_curr)\n",
    "                # log SELL\n",
    "                trade_log.append({\n",
    "                    \"date\": pd.Timestamp(dt).isoformat(),\n",
    "                    \"ticker\": tk,\n",
    "                    \"action\": \"SELL\",\n",
    "                    \"price\": float(exit_price),\n",
    "                    \"weight\": float(info[\"weight\"]),\n",
    "                    \"shares\": float(shares),\n",
    "                    \"notional_at_entry\": float(info[\"notional\"]),\n",
    "                    \"pnl_currency\": float(realized_pnl_curr),\n",
    "                    \"reason\": exit_type\n",
    "                })\n",
    "                closed_tks.append(tk)\n",
    "                del pos_info[tk]\n",
    "\n",
    "        # 2) New entries / rebalances at open (after exits)\n",
    "        new_entries = []\n",
    "        for tk in tickers:\n",
    "            new_w = float(target_w_ticker[tk])\n",
    "            old_w = float(prev_weights.get(tk, 0.0))\n",
    "            # if target weight >0 and change relative to position -> create/update\n",
    "            if new_w > 0 and (tk not in pos_info or abs(new_w - old_w) > 1e-6):\n",
    "                op = px_open.loc[dt, tk] if (tk in px_open.columns and dt in px_open.index) else np.nan\n",
    "                if pd.isna(op) or op == 0:\n",
    "                    continue\n",
    "                notional = capital_before * new_w\n",
    "                shares = (notional / op) if op>0 else 0.0\n",
    "                pos_info[tk] = {\"entry_price\": float(op), \"entry_date\": dt, \"weight\": new_w, \"hold_days\": 0, \"shares\": float(shares), \"notional\": float(notional)}\n",
    "                new_entries.append(tk)\n",
    "                trade_log.append({\n",
    "                    \"date\": pd.Timestamp(dt).isoformat(),\n",
    "                    \"ticker\": tk,\n",
    "                    \"action\": \"BUY\",\n",
    "                    \"price\": float(op),\n",
    "                    \"weight\": float(new_w),\n",
    "                    \"shares\": float(shares),\n",
    "                    \"notional_at_entry\": float(notional),\n",
    "                    \"pnl_currency\": 0.0,\n",
    "                    \"reason\": \"entry/rebalance\"\n",
    "                })\n",
    "            else:\n",
    "                # weight update for existing positions\n",
    "                if tk in pos_info:\n",
    "                    pos_info[tk][\"weight\"] = new_w\n",
    "\n",
    "        prev_weights = target_w_ticker.copy()\n",
    "\n",
    "        # 3) Compute day returns (based on current positions after exits and after new entries executed at open)\n",
    "        day_ret_fraction = 0.0\n",
    "        prev_close = px_close.shift(1).loc[dt] if dt in px_close.index else pd.Series(index=tickers, data=[np.nan]*len(tickers))\n",
    "        close_today = px_close.loc[dt]\n",
    "\n",
    "        for tk in tickers:\n",
    "            w = float(prev_weights[tk])\n",
    "            if math.isclose(w, 0.0):\n",
    "                continue\n",
    "            if tk in new_entries:\n",
    "                # new entries: open -> close\n",
    "                op = px_open.loc[dt, tk] if (tk in px_open.columns and dt in px_open.index) else np.nan\n",
    "                cl = close_today.get(tk, np.nan)\n",
    "                if pd.isna(op) or pd.isna(cl) or op==0:\n",
    "                    ret = 0.0\n",
    "                else:\n",
    "                    ret = (cl / op - 1.0)\n",
    "            else:\n",
    "                prev_c = prev_close.get(tk, np.nan)\n",
    "                cl = close_today.get(tk, np.nan)\n",
    "                if pd.isna(prev_c) or prev_c==0 or pd.isna(cl):\n",
    "                    ret = 0.0\n",
    "                else:\n",
    "                    ret = (cl / prev_c - 1.0)\n",
    "            day_ret_fraction += w * float(ret)\n",
    "\n",
    "        # realized_pnl_currency from SL/TP already computed; convert to fraction of capital_before\n",
    "        realized_pnl_fraction = (realized_pnl_currency / capital_before) if capital_before != 0 else 0.0\n",
    "\n",
    "        total_ret = day_ret_fraction + realized_pnl_fraction\n",
    "        fee = (COST_BPS / 1e4) * turnover\n",
    "        total_ret_net = total_ret - fee\n",
    "\n",
    "        # update capital multiplicatively\n",
    "        capital = capital_before * (1.0 + total_ret_net)\n",
    "        port_vals.iloc[i] = capital\n",
    "        daily_returns.iloc[i] = total_ret_net\n",
    "\n",
    "        # log holdings snapshot at end of day (after update)\n",
    "        for tk, info in pos_info.items():\n",
    "            holding_log.append({\n",
    "                \"date\": pd.Timestamp(dt).isoformat(),\n",
    "                \"ticker\": tk,\n",
    "                \"weight\": float(info[\"weight\"]),\n",
    "                \"entry_price\": float(info[\"entry_price\"]),\n",
    "                \"shares\": float(info.get(\"shares\", 0.0)),\n",
    "                \"hold_days\": int(info.get(\"hold_days\", 0)),\n",
    "                \"notional_at_entry\": float(info.get(\"notional\", 0.0))\n",
    "            })\n",
    "\n",
    "        # increment hold_days and forced close if too long\n",
    "        for tk in list(pos_info.keys()):\n",
    "            pos_info[tk][\"hold_days\"] = pos_info[tk].get(\"hold_days\", 0) + 1\n",
    "            if pos_info[tk][\"hold_days\"] >= MAX_HOLD_DAYS:\n",
    "                # forced close at close price today (mark-to-market)\n",
    "                cl = px_close.loc[dt, tk] if (tk in px_close.columns and dt in px_close.index) else np.nan\n",
    "                if not pd.isna(cl):\n",
    "                    shares = pos_info[tk].get(\"shares\", 0.0)\n",
    "                    entry_price = pos_info[tk][\"entry_price\"]\n",
    "                    realized_pnl_curr = shares * (cl - entry_price)\n",
    "                    trade_log.append({\n",
    "                        \"date\": pd.Timestamp(dt).isoformat(),\n",
    "                        \"ticker\": tk,\n",
    "                        \"action\": \"SELL\",\n",
    "                        \"price\": float(cl),\n",
    "                        \"weight\": float(pos_info[tk][\"weight\"]),\n",
    "                        \"shares\": float(shares),\n",
    "                        \"notional_at_entry\": float(pos_info[tk].get(\"notional\", 0.0)),\n",
    "                        \"pnl_currency\": float(realized_pnl_curr),\n",
    "                        \"reason\": \"forced_hold_limit\"\n",
    "                    })\n",
    "                del pos_info[tk]\n",
    "\n",
    "        # update prev_weights already done\n",
    "        # housekeeping\n",
    "        gc.collect()\n",
    "    # end dates loop\n",
    "\n",
    "    # Save outputs\n",
    "    pd.Series(port_vals).to_csv(out_dir / \"portfolio_value.csv\")\n",
    "    pd.Series(daily_returns).to_csv(out_dir / \"daily_returns.csv\")\n",
    "    turnover_series.to_csv(out_dir / \"turnover.csv\")\n",
    "\n",
    "    trade_df = pd.DataFrame(trade_log)\n",
    "    holding_df = pd.DataFrame(holding_log)\n",
    "    trade_df.to_csv(out_dir / \"trade_log.csv\", index=False)\n",
    "    holding_df.to_csv(out_dir / \"holdings_snapshot.csv\", index=False)\n",
    "\n",
    "    # telegram payload + latest holdings\n",
    "    final_cap = float(port_vals.dropna().iloc[-1]) if port_vals.dropna().size>0 else float(capital)\n",
    "    stats = compute_stats_from_series(port_vals)\n",
    "    top_holdings = holding_df[holding_df[\"date\"]==holding_df[\"date\"].max()].sort_values(\"weight\", ascending=False).head(10) if not holding_df.empty else pd.DataFrame()\n",
    "    telegram_text = []\n",
    "    telegram_text.append(f\"Config: {cfg_name}\")\n",
    "    telegram_text.append(f\"Final capital: {final_cap:,.2f}\")\n",
    "    telegram_text.append(f\"ROI: {stats['ROI']:.2%} | Sharpe: {stats['Sharpe']:.3f} | MaxDD: {stats['MaxDD']:.2%}\")\n",
    "    telegram_text.append(\"Top holdings (end):\")\n",
    "    if not top_holdings.empty:\n",
    "        for _, r in top_holdings.iterrows():\n",
    "            telegram_text.append(f\"  {r['ticker']}: weight={r['weight']:.2%}, shares={int(r['shares'])}, entry={r['entry_price']:.2f}\")\n",
    "    else:\n",
    "        telegram_text.append(\"  (no holdings)\")\n",
    "    with open(out_dir / \"telegram_message.txt\", \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"\\n\".join(telegram_text))\n",
    "\n",
    "    # also save json payload with summary + last holdings for easy reading by block12\n",
    "    payload = {\"config\": cfg_name, \"final_capital\": final_cap, \"stats\": stats}\n",
    "    payload[\"latest_holdings\"] = top_holdings.to_dict(orient=\"records\") if not top_holdings.empty else []\n",
    "    with open(out_dir / \"telegram_payload.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(payload, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # return summary row\n",
    "    res = {\n",
    "        \"config\": cfg_name,\n",
    "        \"sl\": sl,\n",
    "        \"tp\": tp,\n",
    "        \"ROI\": stats[\"ROI\"],\n",
    "        \"Vol\": stats[\"Vol\"],\n",
    "        \"Sharpe\": stats[\"Sharpe\"],\n",
    "        \"MaxDD\": stats[\"MaxDD\"],\n",
    "        \"WinRate\": stats[\"WinRate\"],\n",
    "        \"final_capital\": final_cap,\n",
    "        \"n_days\": len(port_vals.dropna())\n",
    "    }\n",
    "    return res\n",
    "\n",
    "# Loop grid\n",
    "for sl in GRID_SL:\n",
    "    for tp in GRID_TP:\n",
    "        if tp <= sl: continue\n",
    "        try:\n",
    "            row = run_config(sl, tp)\n",
    "            summary_rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"Config SL={sl},TP={tp} failed: {e}\")\n",
    "\n",
    "# Save overall grid summary\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(by=\"Sharpe\", ascending=False).reset_index(drop=True)\n",
    "summary_df.to_csv(OUTPUT_ROOT / \"grid_summary.csv\", index=False)\n",
    "print(\"\\nGrid done. Summary saved:\", str(OUTPUT_ROOT / \"grid_summary.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e0e1c",
   "metadata": {},
   "source": [
    "**BLOCK 10C:chọnconfig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "209d678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates test: 2025-04-10 00:00:00 → 2025-10-02 00:00:00 | 121 ngày | 391 mã\n",
      "✅ Block 10C hoàn tất. Saved to backtest_test\n"
     ]
    }
   ],
   "source": [
    "# Block 10C — Backtest Test Set với SL/TP (min holding days = 2, fix NaN + clean log)\n",
    "import os, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "SIG_DIR   = Path(\"./signals/\")\n",
    "BT_DIR    = Path(\"./backtest_test/\")\n",
    "os.makedirs(BT_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE  = SIG_DIR / \"a3c_signals_infer.csv\"   \n",
    "DF_BACK   = Path(\"./backtest_ddpg/df_backtest.csv\")\n",
    "\n",
    "SL, TP    = 0.01, 0.04   # chọn từ kết quả val\n",
    "MIN_HOLD  = 2            \n",
    "capital0  = 1_000_000\n",
    "\n",
    "# ---------- Load ----------\n",
    "if not SIG_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Không tìm thấy file signals TEST: {SIG_FILE}\")\n",
    "if not DF_BACK.exists():\n",
    "    raise FileNotFoundError(f\"Không tìm thấy df_backtest: {DF_BACK}\")\n",
    "\n",
    "signals = pd.read_csv(SIG_FILE, parse_dates=[\"date\"])\n",
    "df_px   = pd.read_csv(DF_BACK, parse_dates=[\"date\"])\n",
    "df_px   = df_px.rename(columns={\"date\":\"timestamp\"}).sort_values([\"timestamp\",\"ticker\"])\n",
    "\n",
    "# pivot giá\n",
    "px_open  = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"open\")\n",
    "px_high  = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"high\")\n",
    "px_low   = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"low\")\n",
    "px_close = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\")\n",
    "\n",
    "dates   = sorted(list(set(signals[\"date\"]) & set(px_close.index)))\n",
    "tickers = sorted(signals[\"ticker\"].unique())\n",
    "print(f\"Dates test: {dates[0]} → {dates[-1]} | {len(dates)} ngày | {len(tickers)} mã\")\n",
    "\n",
    "# ---------- Backtest ----------\n",
    "capital   = capital0\n",
    "positions = {}  \n",
    "equity_curve = []\n",
    "trade_log = []\n",
    "\n",
    "for dt in dates:\n",
    "    sig_today = signals[signals[\"date\"]==dt].set_index(\"ticker\")[\"signal\"].to_dict()\n",
    "    cap_avail = capital\n",
    "\n",
    "    # update positions\n",
    "    closed_today = []\n",
    "    for tkr, pos in list(positions.items()):\n",
    "        if tkr not in px_close.columns: \n",
    "            continue\n",
    "        o,h,l,c = px_open.at[dt,tkr], px_high.at[dt,tkr], px_low.at[dt,tkr], px_close.at[dt,tkr]\n",
    "        if any(pd.isna([o,h,l,c])): \n",
    "            continue\n",
    "\n",
    "        pos[\"holding_days\"] += 1\n",
    "        exit_flag = False; exit_price = None; reason = None\n",
    "\n",
    "        if pos[\"holding_days\"] >= MIN_HOLD:\n",
    "            if pos[\"side\"]==1:  # long\n",
    "                if l <= pos[\"entry\"]*(1-SL):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1-SL); reason=\"SL\"\n",
    "                elif h >= pos[\"entry\"]*(1+TP):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1+TP); reason=\"TP\"\n",
    "            elif pos[\"side\"]==-1: # short\n",
    "                if h >= pos[\"entry\"]*(1+SL):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1+SL); reason=\"SL\"\n",
    "                elif l <= pos[\"entry\"]*(1-TP):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1-TP); reason=\"TP\"\n",
    "\n",
    "        if exit_flag:\n",
    "            pnl = (exit_price - pos[\"entry\"]) / pos[\"entry\"] * pos[\"side\"] * pos[\"size\"]\n",
    "            capital += pnl\n",
    "            trade_log.append({\n",
    "                \"ticker\": tkr, \"side\": pos[\"side\"],\n",
    "                \"date_entry\": pos[\"entry_date\"], \"entry\": pos[\"entry\"],\n",
    "                \"date_exit\": dt, \"exit\": exit_price,\n",
    "                \"size\": pos[\"size\"], \"holding_days\": pos[\"holding_days\"],\n",
    "                \"pnl\": pnl, \"reason\": reason\n",
    "            })\n",
    "            closed_today.append(tkr)\n",
    "\n",
    "    for t in closed_today: \n",
    "        del positions[t]\n",
    "\n",
    "    # mở vị thế mới\n",
    "    for tkr, sig in sig_today.items():\n",
    "        if sig==0 or tkr in positions: \n",
    "            continue\n",
    "        if tkr not in px_open.columns: \n",
    "            continue\n",
    "        entry_price = px_open.at[dt,tkr]\n",
    "        if pd.isna(entry_price): \n",
    "            continue\n",
    "        size = cap_avail/len(sig_today) if len(sig_today)>0 else 0\n",
    "        positions[tkr] = {\n",
    "            \"side\": sig, \"entry\": entry_price, \"entry_date\": dt,\n",
    "            \"size\": size, \"holding_days\": 0\n",
    "        }\n",
    "        trade_log.append({\n",
    "            \"ticker\": tkr, \"side\": sig,\n",
    "            \"date_entry\": dt, \"entry\": entry_price,\n",
    "            \"size\": size, \"holding_days\":0,\n",
    "            \"pnl\": 0.0, \"reason\":\"entry\"\n",
    "        })\n",
    "\n",
    "    # mark-to-market equity\n",
    "    eq_val = capital\n",
    "    for t,pos in positions.items():\n",
    "        if t in px_close.columns:\n",
    "            px_val = px_close.at[dt,t]\n",
    "            if pd.notna(px_val):\n",
    "                eq_val += (px_val - pos[\"entry\"]) / pos[\"entry\"] * pos[\"side\"] * pos[\"size\"]\n",
    "    if pd.isna(eq_val):\n",
    "        eq_val = capital\n",
    "    equity_curve.append({\"date\": dt, \"equity\": eq_val})\n",
    "\n",
    "# ---------- Save ----------\n",
    "equity_df = pd.DataFrame(equity_curve).set_index(\"date\")\n",
    "equity_df.to_csv(BT_DIR/\"equity_curve.csv\")\n",
    "\n",
    "pd.DataFrame(trade_log).to_csv(BT_DIR/\"trade_log.csv\", index=False)\n",
    "\n",
    "# extra files cho Block 11a-Test\n",
    "equity_df.to_csv(BT_DIR/\"portfolio_value.csv\")\n",
    "equity_df[\"returns\"] = equity_df[\"equity\"].pct_change().fillna(0.0)\n",
    "equity_df[[\"returns\"]].to_csv(BT_DIR/\"daily_returns.csv\")\n",
    "\n",
    "print(f\"✅ Block 10C hoàn tất. Saved to {BT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f267a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates test: 2025-02-06 00:00:00 → 2025-10-02 00:00:00 | 165 ngày | 391 mã\n",
      "✅ Block 10C hoàn tất. Mode=Strict, Fee=0.10%\n"
     ]
    }
   ],
   "source": [
    "# Block 10C — Backtest Test Set với SL/TP (Rebalance/Strict option + Fee)\n",
    "import os, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "SIG_DIR   = Path(\"./signals/\")\n",
    "BT_DIR    = Path(\"./backtest_test/\")\n",
    "os.makedirs(BT_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE  = SIG_DIR / \"a3c_signals_infer.csv\"   \n",
    "DF_BACK   = Path(\"./backtest_ddpg/df_backtest.csv\")\n",
    "\n",
    "# --- Params ---\n",
    "SL, TP     = 0.01, 0.04   # stoploss & takeprofit\n",
    "MIN_HOLD   = 2            \n",
    "capital0   = 1_000_000\n",
    "FEE_RATE   = 0.001        # 0.1% phí giao dịch\n",
    "REBALANCE  = False        # True = chia lại vốn mỗi ngày, False = strict free-capital\n",
    "\n",
    "# ---------- Load ----------\n",
    "signals = pd.read_csv(SIG_FILE, parse_dates=[\"date\"])\n",
    "df_px   = pd.read_csv(DF_BACK, parse_dates=[\"date\"])\n",
    "df_px   = df_px.rename(columns={\"date\":\"timestamp\"}).sort_values([\"timestamp\",\"ticker\"])\n",
    "\n",
    "px_open  = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"open\")\n",
    "px_high  = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"high\")\n",
    "px_low   = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"low\")\n",
    "px_close = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\")\n",
    "\n",
    "dates   = sorted(list(set(signals[\"date\"]) & set(px_close.index)))\n",
    "tickers = sorted(signals[\"ticker\"].unique())\n",
    "print(f\"Dates test: {dates[0]} → {dates[-1]} | {len(dates)} ngày | {len(tickers)} mã\")\n",
    "\n",
    "# ---------- Backtest ----------\n",
    "capital   = capital0\n",
    "positions = {}  \n",
    "equity_curve = []\n",
    "trade_log = []\n",
    "\n",
    "for dt in dates:\n",
    "    sig_today = signals[signals[\"date\"]==dt].set_index(\"ticker\")[\"signal\"].to_dict()\n",
    "\n",
    "    # ---------- Update positions ----------\n",
    "    closed_today = []\n",
    "    for tkr, pos in list(positions.items()):\n",
    "        if tkr not in px_close.columns: \n",
    "            continue\n",
    "        o,h,l,c = px_open.at[dt,tkr], px_high.at[dt,tkr], px_low.at[dt,tkr], px_close.at[dt,tkr]\n",
    "        if any(pd.isna([o,h,l,c])): \n",
    "            continue\n",
    "\n",
    "        pos[\"holding_days\"] += 1\n",
    "        exit_flag = False; exit_price = None; reason = None\n",
    "\n",
    "        if pos[\"holding_days\"] >= MIN_HOLD:\n",
    "            if pos[\"side\"]==1:  # long\n",
    "                if l <= pos[\"entry\"]*(1-SL):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1-SL); reason=\"SL\"\n",
    "                elif h >= pos[\"entry\"]*(1+TP):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1+TP); reason=\"TP\"\n",
    "            elif pos[\"side\"]==-1: # short\n",
    "                if h >= pos[\"entry\"]*(1+SL):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1+SL); reason=\"SL\"\n",
    "                elif l <= pos[\"entry\"]*(1-TP):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1-TP); reason=\"TP\"\n",
    "\n",
    "        if exit_flag:\n",
    "            # pnl raw\n",
    "            pnl_raw = (exit_price - pos[\"entry\"]) / pos[\"entry\"] * pos[\"side\"] * pos[\"size\"]\n",
    "            # phí\n",
    "            fee_open  = pos[\"size\"] * FEE_RATE\n",
    "            fee_close = (pos[\"size\"] * exit_price / pos[\"entry\"]) * FEE_RATE\n",
    "            pnl_after = pnl_raw - fee_open - fee_close\n",
    "            capital += pnl_after\n",
    "\n",
    "            trade_log.append({\n",
    "                \"ticker\": tkr, \"side\": pos[\"side\"],\n",
    "                \"date_entry\": pos[\"entry_date\"], \"entry\": pos[\"entry\"],\n",
    "                \"date_exit\": dt, \"exit\": exit_price,\n",
    "                \"size\": pos[\"size\"], \"holding_days\": pos[\"holding_days\"],\n",
    "                \"pnl_raw\": pnl_raw, \"pnl_after_fee\": pnl_after, \"reason\": reason\n",
    "            })\n",
    "            closed_today.append(tkr)\n",
    "\n",
    "    for t in closed_today: \n",
    "        del positions[t]\n",
    "\n",
    "    # ---------- Mở vị thế mới ----------\n",
    "    if REBALANCE:\n",
    "        # phân bổ lại vốn đều cho toàn bộ positions + lệnh mới\n",
    "        n_new = sum([1 for tkr,sig in sig_today.items() if sig!=0 and tkr not in positions])\n",
    "        n_total = len(positions) + n_new\n",
    "        if n_total > 0:\n",
    "            alloc = capital / n_total\n",
    "            # cập nhật lại size các pos cũ\n",
    "            for tkr,pos in positions.items():\n",
    "                pos[\"size\"] = alloc\n",
    "            # mở pos mới\n",
    "            for tkr,sig in sig_today.items():\n",
    "                if sig==0 or tkr in positions: continue\n",
    "                entry_price = px_open.at[dt,tkr]\n",
    "                if pd.isna(entry_price): continue\n",
    "                positions[tkr] = {\n",
    "                    \"side\": sig, \"entry\": entry_price, \"entry_date\": dt,\n",
    "                    \"size\": alloc, \"holding_days\": 0\n",
    "                }\n",
    "                fee_open = alloc * FEE_RATE\n",
    "                trade_log.append({\n",
    "                    \"ticker\": tkr, \"side\": sig,\n",
    "                    \"date_entry\": dt, \"entry\": entry_price,\n",
    "                    \"size\": alloc, \"holding_days\":0,\n",
    "                    \"pnl_raw\": 0.0, \"pnl_after_fee\": -fee_open,\n",
    "                    \"reason\":\"entry\"\n",
    "                })\n",
    "                capital -= fee_open\n",
    "    else:\n",
    "        # strict free-capital\n",
    "        free_cap = capital - sum([p[\"size\"] for p in positions.values()])\n",
    "        new_sigs = [tkr for tkr,sig in sig_today.items() if sig!=0 and tkr not in positions]\n",
    "        if free_cap > 0 and len(new_sigs)>0:\n",
    "            alloc = free_cap / len(new_sigs)\n",
    "            for tkr in new_sigs:\n",
    "                entry_price = px_open.at[dt,tkr]\n",
    "                if pd.isna(entry_price): continue\n",
    "                positions[tkr] = {\n",
    "                    \"side\": sig_today[tkr], \"entry\": entry_price, \"entry_date\": dt,\n",
    "                    \"size\": alloc, \"holding_days\": 0\n",
    "                }\n",
    "                fee_open = alloc * FEE_RATE\n",
    "                trade_log.append({\n",
    "                    \"ticker\": tkr, \"side\": sig_today[tkr],\n",
    "                    \"date_entry\": dt, \"entry\": entry_price,\n",
    "                    \"size\": alloc, \"holding_days\":0,\n",
    "                    \"pnl_raw\": 0.0, \"pnl_after_fee\": -fee_open,\n",
    "                    \"reason\":\"entry\"\n",
    "                })\n",
    "                capital -= fee_open\n",
    "\n",
    "    # ---------- mark-to-market equity ----------\n",
    "    eq_val = capital\n",
    "    for t,pos in positions.items():\n",
    "        if t in px_close.columns:\n",
    "            px_val = px_close.at[dt,t]\n",
    "            if pd.notna(px_val):\n",
    "                eq_val += (px_val - pos[\"entry\"]) / pos[\"entry\"] * pos[\"side\"] * pos[\"size\"]\n",
    "    equity_curve.append({\"date\": dt, \"equity\": eq_val})\n",
    "\n",
    "# ---------- Save ----------\n",
    "equity_df = pd.DataFrame(equity_curve).set_index(\"date\")\n",
    "equity_df.to_csv(BT_DIR/\"equity_curve.csv\")\n",
    "pd.DataFrame(trade_log).to_csv(BT_DIR/\"trade_log.csv\", index=False)\n",
    "\n",
    "equity_df.to_csv(BT_DIR/\"portfolio_value.csv\")\n",
    "equity_df[\"returns\"] = equity_df[\"equity\"].pct_change().fillna(0.0)\n",
    "equity_df[[\"returns\"]].to_csv(BT_DIR/\"daily_returns.csv\")\n",
    "\n",
    "print(f\"✅ Block 10C hoàn tất. Mode={'Rebalance' if REBALANCE else 'Strict'}, Fee={FEE_RATE*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c39139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates test: 2025-02-06 00:00:00 → 2025-10-02 00:00:00 | 165 ngày | 391 mã\n",
      "✅ Block 10C hoàn tất. Mode=Strict, Fee=0.10%, MaxPos=100, TopK=30\n"
     ]
    }
   ],
   "source": [
    "# Block 10C — Backtest Test Set với SL/TP (Strict free-capital + Top-k signals + Fee)\n",
    "import os, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "SIG_DIR   = Path(\"./signals/\")\n",
    "BT_DIR    = Path(\"./backtest_test/\")\n",
    "os.makedirs(BT_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE  = SIG_DIR / \"a3c_signals_infer.csv\"   \n",
    "DF_BACK   = Path(\"./backtest_ddpg/df_backtest.csv\")\n",
    "\n",
    "# --- Params ---\n",
    "SL, TP       = 0.01, 0.04   # stoploss & takeprofit (cố định)\n",
    "MIN_HOLD     = 2            \n",
    "capital0     = 1_000_000\n",
    "FEE_RATE     = 0.001        # 0.1% phí giao dịch\n",
    "MAX_POSITIONS= 100           # tổng số mã tối đa giữ trong danh mục\n",
    "TOPK_DAILY   = 30            # số tín hiệu tối đa mở mới mỗi ngày\n",
    "\n",
    "# ---------- Load ----------\n",
    "signals = pd.read_csv(SIG_FILE, parse_dates=[\"date\"])\n",
    "df_px   = pd.read_csv(DF_BACK, parse_dates=[\"date\"])\n",
    "df_px   = df_px.rename(columns={\"date\":\"timestamp\"}).sort_values([\"timestamp\",\"ticker\"])\n",
    "\n",
    "px_open  = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"open\")\n",
    "px_high  = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"high\")\n",
    "px_low   = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"low\")\n",
    "px_close = df_px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\")\n",
    "\n",
    "dates   = sorted(list(set(signals[\"date\"]) & set(px_close.index)))\n",
    "tickers = sorted(signals[\"ticker\"].unique())\n",
    "print(f\"Dates test: {dates[0]} → {dates[-1]} | {len(dates)} ngày | {len(tickers)} mã\")\n",
    "\n",
    "# ---------- Backtest ----------\n",
    "capital   = capital0\n",
    "positions = {}  \n",
    "equity_curve = []\n",
    "trade_log = []\n",
    "\n",
    "for dt in dates:\n",
    "    sig_today = signals[signals[\"date\"]==dt].set_index(\"ticker\")[\"signal\"].to_dict()\n",
    "\n",
    "    # ---------- Update positions ----------\n",
    "    closed_today = []\n",
    "    for tkr, pos in list(positions.items()):\n",
    "        if tkr not in px_close.columns: \n",
    "            continue\n",
    "        o,h,l,c = px_open.at[dt,tkr], px_high.at[dt,tkr], px_low.at[dt,tkr], px_close.at[dt,tkr]\n",
    "        if any(pd.isna([o,h,l,c])): \n",
    "            continue\n",
    "\n",
    "        pos[\"holding_days\"] += 1\n",
    "        exit_flag = False; exit_price = None; reason = None\n",
    "\n",
    "        if pos[\"holding_days\"] >= MIN_HOLD:\n",
    "            if pos[\"side\"]==1:  # long\n",
    "                if l <= pos[\"entry\"]*(1-SL):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1-SL); reason=\"SL\"\n",
    "                elif h >= pos[\"entry\"]*(1+TP):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1+TP); reason=\"TP\"\n",
    "            elif pos[\"side\"]==-1: # short\n",
    "                if h >= pos[\"entry\"]*(1+SL):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1+SL); reason=\"SL\"\n",
    "                elif l <= pos[\"entry\"]*(1-TP):\n",
    "                    exit_flag=True; exit_price=pos[\"entry\"]*(1-TP); reason=\"TP\"\n",
    "\n",
    "        if exit_flag:\n",
    "            pnl_raw = (exit_price - pos[\"entry\"]) / pos[\"entry\"] * pos[\"side\"] * pos[\"size\"]\n",
    "            fee_open  = pos[\"size\"] * FEE_RATE\n",
    "            fee_close = (pos[\"size\"] * exit_price / pos[\"entry\"]) * FEE_RATE\n",
    "            pnl_after = pnl_raw - fee_open - fee_close\n",
    "            capital += pnl_after\n",
    "\n",
    "            trade_log.append({\n",
    "                \"ticker\": tkr, \"side\": pos[\"side\"],\n",
    "                \"date_entry\": pos[\"entry_date\"], \"entry\": pos[\"entry\"],\n",
    "                \"date_exit\": dt, \"exit\": exit_price,\n",
    "                \"size\": pos[\"size\"], \"holding_days\": pos[\"holding_days\"],\n",
    "                \"pnl_raw\": pnl_raw, \"pnl_after_fee\": pnl_after, \"reason\": reason\n",
    "            })\n",
    "            closed_today.append(tkr)\n",
    "\n",
    "    for t in closed_today: \n",
    "        del positions[t]\n",
    "\n",
    "    # ---------- Mở vị thế mới (giới hạn Top-k + Max positions) ----------\n",
    "    if len(positions) < MAX_POSITIONS:\n",
    "        free_cap = capital - sum([p[\"size\"] for p in positions.values()])\n",
    "        new_sigs = [tkr for tkr,sig in sig_today.items() if sig!=0 and tkr not in positions]\n",
    "        if len(new_sigs) > 0 and free_cap > 0:\n",
    "            # chỉ lấy tối đa TOPK_DAILY\n",
    "            new_sigs = new_sigs[:TOPK_DAILY]\n",
    "            # nếu quá MAX_POSITIONS thì cắt bớt\n",
    "            slots_avail = MAX_POSITIONS - len(positions)\n",
    "            new_sigs = new_sigs[:slots_avail]\n",
    "            # phân bổ vốn\n",
    "            alloc = free_cap / len(new_sigs) if len(new_sigs)>0 else 0\n",
    "            for tkr in new_sigs:\n",
    "                entry_price = px_open.at[dt,tkr]\n",
    "                if pd.isna(entry_price): continue\n",
    "                positions[tkr] = {\n",
    "                    \"side\": sig_today[tkr], \"entry\": entry_price, \"entry_date\": dt,\n",
    "                    \"size\": alloc, \"holding_days\": 0\n",
    "                }\n",
    "                fee_open = alloc * FEE_RATE\n",
    "                trade_log.append({\n",
    "                    \"ticker\": tkr, \"side\": sig_today[tkr],\n",
    "                    \"date_entry\": dt, \"entry\": entry_price,\n",
    "                    \"size\": alloc, \"holding_days\":0,\n",
    "                    \"pnl_raw\": 0.0, \"pnl_after_fee\": -fee_open,\n",
    "                    \"reason\":\"entry\"\n",
    "                })\n",
    "                capital -= fee_open\n",
    "\n",
    "    # ---------- mark-to-market equity ----------\n",
    "    eq_val = capital\n",
    "    for t,pos in positions.items():\n",
    "        if t in px_close.columns:\n",
    "            px_val = px_close.at[dt,t]\n",
    "            if pd.notna(px_val):\n",
    "                eq_val += (px_val - pos[\"entry\"]) / pos[\"entry\"] * pos[\"side\"] * pos[\"size\"]\n",
    "    equity_curve.append({\"date\": dt, \"equity\": eq_val})\n",
    "\n",
    "# ---------- Save ----------\n",
    "equity_df = pd.DataFrame(equity_curve).set_index(\"date\")\n",
    "equity_df.to_csv(BT_DIR/\"equity_curve.csv\")\n",
    "pd.DataFrame(trade_log).to_csv(BT_DIR/\"trade_log.csv\", index=False)\n",
    "\n",
    "equity_df.to_csv(BT_DIR/\"portfolio_value.csv\")\n",
    "equity_df[\"returns\"] = equity_df[\"equity\"].pct_change().fillna(0.0)\n",
    "equity_df[[\"returns\"]].to_csv(BT_DIR/\"daily_returns.csv\")\n",
    "\n",
    "print(f\"✅ Block 10C hoàn tất. Mode=Strict, Fee={FEE_RATE*100:.2f}%, MaxPos={MAX_POSITIONS}, TopK={TOPK_DAILY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5b470",
   "metadata": {},
   "source": [
    "**Block 11: Thống kê kết quả và vẽ biểu đồ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8de77d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Kết quả Test Set:\n",
      "Ngày bắt đầu               2025-02-06\n",
      "Ngày kết thúc              2025-10-02\n",
      "Giá trị cuối             1169993.2623\n",
      "ROI (%)                     16.929174\n",
      "Biến động (năm, %)          14.988755\n",
      "Sharpe                       1.668602\n",
      "Sortino                      2.103304\n",
      "MaxDrawdown (%)             -7.433818\n",
      "Tỷ lệ phiên thắng (%)            60.0\n",
      "Số ngày                           165\n",
      "dtype: object\n",
      "\n",
      "📊 Stress Test:\n",
      "Ngày bắt đầu                 2025-03-26\n",
      "Ngày kết thúc                2025-04-15\n",
      "Giá trị cuối             1008343.239927\n",
      "ROI (%)                       -0.302326\n",
      "Biến động (năm, %)            33.888843\n",
      "Sharpe                        -0.005699\n",
      "Sortino                       -0.008435\n",
      "MaxDrawdown (%)               -5.814073\n",
      "Tỷ lệ phiên thắng (%)         35.714286\n",
      "Số ngày                              14\n",
      "dtype: object\n",
      "\n",
      "✅ Block 11a-Test hoàn tất. Lưu kết quả tại ./backtest_best_test/\n"
     ]
    }
   ],
   "source": [
    "# Block 11a-Test — Hiệu suất & Stress Test (Test set, SL/TP cố định)\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "OUTPUT_DIR   = \"./backtest_test/\"\n",
    "BEST_DIR     = \"./backtest_best_test/\"\n",
    "os.makedirs(BEST_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    peak = series.cummax()\n",
    "    dd = (series / peak) - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def compute_stats(port_val: pd.Series, bench_val: pd.Series | None = None):\n",
    "    port_ret = port_val.pct_change().fillna(0.0)\n",
    "    stats = {\n",
    "        \"Ngày bắt đầu\": port_val.index.min().strftime(\"%Y-%m-%d\"),\n",
    "        \"Ngày kết thúc\": port_val.index.max().strftime(\"%Y-%m-%d\"),\n",
    "        \"Giá trị cuối\": float(port_val.iloc[-1]),\n",
    "        \"ROI (%)\": float((port_val.iloc[-1] / port_val.iloc[0] - 1.0) * 100),\n",
    "        \"Biến động (năm, %)\": float(port_ret.std() * np.sqrt(252) * 100),\n",
    "        \"Sharpe\": float((port_ret.mean() / port_ret.std()) * np.sqrt(252)) if port_ret.std() > 0 else 0,\n",
    "        \"Sortino\": float((port_ret.mean() / port_ret[port_ret < 0].std()) * np.sqrt(252)) if port_ret[port_ret < 0].std() > 0 else 0,\n",
    "        \"MaxDrawdown (%)\": float(max_drawdown(port_val) * 100),\n",
    "        \"Tỷ lệ phiên thắng (%)\": float((port_ret > 0).mean() * 100),\n",
    "        \"Số ngày\": int(len(port_ret))\n",
    "    }\n",
    "    if bench_val is not None and len(bench_val) > 1:\n",
    "        stats.update({\n",
    "            \"Giá trị cuối (VNINDEX)\": float(bench_val.iloc[-1]),\n",
    "            \"ROI VNINDEX (%)\": float((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100),\n",
    "            \"Chênh lệch so với VNINDEX (pp)\": stats[\"ROI (%)\"] - ((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100)\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "def plot_equity(port_val, bench_val, title, path):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(port_val, label=\"Chiến lược\", linewidth=1.6)\n",
    "    if bench_val is not None:\n",
    "        plt.plot(bench_val, label=\"VNINDEX\", linewidth=1.2)\n",
    "    plt.title(title); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "# ---------- 1. Load dữ liệu ----------\n",
    "eq_path = os.path.join(OUTPUT_DIR, \"equity_curve.csv\")\n",
    "if not os.path.exists(eq_path):\n",
    "    raise FileNotFoundError(\"Chưa có equity_curve.csv từ Block 10C\")\n",
    "\n",
    "port_val = pd.read_csv(eq_path, index_col=0, parse_dates=True)[\"equity\"].sort_index()\n",
    "daily_returns = port_val.pct_change().fillna(0.0)\n",
    "\n",
    "# benchmark từ df_backtest\n",
    "df_backtest_path = \"./backtest_ddpg/df_backtest.csv\"\n",
    "bench_val, bench_ret = None, None\n",
    "if os.path.exists(df_backtest_path):\n",
    "    df_backtest = pd.read_csv(df_backtest_path, parse_dates=[\"date\"])\n",
    "    df_backtest = df_backtest.groupby([\"date\",\"ticker\"],as_index=False).agg({\"close\":\"last\"})\n",
    "    px = df_backtest.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "    if \"VNINDEX\" in px.columns:\n",
    "        bench_val = px[\"VNINDEX\"].reindex(port_val.index).ffill().bfill()\n",
    "        bench_ret = bench_val.pct_change().fillna(0.0)\n",
    "\n",
    "# ---------- 2. Compute Stats ----------\n",
    "stats_test = compute_stats(port_val, bench_val)\n",
    "\n",
    "# ---------- 3. Plot cơ bản ----------\n",
    "plot_equity(port_val, bench_val, \"Equity Curve (Test set)\", os.path.join(BEST_DIR,\"equity.png\"))\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.hist(daily_returns.dropna(), bins=50, alpha=0.6, label=\"Chiến lược\")\n",
    "if bench_ret is not None:\n",
    "    plt.hist(bench_ret.dropna(), bins=50, alpha=0.6, label=\"VNINDEX\")\n",
    "plt.title(\"Histogram lợi nhuận ngày\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(BEST_DIR,\"hist.png\")); plt.close()\n",
    "\n",
    "# ---------- 4. Stress Test (ví dụ Trump 46%) ----------\n",
    "stress_start, stress_end = pd.Timestamp(\"2025-03-26\"), pd.Timestamp(\"2025-04-15\")\n",
    "sub_port = port_val.loc[stress_start:stress_end]\n",
    "sub_bench = bench_val.loc[stress_start:stress_end] if bench_val is not None else None\n",
    "stats_stress = {}\n",
    "if len(sub_port) > 1:\n",
    "    stats_stress = compute_stats(sub_port, sub_bench)\n",
    "    plot_equity(sub_port, sub_bench, \"Stress Test (Test set)\", os.path.join(BEST_DIR,\"equity_stress.png\"))\n",
    "\n",
    "# ---------- 5. Save ----------\n",
    "all_stats={\"TestSet\":stats_test}\n",
    "if stats_stress: all_stats[\"StressTest\"]=stats_stress\n",
    "pd.DataFrame(all_stats).T.to_csv(os.path.join(BEST_DIR,\"stats.csv\"))\n",
    "\n",
    "print(\"\\n📊 Kết quả Test Set:\"); print(pd.Series(stats_test))\n",
    "if stats_stress: \n",
    "    print(\"\\n📊 Stress Test:\"); print(pd.Series(stats_stress))\n",
    "print(f\"\\n✅ Block 11a-Test hoàn tất. Lưu kết quả tại {BEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3329dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Fetching VNINDEX from FiinQuantX...\n",
      "Fetching data, it may take a while. Please wait...\n",
      "📊 Stress Test Stats saved to backtest_best_test\\stats_stress.csv\n",
      "✅ Block 11b-Test FULL hoàn tất. Stats + Charts lưu tại: backtest_best_test\n"
     ]
    }
   ],
   "source": [
    "# Block 11b-Test — Full performance & diagnostics (Strategy vs VNINDEX benchmark)\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from FiinQuantX import FiinSession\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "# ---------- Paths ----------\n",
    "BACKTEST_TEST_DIR = Path(\"./backtest_test\")\n",
    "OUT_DIR           = Path(\"./backtest_best_test\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EQ_PATH = BACKTEST_TEST_DIR / \"equity_curve.csv\"\n",
    "if not EQ_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Không tìm thấy equity_curve.csv tại {EQ_PATH}. Chạy Block 10C trước.\")\n",
    "\n",
    "# ---------- Load portfolio equity ----------\n",
    "eq_df = pd.read_csv(EQ_PATH, parse_dates=[0], index_col=0).sort_index()\n",
    "port_val = eq_df[\"equity\"] if \"equity\" in eq_df.columns else eq_df.iloc[:,0]\n",
    "port_val = port_val.astype(float).replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "\n",
    "# ---------- Fetch VNINDEX benchmark ----------\n",
    "print(\"📈 Fetching VNINDEX from FiinQuantX...\")\n",
    "client = FiinSession(username=\"DSTC_18@fiinquant.vn\", password=\"Fiinquant0606\").login()\n",
    "bench = client.Fetch_Trading_Data(\n",
    "    realtime=False, tickers=\"VNINDEX\", fields=['close'],\n",
    "    adjusted=True, by=\"1d\", from_date=str(port_val.index.min().date())\n",
    ").get_data()\n",
    "bench[\"date\"] = pd.to_datetime(bench[\"timestamp\"])\n",
    "bench = bench.set_index(\"date\")[\"close\"].sort_index().reindex(port_val.index).ffill().bfill()\n",
    "\n",
    "INIT_CAP = float(port_val.iloc[0])\n",
    "bench_equity = (1.0 + bench.pct_change().fillna(0.0)).cumprod() * INIT_CAP\n",
    "\n",
    "# ---------- Stats helper ----------\n",
    "def max_drawdown(series):\n",
    "    peak = series.cummax()\n",
    "    return float(((series/peak)-1).min())\n",
    "\n",
    "def compute_stats(port_series, bench_series=None):\n",
    "    r = port_series.pct_change().dropna()\n",
    "    roi = (port_series.iloc[-1]/port_series.iloc[0]-1.0)\n",
    "    vol = r.std()*np.sqrt(252)\n",
    "    sharpe = (r.mean()/r.std())*np.sqrt(252) if r.std()>0 else 0.0\n",
    "    sortino = (r.mean()/r[r<0].std())*np.sqrt(252) if r[r<0].std()>0 else 0.0\n",
    "    mdd = max_drawdown(port_series)\n",
    "    win = (r>0).mean()\n",
    "    stats = {\n",
    "        \"Start\": str(port_series.index.min().date()),\n",
    "        \"End\": str(port_series.index.max().date()),\n",
    "        \"Initial\": float(port_series.iloc[0]),\n",
    "        \"Final\": float(port_series.iloc[-1]),\n",
    "        \"ROI\": roi, \"Vol(ann)\": vol, \"Sharpe\": sharpe,\n",
    "        \"Sortino\": sortino, \"MaxDD\": mdd, \"WinRate\": win\n",
    "    }\n",
    "    if bench_series is not None:\n",
    "        broi = (bench_series.iloc[-1]/bench_series.iloc[0]-1.0)\n",
    "        stats.update({\"BenchFinal\": float(bench_series.iloc[-1]), \"BenchROI\": broi, \"ExcessROI\": roi-broi})\n",
    "    return stats\n",
    "\n",
    "stats = compute_stats(port_val, bench_equity)\n",
    "pd.Series(stats).to_csv(OUT_DIR/\"stats_test.csv\")\n",
    "\n",
    "# ---------- Charts ----------\n",
    "# Equity curve\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(port_val, label=\"Strategy\", linewidth=2)\n",
    "plt.plot(bench_equity, label=\"VNINDEX\", linewidth=1.5)\n",
    "plt.title(\"Equity Curve — Strategy vs VNINDEX\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.savefig(OUT_DIR/\"equity_vs_benchmark.png\"); plt.close()\n",
    "\n",
    "# Drawdown\n",
    "dd = (port_val/port_val.cummax()-1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(dd, label=\"Drawdown\")\n",
    "plt.fill_between(dd.index, dd, 0, alpha=0.3)\n",
    "plt.title(\"Drawdown Curve\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig(OUT_DIR/\"drawdown.png\"); plt.close()\n",
    "\n",
    "# Histogram\n",
    "port_ret = port_val.pct_change().dropna()\n",
    "bench_ret = bench_equity.pct_change().dropna()\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.hist(port_ret, bins=50, alpha=0.6, label=\"Strategy\")\n",
    "plt.hist(bench_ret, bins=50, alpha=0.4, label=\"VNINDEX\")\n",
    "plt.title(\"Histogram daily returns\")\n",
    "plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.savefig(OUT_DIR/\"hist.png\"); plt.close()\n",
    "\n",
    "# Monthly heatmap\n",
    "monthly = port_ret.add(1).resample(\"M\").prod().sub(1)\n",
    "heat = monthly.to_frame(\"ret\")\n",
    "heat[\"Year\"] = heat.index.year; heat[\"Month\"] = heat.index.month\n",
    "hm = heat.pivot(index=\"Year\", columns=\"Month\", values=\"ret\").fillna(0)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(hm, annot=True, fmt=\".1%\", center=0, cmap=\"RdYlGn\")\n",
    "plt.title(\"Monthly Return Heatmap\")\n",
    "plt.savefig(OUT_DIR/\"heatmap.png\"); plt.close()\n",
    "\n",
    "# Rolling Sharpe (60d)\n",
    "roll_sharpe = port_ret.rolling(60).mean() / port_ret.rolling(60).std() * np.sqrt(252)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(roll_sharpe, label=\"Rolling Sharpe (60d)\")\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.title(\"Rolling Sharpe Ratio (60d window)\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "plt.savefig(OUT_DIR/\"rolling_sharpe.png\"); plt.close()\n",
    "\n",
    "# Rolling Beta (60d OLS vs VNINDEX)\n",
    "roll_beta = []\n",
    "for i in range(60, len(port_ret)):\n",
    "    y = port_ret.iloc[i-60:i].values\n",
    "    x = bench_ret.iloc[i-60:i].values\n",
    "    if np.std(x)>0:\n",
    "        model = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "        roll_beta.append((port_ret.index[i], model.params[1]))\n",
    "roll_beta = pd.DataFrame(roll_beta, columns=[\"date\",\"beta\"]).set_index(\"date\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(roll_beta[\"beta\"], label=\"Rolling Beta (60d)\")\n",
    "plt.axhline(1, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Rolling Beta vs VNINDEX\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "plt.savefig(OUT_DIR/\"rolling_beta.png\"); plt.close()\n",
    "\n",
    "# ---------- Stress Test (same window as 11a) ----------\n",
    "stress_start, stress_end = pd.Timestamp(\"2025-03-26\"), pd.Timestamp(\"2025-04-15\")\n",
    "sub_port = port_val.loc[stress_start:stress_end]\n",
    "sub_bench = bench_equity.loc[stress_start:stress_end]\n",
    "\n",
    "if len(sub_port) > 1:\n",
    "    stress_stats = compute_stats(sub_port, sub_bench)\n",
    "    pd.Series(stress_stats).to_csv(OUT_DIR/\"stats_stress.csv\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(sub_port, label=\"Strategy\", linewidth=2)\n",
    "    plt.plot(sub_bench, label=\"VNINDEX\", linewidth=1.5)\n",
    "    plt.title(\"Stress Test Equity Curve\")\n",
    "    plt.legend(); plt.grid(alpha=0.3)\n",
    "    plt.savefig(OUT_DIR/\"equity_stress.png\"); plt.close()\n",
    "    print(\"📊 Stress Test Stats saved to\", OUT_DIR/\"stats_stress.csv\")\n",
    "    \n",
    "\n",
    "print(\"✅ Block 11b-Test FULL hoàn tất. Stats + Charts lưu tại:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd5497",
   "metadata": {},
   "source": [
    "**Block 12A: gửi tín hiệu lên telegram với dữ liệu quá khứ**\n",
    "\n",
    "`Lưu ý` tuy quá khứ nhưng nhóm không để bị nhìn trước tương lai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d166464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 12a — Gửi báo cáo Telegram (Summary + Chart + CSV chi tiết tùy chọn)\n",
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== Đường dẫn ====\n",
    "BT_DIR = \"./backtest_test/\"\n",
    "EQ_PATH = os.path.join(BT_DIR, \"equity_curve.csv\")\n",
    "LOG_PATH = os.path.join(BT_DIR, \"trade_log.csv\")\n",
    "\n",
    "# ==== Load config ====\n",
    "with open(\"config.json\",\"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "TG_TOKEN     = cfg[\"telegram\"][\"bot_token\"]\n",
    "TG_CHAT_ID   = cfg[\"telegram\"][\"chat_id\"]\n",
    "TG_THREAD_ID = cfg[\"telegram\"][\"message_thread_id\"]\n",
    "\n",
    "# ==== Load dữ liệu backtest ====\n",
    "equity = pd.read_csv(EQ_PATH, parse_dates=[\"date\"], index_col=\"date\")[\"equity\"]\n",
    "trades = pd.read_csv(LOG_PATH, parse_dates=[\"date_entry\",\"date_exit\"])\n",
    "\n",
    "def send_daily_report(report_date: str | pd.Timestamp, details: bool=False, sleep_sec:int=2):\n",
    "    \"\"\"\n",
    "    Gửi báo cáo Telegram cho 1 ngày trong backtest.\n",
    "    - Summary text\n",
    "    - Equity chart\n",
    "    - Tùy chọn gửi CSV chi tiết (opened, closed, open_positions)\n",
    "    \"\"\"\n",
    "    report_date = pd.Timestamp(report_date).normalize()\n",
    "    if report_date not in equity.index:\n",
    "        print(f\"⚠️ {report_date.date()} không có trong equity index\")\n",
    "        return\n",
    "\n",
    "    # --- NAV ---\n",
    "    nav_today = equity.loc[:report_date].iloc[-1]\n",
    "\n",
    "    # --- Lọc giao dịch ---\n",
    "    closed_today = trades[trades[\"date_exit\"].dt.normalize() == report_date]\n",
    "    opened_today = trades[(trades[\"date_entry\"].dt.normalize() == report_date) & (trades[\"reason\"]==\"entry\")]\n",
    "\n",
    "    # --- Danh mục còn mở đến hôm nay (fix) ---\n",
    "    open_positions = trades[\n",
    "        (trades[\"date_entry\"].dt.normalize() <= report_date) &\n",
    "        ((trades[\"date_exit\"].isna()) | (trades[\"date_exit\"].dt.normalize() > report_date))\n",
    "    ].sort_values(\"date_entry\").drop_duplicates(subset=[\"ticker\"], keep=\"last\")\n",
    "\n",
    "    # --- % lãi/lỗ TB hôm nay ---\n",
    "    avg_pnl = None\n",
    "    if len(closed_today) > 0:\n",
    "        pnl_pct = ((closed_today[\"exit\"]/closed_today[\"entry\"] - 1) * closed_today[\"side\"]) * 100\n",
    "        avg_pnl = pnl_pct.mean()\n",
    "\n",
    "    # --- Danh sách ticker (giới hạn 5 để gọn) ---\n",
    "    def list_names(df, label):\n",
    "        if len(df) == 0:\n",
    "            return f\"{label}: Không có\"\n",
    "        names = df[\"ticker\"].astype(str).tolist()\n",
    "        if len(names) > 5:\n",
    "            return f\"{label}: {', '.join(names[:5])}... (+{len(names)-5} mã nữa)\"\n",
    "        return f\"{label}: {', '.join(names)}\"\n",
    "\n",
    "    opened_txt = list_names(opened_today, \"🟢 Mở\")\n",
    "    closed_txt = list_names(closed_today, \"🔴 Đóng\")\n",
    "\n",
    "    # --- Compose message ---\n",
    "    msg = f\"\"\"\n",
    "📅 {report_date.date()}\n",
    "\n",
    "💰 NAV: {nav_today:,.0f}\n",
    "\n",
    "📊 Đang mở: {len(open_positions)} mã\n",
    "{opened_txt}\n",
    "{closed_txt}\n",
    "{'' if avg_pnl is None else f'↳ Lãi/lỗ TB: {avg_pnl:.2f}%'}\n",
    "\"\"\".strip()\n",
    "\n",
    "    # --- Chart equity đến ngày đó ---\n",
    "    chart_path = os.path.join(BT_DIR, f\"equity_until_{report_date.date()}.png\")\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(equity.loc[:report_date], label=\"Chiến lược\")\n",
    "    plt.title(f\"Equity đến {report_date.date()}\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend()\n",
    "    plt.savefig(chart_path, dpi=150); plt.close()\n",
    "\n",
    "    # --- Gửi text ---\n",
    "    send_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendMessage\"\n",
    "    requests.post(send_url, data={\n",
    "        \"chat_id\": TG_CHAT_ID,\n",
    "        \"message_thread_id\": TG_THREAD_ID,\n",
    "        \"text\": msg\n",
    "    })\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # --- Gửi ảnh ---\n",
    "    photo_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendPhoto\"\n",
    "    with open(chart_path,\"rb\") as f:\n",
    "        requests.post(photo_url, data={\n",
    "            \"chat_id\": TG_CHAT_ID,\n",
    "            \"message_thread_id\": TG_THREAD_ID,\n",
    "            \"caption\": f\"Equity Curve đến {report_date.date()}\"\n",
    "        }, files={\"photo\":f})\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # --- Nếu bật details: gửi CSV chi tiết ---\n",
    "    if details:\n",
    "        csv_path = os.path.join(BT_DIR, f\"trade_log_detail_{report_date.date()}.csv\")\n",
    "        detail_df = pd.concat([opened_today, closed_today, open_positions])\n",
    "        detail_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        file_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendDocument\"\n",
    "        with open(csv_path,\"rb\") as f:\n",
    "            requests.post(file_url, data={\n",
    "                \"chat_id\": TG_CHAT_ID,\n",
    "                \"message_thread_id\": TG_THREAD_ID,\n",
    "                \"caption\": f\"Chi tiết giao dịch {report_date.date()}\"\n",
    "            }, files={\"document\":f})\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    print(f\"✅ Đã gửi báo cáo Telegram cho ngày {report_date.date()} (details={details})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a4522d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-26 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-27 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-28 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-31 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-01 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-02 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-03 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-04 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-08 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-09 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-10 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-11 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-14 (details=True)\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-15 (details=True)\n"
     ]
    }
   ],
   "source": [
    "# Replay báo cáo Telegram cho giai đoạn 26/3 → 15/4/2025\n",
    "start_date, end_date = pd.Timestamp(\"2025-03-26\"), pd.Timestamp(\"2025-04-15\")\n",
    "\n",
    "for dt in equity.loc[start_date:end_date].index:\n",
    "    send_daily_report(dt, details=True, sleep_sec=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
