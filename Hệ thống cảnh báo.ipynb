{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e1830dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login FiinQuant th√†nh c√¥ng\n",
      "‚ÑπÔ∏è Loaded existing runtime_state.json\n",
      "üéØ Block 1 ho√†n t·∫•t: Config & Login\n"
     ]
    }
   ],
   "source": [
    "# Block 1 ‚Äî Config & Login\n",
    "# =======================================\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "from FiinQuantX import FiinSession\n",
    "\n",
    "# -------------------\n",
    "# Step 1: T·∫°o config.json n·∫øu ch∆∞a c√≥\n",
    "# -------------------\n",
    "default_config = {\n",
    "    \"username\": \"DSTC_18@fiinquant.vn\",\n",
    "    \"password\": \"Fiinquant0606\",\n",
    "    \"batch_size\": 50,\n",
    "    \"freq_per_day\": 2,\n",
    "    \"execution_lag\": 2,\n",
    "    \"quota_monthly\": 480,\n",
    "    \"telegram_token\": \"\",\n",
    "    \"telegram_chat_id\": \"\",\n",
    "    \"google_sheet_id\": \"\",\n",
    "    \"google_service_json\": \"service_account.json\",\n",
    "    \"RSI_buy_threshold\": 35,\n",
    "    \"vol_spike_mult\": 1.5,\n",
    "    \"macd_diff_thresh\": 0,\n",
    "    \"rule_min_score\": 3,\n",
    "    \"alert_min_alloc\": 0.005,\n",
    "    \"cooldown_hours\": 24\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"config.json\"):\n",
    "    with open(\"config.json\",\"w\") as f:\n",
    "        json.dump(default_config, f, indent=2)\n",
    "    print(\"‚ö†Ô∏è config.json not found. Default file created.\")\n",
    "\n",
    "# -------------------\n",
    "# Step 2: Load config\n",
    "# -------------------\n",
    "with open(\"config.json\",\"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "username = config.get(\"username\")\n",
    "password = config.get(\"password\")\n",
    "\n",
    "# -------------------\n",
    "# Step 3: Login FiinQuant\n",
    "# -------------------\n",
    "try:\n",
    "    client = FiinSession(username, password).login()\n",
    "    print(\"‚úÖ Login FiinQuant th√†nh c√¥ng\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Login FiinQuant th·∫•t b·∫°i: {e}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 4: Init Telegram (n·∫øu c√≥)\n",
    "# -------------------\n",
    "bot, CHAT_ID = None, None\n",
    "if config.get(\"telegram_token\") and config.get(\"telegram_chat_id\"):\n",
    "    import telegram\n",
    "    try:\n",
    "        bot = telegram.Bot(token=config[\"telegram_token\"])\n",
    "        CHAT_ID = config[\"telegram_chat_id\"]\n",
    "        print(\"‚úÖ Telegram Bot ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Telegram init l·ªói: {e}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 5: Init Google Sheets (n·∫øu c√≥)\n",
    "# -------------------\n",
    "gs_client, sheet_id = None, None\n",
    "if config.get(\"google_sheet_id\") and os.path.exists(config.get(\"google_service_json\",\"\")):\n",
    "    import gspread\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    try:\n",
    "        scope = [\"https://spreadsheets.google.com/feeds\",\"https://www.googleapis.com/auth/drive\"]\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_name(config[\"google_service_json\"], scope)\n",
    "        gs_client = gspread.authorize(creds)\n",
    "        sheet_id = config[\"google_sheet_id\"]\n",
    "        print(\"‚úÖ Google Sheets client ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Google Sheets init l·ªói: {e}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 6: Init runtime_state.json\n",
    "# -------------------\n",
    "runtime_file = \"runtime_state.json\"\n",
    "if not os.path.exists(runtime_file):\n",
    "    runtime_state = {\n",
    "        \"api_used\": 0,\n",
    "        \"month\": datetime.now().strftime(\"%Y-%m\"),\n",
    "        \"last_universe\": None,\n",
    "        \"last_run_time\": None,\n",
    "        \"last_alerts\": {}\n",
    "    }\n",
    "    with open(runtime_file, \"w\") as f:\n",
    "        json.dump(runtime_state, f, indent=2)\n",
    "    print(\"‚úÖ runtime_state.json created\")\n",
    "else:\n",
    "    with open(runtime_file, \"r\") as f:\n",
    "        runtime_state = json.load(f)\n",
    "    print(\"‚ÑπÔ∏è Loaded existing runtime_state.json\")\n",
    "\n",
    "# -------------------\n",
    "# Done\n",
    "# -------------------\n",
    "print(\"üéØ Block 1 ho√†n t·∫•t: Config & Login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19969cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë tickers to√†n th·ªã tr∆∞·ªùng: 413\n",
      "Fetching data, it may take a while. Please wait...\n",
      "‚ö†Ô∏è L·ªói khi l·∫•y FA FUETPVND: 'FUETPVND'\n",
      "üíæ Saved universe/universe_list.csv with 200 tickers\n",
      "üìò Metadata saved: universe/universe_metadata.json\n",
      "‚úÖ Universe sample:\n",
      "    ticker  liquidity_20d  momentum_90d  momentum_252d  FA_flag  \\\n",
      "385    VIX   1.456158e+12      1.821318            0.0        1   \n",
      "50     CII   7.529223e+11      1.023096            0.0        1   \n",
      "311    SSI   2.146069e+12      0.688285            0.0        1   \n",
      "145    GEX   4.919533e+11      0.974061            0.0        1   \n",
      "395    VPB   1.533244e+12      0.638814            0.0        1   \n",
      "\n",
      "     liquidity_rank  momentum_rank     score  \n",
      "385        0.990291       0.997573  0.993932  \n",
      "50         0.970874       0.985437  0.978155  \n",
      "311        0.997573       0.949029  0.973301  \n",
      "145        0.946602       0.983010  0.964806  \n",
      "395        0.992718       0.927184  0.959951  \n"
     ]
    }
   ],
   "source": [
    "# Block 2 ‚Äî Universe Selection (Monthly job)\n",
    "# ================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Step 1: L·∫•y danh s√°ch tickers to√†n th·ªã tr∆∞·ªùng ----\n",
    "tickers_hose  = list(client.TickerList(ticker=\"VNINDEX\"))\n",
    "tickers_all = list(set(tickers_hose))\n",
    "print(f\"T·ªïng s·ªë tickers to√†n th·ªã tr∆∞·ªùng: {len(tickers_all)}\")\n",
    "\n",
    "# ---- Step 2: L·∫•y d·ªØ li·ªáu OHLCV 6 th√°ng g·∫ßn nh·∫•t ----\n",
    "ohlcv = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=tickers_all,\n",
    "    fields=[\"close\",\"volume\"],\n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2025-03-01\"   # ~6 th√°ng g·∫ßn ƒë√¢y\n",
    ").get_data()\n",
    "\n",
    "ohlcv[\"timestamp\"] = pd.to_datetime(ohlcv[\"timestamp\"])\n",
    "\n",
    "# ---- Step 3: T√≠nh liquidity (20d avg value traded) ----\n",
    "ohlcv[\"value\"] = ohlcv[\"close\"] * ohlcv[\"volume\"]\n",
    "liquidity_20d = ohlcv.groupby(\"ticker\")[\"value\"].rolling(20).mean().reset_index(level=0, drop=True)\n",
    "latest_liquidity = liquidity_20d.groupby(ohlcv[\"ticker\"]).last()\n",
    "\n",
    "# ---- Step 4: T√≠nh momentum (90d, 252d) ----\n",
    "def calc_momentum(series, window):\n",
    "    if len(series) < window: \n",
    "        return np.nan\n",
    "    return series.iloc[-1] / series.iloc[-window] - 1\n",
    "\n",
    "momentum_90d = ohlcv.groupby(\"ticker\")[\"close\"].apply(lambda x: calc_momentum(x, 90))\n",
    "momentum_252d = ohlcv.groupby(\"ticker\")[\"close\"].apply(lambda x: calc_momentum(x, 252))\n",
    "\n",
    "# ---- Step 5: L·∫•y d·ªØ li·ªáu FA (check FA flag) ----\n",
    "fa_flags = {}\n",
    "for t in tickers_all:\n",
    "    try:\n",
    "        fi_list = client.FundamentalAnalysis().get_ratios(\n",
    "            tickers=[t],\n",
    "            TimeFilter=\"Quarterly\",\n",
    "            LatestYear=2025,\n",
    "            NumberOfPeriod=8,\n",
    "            Consolidated=True\n",
    "        )\n",
    "        fa_flags[t] = 1 if fi_list else 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi l·∫•y FA {t}: {e}\")\n",
    "        fa_flags[t] = 0\n",
    "\n",
    "fa_flag_series = pd.Series(fa_flags, name=\"FA_flag\")\n",
    "\n",
    "# ---- Step 6: G·ªôp d·ªØ li·ªáu ----\n",
    "df_universe = pd.DataFrame({\n",
    "    \"ticker\": list(latest_liquidity.index),\n",
    "    \"liquidity_20d\": latest_liquidity.values,\n",
    "    \"momentum_90d\": momentum_90d.values,\n",
    "    \"momentum_252d\": momentum_252d.values\n",
    "}).set_index(\"ticker\")\n",
    "\n",
    "df_universe = df_universe.join(fa_flag_series, how=\"left\").fillna(0)\n",
    "df_universe = df_universe.reset_index()\n",
    "\n",
    "# ---- Step 7: Ranking & ch·ªçn Universe ----\n",
    "df_universe[\"liquidity_rank\"] = df_universe[\"liquidity_20d\"].rank(pct=True)\n",
    "df_universe[\"momentum_rank\"]  = df_universe[\"momentum_90d\"].rank(pct=True)\n",
    "\n",
    "# Composite score (simple average)\n",
    "df_universe[\"score\"] = (df_universe[\"liquidity_rank\"] + df_universe[\"momentum_rank\"]) / 2\n",
    "\n",
    "# Ch·ªçn top 200\n",
    "univ_final = df_universe.sort_values(\"score\", ascending=False).head(200)\n",
    "\n",
    "# ---- Step 8: Save results ----\n",
    "os.makedirs(\"universe\", exist_ok=True)\n",
    "\n",
    "csv_file = \"universe/universe_list.csv\"\n",
    "meta_file = \"universe/universe_metadata.json\"\n",
    "\n",
    "univ_final.to_csv(csv_file, index=False)\n",
    "print(f\"üíæ Saved {csv_file} with {len(univ_final)} tickers\")\n",
    "\n",
    "meta = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"universe_size\": len(univ_final),\n",
    "    \"selection_method\": \"liquidity + momentum + FA_flag\",\n",
    "}\n",
    "with open(meta_file, \"w\") as f:\n",
    "    json.dump(meta, f, indent=4)\n",
    "\n",
    "print(f\"üìò Metadata saved: {meta_file}\")\n",
    "print(\"‚úÖ Universe sample:\")\n",
    "print(univ_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcba7fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Logging in to FiinQuant...\n",
      "[init] Fiin client logged in -> client variable available\n",
      "[init] FiinIndicator available as fi\n",
      "[run] Loaded 200 tickers from universe\\universe_list.csv\n",
      "[backfill] requesting historical OHLCV for 200 tickers from 2025-03-21 to 2025-09-21\n",
      "Fetching data, it may take a while. Please wait...\n",
      "[backfill] fetched batch 50 -> rows 6242\n",
      "Fetching data, it may take a while. Please wait...\n",
      "[backfill] fetched batch 50 -> rows 6242\n",
      "Fetching data, it may take a while. Please wait...\n",
      "[backfill] fetched batch 50 -> rows 6250\n",
      "Fetching data, it may take a while. Please wait...\n",
      "[backfill] fetched batch 50 -> rows 6250\n",
      "[backfill] saved master parquet ./data\\ohlcv_master.parquet rows=24,984 api_calls=4 failed_tickers=0\n",
      "[run] Master parquet ready at: ./data\\ohlcv_master.parquet\n",
      "Global objects available: client (FiinSession), fi (FiinIndicator or None).\n"
     ]
    }
   ],
   "source": [
    "# Cell: Build / Backfill OHLCV master parquet (compatible with pipeline)\n",
    "# Place this cell BEFORE your main realtime pipeline cell (the 1000+ lines)\n",
    "# ---------------------------\n",
    "import os, json, time, math, traceback\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "\n",
    "# --- Config: ch·ªânh n·∫øu c·∫ßn ho·∫∑c t·∫°o config.json v·ªõi keys username/password ---\n",
    "CONFIG_PATH = \"config.json\"\n",
    "cfg = {}\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH,\"r\") as f:\n",
    "        try:\n",
    "            cfg = json.load(f)\n",
    "        except:\n",
    "            cfg = {}\n",
    "# If config.json missing or no creds, fill here (you can keep them in config.json instead)\n",
    "USERNAME = cfg.get(\"username\") or \"DSTC_18@fiinquant.vn\"    # <-- ho·∫∑c ƒë·ªÉ tr·ªëng v√† s·ª≠a\n",
    "PASSWORD = cfg.get(\"password\") or \"Fiinquant0606\"          # <-- ho·∫∑c ƒë·ªÉ tr·ªëng v√† s·ª≠a\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "MASTER_PARQUET = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "\n",
    "# Backfill params\n",
    "BACKFILL_MONTHS = int(cfg.get(\"backfill_months\", 6))   # 6 months default\n",
    "BATCH_SIZE = int(cfg.get(\"batch_size\", 50))\n",
    "FIELDS = cfg.get(\"fetch_fields\", ['open','high','low','close','volume'])  # th√™m fields n·∫øu mu·ªën\n",
    "FORCE_BACKFILL = bool(cfg.get(\"force_backfill\", True))  # n·∫øu True th√¨ t·∫£i l·∫°i d√π file ƒë√£ t·ªìn t·∫°i\n",
    "\n",
    "# runtime_state file used by pipeline\n",
    "RUNTIME_FILE = \"runtime_state.json\"\n",
    "if os.path.exists(RUNTIME_FILE):\n",
    "    with open(RUNTIME_FILE,\"r\") as f:\n",
    "        runtime_state = json.load(f)\n",
    "else:\n",
    "    runtime_state = {\"api_used\":0, \"month\": datetime.now().strftime(\"%Y-%m\"), \"last_universe\": None, \"last_run_time\": None, \"last_alerts\": {}}\n",
    "    with open(RUNTIME_FILE,\"w\") as f:\n",
    "        json.dump(runtime_state, f, indent=2)\n",
    "\n",
    "def save_runtime_state():\n",
    "    runtime_state[\"last_run_time\"] = datetime.now().isoformat()\n",
    "    with open(RUNTIME_FILE,\"w\") as f:\n",
    "        json.dump(runtime_state, f, indent=2)\n",
    "\n",
    "# --- Import FiinQuantX and login ---\n",
    "try:\n",
    "    from FiinQuantX import FiinSession\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Kh√¥ng t√¨m th·∫•y FiinQuantX. C√†i ƒë·∫∑t th∆∞ vi·ªán ho·∫∑c ƒë·∫∑t ƒë√∫ng PYTHONPATH. L·ªói: \" + str(e))\n",
    "\n",
    "print(\"[init] Logging in to FiinQuant...\")\n",
    "client = None\n",
    "try:\n",
    "    client = FiinSession(username=USERNAME, password=PASSWORD).login()\n",
    "    print(\"[init] Fiin client logged in -> client variable available\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Fiin login failed: \" + str(e))\n",
    "\n",
    "# make indicator helper available to pipeline\n",
    "try:\n",
    "    fi = client.FiinIndicator()\n",
    "    print(\"[init] FiinIndicator available as fi\")\n",
    "except Exception:\n",
    "    fi = None\n",
    "    print(\"[init] Warning: client.FiinIndicator() not available (fi=None)\")\n",
    "\n",
    "# --- helper: chunk iterator ---\n",
    "def chunked_iterable(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(islice(it, size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "# --- helper: safe fetch per batch with retry ---\n",
    "def fetch_batch(tickers_batch, from_date, to_date=None, retries=3, wait=2):\n",
    "    \"\"\"Return DataFrame or raise.\"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            # Use Fetch_Trading_Data with realtime=False as in your examples\n",
    "            params = {\n",
    "                \"realtime\": False,\n",
    "                \"tickers\": tickers_batch,\n",
    "                \"fields\": FIELDS,\n",
    "                \"adjusted\": True,\n",
    "                \"by\": \"1d\",\n",
    "                \"from_date\": from_date\n",
    "            }\n",
    "            if to_date:\n",
    "                params[\"to_date\"] = to_date\n",
    "            res = client.Fetch_Trading_Data(**params).get_data()\n",
    "            # Expect DataFrame-like\n",
    "            if isinstance(res, pd.DataFrame):\n",
    "                return res\n",
    "            else:\n",
    "                # try to coerce to DataFrame\n",
    "                return pd.DataFrame(res)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"[fetch_batch] attempt {attempt}/{retries} failed for batch size {len(tickers_batch)}: {e}\")\n",
    "            time.sleep(wait * attempt)\n",
    "    raise RuntimeError(f\"All retries failed for batch: {tickers_batch[:5]}...\")\n",
    "\n",
    "# --- main backfill routine ---\n",
    "def build_master_ohlcv(tickers=None, months=BACKFILL_MONTHS, batch_size=BATCH_SIZE, force=FORCE_BACKFILL):\n",
    "    \"\"\"\n",
    "    If MASTER_PARQUET exists and not force -> skip.\n",
    "    Otherwise fetch historical data for tickers (chunked) from (today - months) to today,\n",
    "    save to MASTER_PARQUET (parquet).\n",
    "    Returns path or None.\n",
    "    \"\"\"\n",
    "    if os.path.exists(MASTER_PARQUET) and (not force):\n",
    "        print(f\"[run] {MASTER_PARQUET} exists. Set force=True to overwrite.\")\n",
    "        return MASTER_PARQUET\n",
    "\n",
    "    # If tickers not provided, try to read universe file (universe/universe_list.csv), else default to VN30 members\n",
    "    if tickers is None:\n",
    "        univ_path = os.path.join(\"universe\",\"universe_list.csv\")\n",
    "        if os.path.exists(univ_path):\n",
    "            try:\n",
    "                tt = pd.read_csv(univ_path)[\"ticker\"].tolist()\n",
    "                tickers = [t for t in tt if isinstance(t, str)]\n",
    "                print(f\"[run] Loaded {len(tickers)} tickers from {univ_path}\")\n",
    "            except Exception as e:\n",
    "                print(\"[run] cannot read universe file:\", e)\n",
    "                tickers = None\n",
    "    if tickers is None:\n",
    "        # fallback: try to request VN30 or VNINDEX tickers (example usage)\n",
    "        try:\n",
    "            print(\"[run] universe not provided -> trying client.TickerList(ticker='VNINDEX')\")\n",
    "            tlist = client.TickerList(ticker=\"VNINDEX\")\n",
    "            # TickerList may return list-like or objects; try to coerce\n",
    "            if isinstance(tlist, (list,tuple,set)):\n",
    "                tickers = list(tlist)\n",
    "            else:\n",
    "                # If object, try iterate\n",
    "                tickers = list(tlist)\n",
    "            print(f\"[run] got {len(tickers)} tickers from VNINDEX\")\n",
    "        except Exception as e:\n",
    "            print(\"[run] fallback failed, set tickers manually or create universe/universe_list.csv. Error:\", e)\n",
    "            raise RuntimeError(\"No tickers available to backfill\")\n",
    "\n",
    "    # compute from_date\n",
    "    to_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    from_date_dt = datetime.now() - pd.DateOffset(months=months)\n",
    "    from_date = from_date_dt.strftime(\"%Y-%m-%d\")\n",
    "    print(f\"[backfill] requesting historical OHLCV for {len(tickers)} tickers from {from_date} to {to_date}\")\n",
    "\n",
    "    all_dfs = []\n",
    "    failed = []\n",
    "    calls = 0\n",
    "\n",
    "    for batch in chunked_iterable(tickers, batch_size):\n",
    "        try:\n",
    "            df = fetch_batch(batch, from_date=from_date, to_date=to_date)\n",
    "            calls += 1\n",
    "            # normalize column names if needed\n",
    "            if df is None or df.empty:\n",
    "                print(f\"[backfill] batch returned empty for {len(batch)} tickers\")\n",
    "                continue\n",
    "            # Ensure ticker column exists\n",
    "            if \"ticker\" not in df.columns and \"ticker_code\" in df.columns:\n",
    "                df = df.rename(columns={\"ticker_code\":\"ticker\"})\n",
    "            # Convert timestamp column name variations to 'timestamp'\n",
    "            if \"timestamp\" in df.columns:\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "            elif \"date\" in df.columns:\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"date\"])\n",
    "            elif \"trade_date\" in df.columns:\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"trade_date\"])\n",
    "            else:\n",
    "                # try to infer index datetime\n",
    "                try:\n",
    "                    df = df.reset_index()\n",
    "                    if \"index\" in df.columns:\n",
    "                        df = df.rename(columns={\"index\":\"timestamp\"})\n",
    "                        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "                except:\n",
    "                    pass\n",
    "            # ensure ticker column present by checking unique tickers in batch\n",
    "            if \"ticker\" not in df.columns:\n",
    "                # try to add ticker column if the API returned single-ticker DF\n",
    "                if len(batch) == 1:\n",
    "                    df[\"ticker\"] = batch[0]\n",
    "                else:\n",
    "                    # attempt to detect ticker column name\n",
    "                    for cand in [\"symbol\",\"code\",\"ticker_code\"]:\n",
    "                        if cand in df.columns:\n",
    "                            df = df.rename(columns={cand:\"ticker\"})\n",
    "                            break\n",
    "            # keep minimal columns and cast\n",
    "            keep_cols = [c for c in [\"timestamp\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"volume\"] if c in df.columns]\n",
    "            df = df[keep_cols].copy()\n",
    "            all_dfs.append(df)\n",
    "            print(f\"[backfill] fetched batch {len(batch)} -> rows {len(df)}\")\n",
    "            # update runtime_state api count\n",
    "            runtime_state[\"api_used\"] = runtime_state.get(\"api_used\",0) + 1\n",
    "            save_runtime_state()\n",
    "            time.sleep(0.5)  # small pause to be gentle\n",
    "        except Exception as e:\n",
    "            print(f\"[backfill] batch failed: {e}\")\n",
    "            failed.extend(batch)\n",
    "            # don't stop; continue with next batch\n",
    "            time.sleep(1)\n",
    "\n",
    "    # concat\n",
    "    if len(all_dfs) == 0:\n",
    "        print(\"[backfill] no data fetched.\")\n",
    "        return None\n",
    "    df_master = pd.concat(all_dfs, ignore_index=True)\n",
    "    # drop duplicate (timestamp, ticker)\n",
    "    if \"timestamp\" in df_master.columns and \"ticker\" in df_master.columns:\n",
    "        df_master = df_master.drop_duplicates(subset=[\"timestamp\",\"ticker\"])\n",
    "    # sort\n",
    "    if \"timestamp\" in df_master.columns:\n",
    "        df_master = df_master.sort_values([\"ticker\",\"timestamp\"]).reset_index(drop=True)\n",
    "    # save parquet\n",
    "    df_master.to_parquet(MASTER_PARQUET, index=False)\n",
    "    print(f\"[backfill] saved master parquet {MASTER_PARQUET} rows={len(df_master):,} api_calls={calls} failed_tickers={len(failed)}\")\n",
    "    if failed:\n",
    "        print(\"[backfill] failed tickers sample:\", failed[:50])\n",
    "    return MASTER_PARQUET\n",
    "\n",
    "# --- Run backfill if needed (auto) ---\n",
    "if (not os.path.exists(MASTER_PARQUET)) or FORCE_BACKFILL:\n",
    "    try:\n",
    "        path = build_master_ohlcv(months=BACKFILL_MONTHS, batch_size=BATCH_SIZE, force=FORCE_BACKFILL)\n",
    "        if path:\n",
    "            print(\"[run] Master parquet ready at:\", path)\n",
    "    except Exception as e:\n",
    "        print(\"[run] backfill error:\", e)\n",
    "else:\n",
    "    print(\"[run] Master parquet already present at\", MASTER_PARQUET, \"- skipping backfill (set force=True to overwrite)\")\n",
    "\n",
    "# Export globals for downstream pipeline cells\n",
    "# client and fi are defined above; ensure they are in global namespace for other cells to import/use.\n",
    "print(\"Global objects available: client (FiinSession), fi (FiinIndicator or None).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91de6934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Logging into FiinQuant...\n",
      "[init] Fiin client logged in. FiinIndicator ok? True\n",
      "[init] Google Sheets ready: DSTRound3\n"
     ]
    }
   ],
   "source": [
    "# realtime_pipeline_hybrid.py\n",
    "# Full pipeline: Block1..Block9 + stress-replay + monitoring\n",
    "# Paste this entire file into a notebook cell or .py and run.\n",
    "# WARNING: This script uses network APIs (FiinQuant, Telegram, Google Sheets).\n",
    "# Make sure credentials files exist and environment is configured.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "import csv\n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "import threading\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Try import torch (optional)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    TORCH = True\n",
    "except Exception:\n",
    "    TORCH = False\n",
    "\n",
    "# FiinQuantX client (user must have installed this lib)\n",
    "from FiinQuantX import FiinSession, RealTimeData\n",
    "\n",
    "# Google Sheets\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG (edit if needed)\n",
    "# ---------------------------\n",
    "# Credentials provided by user (you gave these earlier)\n",
    "FQ_USERNAME = \"DSTC_18@fiinquant.vn\"\n",
    "FQ_PASSWORD = \"Fiinquant0606\"\n",
    "\n",
    "# Google Sheets service account JSON (you said it's dstround3.json)\n",
    "GS_SERVICE_JSON = \"dstround3.json\"\n",
    "GS_SHEET_ID = \"17FRrF63TFE3bmAseoV4vQK5EA9nYaTaRRnLyd1F8MGU\"\n",
    "GS_WORKSHEET = \"DSTRound3\"\n",
    "\n",
    "# Telegram bot config (use supergroup chat id that worked for you)\n",
    "TG_TOKEN = \"8454050043:AAG_quR7eSALqh9WVRvx6DRZVxtRde_OpFQ\"\n",
    "TG_CHAT_ID = \"-1002692813170\"   # you confirmed this worked\n",
    "TG_THREAD_ID = 2                # thread id (you asked to set 2); note: only used if available\n",
    "\n",
    "# Pipeline params (tune)\n",
    "BATCH_SIZE = 50                # max tickers per realtime stream\n",
    "SNAPSHOT_SECONDS = 60          # total snapshot time (split across batches)\n",
    "RSI_TH = 35\n",
    "VOL_SPIKE_MULT = 1.5\n",
    "RULE_MIN_SCORE = 3\n",
    "MIN_ALLOC = 0.005              # 0.5% min allocation to alert\n",
    "COOLDOWN_HOURS = 24\n",
    "EXEC_LAG = 2                   # execution lag T+2\n",
    "TOPK = 10\n",
    "STATE_LKBK = 10\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"./data\"\n",
    "FEATURE_DIR = \"./features\"\n",
    "SIGNAL_DIR = \"./signals\"\n",
    "ALLOC_DIR = \"./allocations\"\n",
    "ALERT_DIR = \"./alerts\"\n",
    "MONITOR_DIR = \"./monitor\"\n",
    "MODEL_DIR = \"./models\"\n",
    "TENSOR_DIR = \"./tensors\"\n",
    "UNIVERSE_DIR = \"./universe\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [DATA_DIR, FEATURE_DIR, SIGNAL_DIR, ALLOC_DIR, ALERT_DIR, MONITOR_DIR, MODEL_DIR, TENSOR_DIR, UNIVERSE_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "RUNTIME_FILE = \"runtime_state.json\"\n",
    "if os.path.exists(RUNTIME_FILE):\n",
    "    with open(RUNTIME_FILE,\"r\") as f:\n",
    "        runtime_state = json.load(f)\n",
    "else:\n",
    "    runtime_state = {\"api_used\":0, \"month\": datetime.now().strftime(\"%Y-%m\"), \"last_run_time\": None, \"last_alerts\": {}}\n",
    "    with open(RUNTIME_FILE,\"w\") as f:\n",
    "        json.dump(runtime_state, f, indent=2)\n",
    "\n",
    "def save_runtime_state():\n",
    "    runtime_state[\"last_run_time\"] = datetime.now().isoformat()\n",
    "    with open(RUNTIME_FILE,\"w\") as f:\n",
    "        json.dump(runtime_state, f, indent=2)\n",
    "\n",
    "# ---------------------------\n",
    "# INIT: clients\n",
    "# ---------------------------\n",
    "print(\"[init] Logging into FiinQuant...\")\n",
    "client = None\n",
    "client_fq = None\n",
    "fi = None\n",
    "try:\n",
    "    client = FiinSession(username=FQ_USERNAME, password=FQ_PASSWORD).login()\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"FiinSession.login() returned None\")\n",
    "    client_fq = client\n",
    "    # Correct way to get indicator helpers (client-provided)\n",
    "    try:\n",
    "        fi = client_fq.FiinIndicator()\n",
    "    except Exception:\n",
    "        # some older libs call FiinIndicator differently; try module-level fallback\n",
    "        try:\n",
    "            from FiinQuantX import FiinIndicator as FI_CLASS\n",
    "            fi = FI_CLASS()\n",
    "        except Exception:\n",
    "            fi = None\n",
    "    print(\"[init] Fiin client logged in. FiinIndicator ok?\", fi is not None)\n",
    "except Exception as e:\n",
    "    print(\"[init] Fiin login failed:\", e)\n",
    "    client = None\n",
    "    client_fq = None\n",
    "    fi = None\n",
    "\n",
    "# Google Sheets init\n",
    "gs_client = None\n",
    "gs_spread = None\n",
    "gs_ws = None\n",
    "if not os.path.exists(GS_SERVICE_JSON):\n",
    "    print(f\"[init] Google service JSON missing: {GS_SERVICE_JSON} ‚Äî skip gsheet init\")\n",
    "else:\n",
    "    try:\n",
    "        gs_creds = Credentials.from_service_account_file(GS_SERVICE_JSON,\n",
    "            scopes=[\"https://www.googleapis.com/auth/spreadsheets\",\"https://www.googleapis.com/auth/drive\"])\n",
    "        gs_client = gspread.authorize(gs_creds)\n",
    "        gs_spread = gs_client.open_by_key(GS_SHEET_ID)\n",
    "        try:\n",
    "            gs_ws = gs_spread.worksheet(GS_WORKSHEET)\n",
    "        except Exception:\n",
    "            # create worksheet if not present\n",
    "            gs_ws = gs_spread.add_worksheet(title=GS_WORKSHEET, rows=\"1000\", cols=\"20\")\n",
    "        print(\"[init] Google Sheets ready:\", gs_spread.title)\n",
    "    except Exception as e:\n",
    "        print(\"[init] Google Sheets init error:\", e)\n",
    "        gs_client = None\n",
    "        gs_spread = None\n",
    "        gs_ws = None\n",
    "\n",
    "# ---------------------------\n",
    "# TELEGRAM helpers\n",
    "# ---------------------------\n",
    "def send_telegram_text(msg: str, thread_id: Optional[int] = None) -> bool:\n",
    "    url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendMessage\"\n",
    "    payload = {\"chat_id\": TG_CHAT_ID, \"text\": msg, \"parse_mode\": \"Markdown\"}\n",
    "    # optionally include thread id if you are certain the bot has permissions and the chat is forum\n",
    "    if thread_id is not None:\n",
    "        payload[\"message_thread_id\"] = int(thread_id)\n",
    "    try:\n",
    "        r = requests.post(url, json=payload, timeout=12)\n",
    "        if r.status_code != 200:\n",
    "            # retry without parse_mode\n",
    "            payload2 = {\"chat_id\": TG_CHAT_ID, \"text\": msg}\n",
    "            r2 = requests.post(url, json=payload2, timeout=12)\n",
    "            if r2.status_code != 200:\n",
    "                print(\"[tg] send failed:\", r.status_code, r.text)\n",
    "                return False\n",
    "            return True\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[tg] exception send:\", e)\n",
    "        return False\n",
    "\n",
    "def send_telegram_file(path: str, caption: str = \"\") -> bool:\n",
    "    url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendDocument\"\n",
    "    data = {\"chat_id\": TG_CHAT_ID, \"caption\": caption}\n",
    "    try:\n",
    "        with open(path,\"rb\") as fh:\n",
    "            files = {\"document\": fh}\n",
    "            r = requests.post(url, data=data, files=files, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            print(\"[tg] sendDocument failed:\", r.status_code, r.text)\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[tg] sendDocument exception:\", e)\n",
    "        return False\n",
    "\n",
    "# ---------------------------\n",
    "# Google Sheets helpers\n",
    "# ---------------------------\n",
    "def append_to_gsheet(df: pd.DataFrame, worksheet_name: Optional[str] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Append dataframe to worksheet. If worksheet_name provided, attempt to open/create that worksheet.\n",
    "    Otherwise append to default gs_ws.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return True\n",
    "    try:\n",
    "        if gs_client is None or gs_spread is None:\n",
    "            print(\"[gs] gsheet client not initialized\")\n",
    "            return False\n",
    "        ws = gs_ws\n",
    "        if worksheet_name:\n",
    "            try:\n",
    "                ws = gs_spread.worksheet(worksheet_name)\n",
    "            except Exception:\n",
    "                ws = gs_spread.add_worksheet(title=worksheet_name, rows=str(len(df)+10), cols=str(len(df.columns)+5))\n",
    "        # convert to list of lists\n",
    "        rows = df.values.tolist()\n",
    "        ws.append_rows(rows, value_input_option=\"RAW\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[gs] append error:\", e)\n",
    "        return False\n",
    "\n",
    "def push_ohlcv_master_to_gsheet(master_parquet_path: str, worksheet_name: str = \"OHLCV_Master\") -> bool:\n",
    "    \"\"\"\n",
    "    Push the master parquet (subset / last N rows) to Google Sheet.\n",
    "    For large datasets don't push everything - push last 20000 rows or last 6 months.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if gs_client is None:\n",
    "            print(\"[gs] gsheet client not initialized\")\n",
    "            return False\n",
    "        df = pd.read_parquet(master_parquet_path)\n",
    "        if df.empty:\n",
    "            return True\n",
    "        # push only last 20000 rows (safe)\n",
    "        to_push = df.tail(20000).copy()\n",
    "        # convert timestamp to string\n",
    "        if \"timestamp\" in to_push.columns:\n",
    "            to_push[\"timestamp\"] = to_push[\"timestamp\"].astype(str)\n",
    "        # push into worksheet (overwrite if exists by creating new)\n",
    "        try:\n",
    "            ws = gs_spread.worksheet(worksheet_name)\n",
    "            # clear and write header+data\n",
    "            ws.clear()\n",
    "        except Exception:\n",
    "            ws = gs_spread.add_worksheet(title=worksheet_name, rows=str(len(to_push)+20), cols=str(len(to_push.columns)+5))\n",
    "        set_with_dataframe(ws, to_push, include_index=False, include_column_header=True)\n",
    "        print(\"[gs] pushed ohlcv master to sheet:\", worksheet_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[gs] push ohlcv error:\", e)\n",
    "        return False\n",
    "\n",
    "def push_csv_to_gsheet(csv_path: str, worksheet_name_prefix: str):\n",
    "    \"\"\"\n",
    "    Push a small csv (signals/features) to a worksheet named worksheet_name_prefix + timestamp.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(csv_path):\n",
    "            return False\n",
    "        df = pd.read_csv(csv_path)\n",
    "        sheet_name = f\"{worksheet_name_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        return append_to_gsheet(df, worksheet_name=sheet_name)\n",
    "    except Exception as e:\n",
    "        print(\"[gs] push csv error:\", e)\n",
    "        return False\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def gfloat(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "# ---------------------------\n",
    "# Block 3: fetch_snapshot (realtime short stream, batch)\n",
    "# ---------------------------\n",
    "def fetch_snapshot(universe: List[str], seconds_snapshot: int = SNAPSHOT_SECONDS) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Create short streaming snapshot across batches to collect the latest ticks/EOD data.\n",
    "    Returns path to incremental CSV created (or None if nothing collected).\n",
    "    \"\"\"\n",
    "    ts_tag = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_csv = os.path.join(DATA_DIR, f\"ohlcv_incremental_{ts_tag}.csv\")\n",
    "    collected = 0\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def on_event(data: RealTimeData):\n",
    "        nonlocal collected\n",
    "        try:\n",
    "            df = data.to_dataFrame()\n",
    "            if df is None or df.empty:\n",
    "                return\n",
    "            if \"timestamp\" in df.columns:\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "            # append to file\n",
    "            with lock:\n",
    "                df.to_csv(out_csv, mode=\"a\", header=not os.path.exists(out_csv), index=False)\n",
    "                collected += len(df)\n",
    "                print(f\"[fetch] saved {len(df)} rows (total {collected})\")\n",
    "        except Exception as e:\n",
    "            print(\"[fetch] callback error:\", e)\n",
    "\n",
    "    # If small universe, open one stream\n",
    "    if len(universe) == 0:\n",
    "        print(\"[fetch] empty universe\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if len(universe) <= BATCH_SIZE:\n",
    "            ev = client_fq.Trading_Data_Stream(tickers=universe, callback=on_event)\n",
    "            ev.start()\n",
    "            # increment api_used for this stream\n",
    "            runtime_state[\"api_used\"] = runtime_state.get(\"api_used\",0) + 1\n",
    "            save_runtime_state()\n",
    "            time.sleep(seconds_snapshot)\n",
    "            try: ev.stop()\n",
    "            except: pass\n",
    "        else:\n",
    "            # split into sequential batches to avoid many simultaneous sockets\n",
    "            n_batches = math.ceil(len(universe)/BATCH_SIZE)\n",
    "            per_batch_seconds = max(1, int(seconds_snapshot / n_batches))\n",
    "            for i in range(0, len(universe), BATCH_SIZE):\n",
    "                batch = universe[i:i+BATCH_SIZE]\n",
    "                print(f\"[fetch] [Batch {i//BATCH_SIZE+1}/{n_batches}] Fetching {len(batch)} tickers...\")\n",
    "                ev = client_fq.Trading_Data_Stream(tickers=batch, callback=on_event)\n",
    "                ev.start()\n",
    "                runtime_state[\"api_used\"] = runtime_state.get(\"api_used\",0) + 1\n",
    "                save_runtime_state()\n",
    "                time.sleep(per_batch_seconds)\n",
    "                try: ev.stop()\n",
    "                except: pass\n",
    "    except Exception as e:\n",
    "        print(\"[fetch] stream start error:\", e)\n",
    "\n",
    "    # After stream, if out_csv exists -> append to master parquet\n",
    "    if os.path.exists(out_csv):\n",
    "        try:\n",
    "            df_new = pd.read_csv(out_csv)\n",
    "            if \"timestamp\" in df_new.columns:\n",
    "                df_new[\"timestamp\"] = pd.to_datetime(df_new[\"timestamp\"])\n",
    "            master = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "            if os.path.exists(master):\n",
    "                df_master = pd.read_parquet(master)\n",
    "                combined = pd.concat([df_master, df_new], ignore_index=True)\n",
    "                combined = combined.drop_duplicates(subset=[\"timestamp\",\"ticker\"])\n",
    "                combined.to_parquet(master, index=False)\n",
    "            else:\n",
    "                df_new.to_parquet(master, index=False)\n",
    "            print(f\"[fetch] appended incremental to master parquet; rows new={len(df_new)}\")\n",
    "        except Exception as e:\n",
    "            print(\"[fetch] append to master error:\", e)\n",
    "\n",
    "    print(f\"[fetch] snapshot complete rows={collected}\")\n",
    "    save_runtime_state()\n",
    "    return out_csv if collected>0 else None\n",
    "\n",
    "# ---------------------------\n",
    "# Block 4: Feature engineering helpers\n",
    "# ---------------------------\n",
    "def build_features_from_incr(incr_csv: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Build features timeseries & today's snapshot from incremental CSV (recent ticks).\n",
    "    \"\"\"\n",
    "    if incr_csv is None or not os.path.exists(incr_csv):\n",
    "        print(\"[features] incremental CSV missing\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(incr_csv)\n",
    "        if df.empty:\n",
    "            print(\"[features] incremental empty\")\n",
    "            return None\n",
    "        # ensure timestamp\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df = df.sort_values([\"ticker\",\"timestamp\"])\n",
    "        frames=[]\n",
    "        # use fi from client_fq (preferred) or local fi\n",
    "        fi_local = None\n",
    "        try:\n",
    "            fi_local = client_fq.FiinIndicator()\n",
    "        except Exception:\n",
    "            fi_local = fi\n",
    "        for tk, g in df.groupby(\"ticker\"):\n",
    "            gg = g.sort_values(\"timestamp\").copy()\n",
    "            try:\n",
    "                # compute indicators using fi_local if present else fallbacks\n",
    "                if fi_local is not None:\n",
    "                    gg[\"ema5\"] = fi_local.ema(gg[\"close\"], window=5)\n",
    "                    gg[\"ema20\"] = fi_local.ema(gg[\"close\"], window=20)\n",
    "                    gg[\"ema50\"] = fi_local.ema(gg[\"close\"], window=50)\n",
    "                    gg[\"rsi14\"] = fi_local.rsi(gg[\"close\"], window=14)\n",
    "                    gg[\"macd\"] = fi_local.macd(gg[\"close\"], window_fast=12, window_slow=26)\n",
    "                    gg[\"macd_signal\"] = fi_local.macd_signal(gg[\"close\"], window_fast=12, window_slow=26, window_sign=9)\n",
    "                    gg[\"macd_diff\"] = gg[\"macd\"] - gg[\"macd_signal\"]\n",
    "                    gg[\"boll_up\"] = fi_local.bollinger_hband(gg[\"close\"], window=20, window_dev=2)\n",
    "                    gg[\"boll_dn\"] = fi_local.bollinger_lband(gg[\"close\"], window=20, window_dev=2)\n",
    "                    gg[\"atr14\"] = fi_local.atr(gg[\"high\"], gg[\"low\"], gg[\"close\"], window=14)\n",
    "                    gg[\"obv\"] = fi_local.obv(gg[\"close\"], gg[\"volume\"])\n",
    "                    gg[\"vwap\"] = fi_local.vwap(gg[\"high\"], gg[\"low\"], gg[\"close\"], gg[\"volume\"], window=20)\n",
    "                    gg[\"vol_z\"] = (gg[\"volume\"] - gg[\"volume\"].rolling(20).mean()) / gg[\"volume\"].rolling(20).std()\n",
    "                else:\n",
    "                    # fallback simple computations\n",
    "                    gg[\"ema5\"] = gg[\"close\"].ewm(span=5).mean()\n",
    "                    gg[\"ema20\"] = gg[\"close\"].ewm(span=20).mean()\n",
    "                    gg[\"rsi14\"] = np.nan\n",
    "                    gg[\"macd\"] = gg[\"close\"].ewm(span=12).mean() - gg[\"close\"].ewm(span=26).mean()\n",
    "                    gg[\"macd_signal\"] = gg[\"macd\"].ewm(span=9).mean()\n",
    "                    gg[\"macd_diff\"] = gg[\"macd\"] - gg[\"macd_signal\"]\n",
    "                    gg[\"boll_up\"] = gg[\"close\"].rolling(20).mean() + 2 * gg[\"close\"].rolling(20).std()\n",
    "                    gg[\"boll_dn\"] = gg[\"close\"].rolling(20).mean() - 2 * gg[\"close\"].rolling(20).std()\n",
    "                    gg[\"vol_z\"] = (gg[\"volume\"] - gg[\"volume\"].rolling(20).mean()) / gg[\"volume\"].rolling(20).std()\n",
    "            except Exception as e:\n",
    "                print(f\"[features] TA error {tk}: {e}\")\n",
    "            frames.append(gg)\n",
    "        df_feat = pd.concat(frames, ignore_index=True)\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        timeseries_path = os.path.join(FEATURE_DIR, f\"features_timeseries_{ts}.parquet\")\n",
    "        df_feat.to_parquet(timeseries_path, index=False)\n",
    "        snapshot = df_feat.groupby(\"ticker\", as_index=False).last()\n",
    "        snap_path = os.path.join(FEATURE_DIR, f\"features_today_{ts}.csv\")\n",
    "        snapshot.to_csv(snap_path, index=False)\n",
    "        print(\"[features] saved timeseries and snapshot\")\n",
    "        return snap_path\n",
    "    except Exception as e:\n",
    "        print(\"[features] build_from_incr error:\", e)\n",
    "        return None\n",
    "\n",
    "def build_features_from_master(master_parquet: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Build features from full ohlcv_master.parquet and return path to snapshot CSV (features_today_*.csv)\n",
    "    This is used when incremental snapshot is empty/outside hours; we compute indicators from master historical data and output 'features_today'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(master_parquet):\n",
    "        print(\"[features_master] master parquet not found:\", master_parquet)\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_parquet(master_parquet)\n",
    "        if df.empty:\n",
    "            print(\"[features_master] master parquet empty\")\n",
    "            return None\n",
    "        # ensure timestamp as datetime\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df = df.sort_values([\"ticker\",\"timestamp\"])\n",
    "        frames=[]\n",
    "        fi_local = None\n",
    "        try:\n",
    "            fi_local = client_fq.FiinIndicator()\n",
    "        except Exception:\n",
    "            fi_local = fi\n",
    "        for tk, g in df.groupby(\"ticker\"):\n",
    "            gg = g.sort_values(\"timestamp\").copy()\n",
    "            try:\n",
    "                if fi_local is not None:\n",
    "                    gg[\"ema5\"] = fi_local.ema(gg[\"close\"], window=5)\n",
    "                    gg[\"ema20\"] = fi_local.ema(gg[\"close\"], window=20)\n",
    "                    gg[\"ema50\"] = fi_local.ema(gg[\"close\"], window=50)\n",
    "                    gg[\"rsi14\"] = fi_local.rsi(gg[\"close\"], window=14)\n",
    "                    gg[\"macd\"] = fi_local.macd(gg[\"close\"], window_fast=12, window_slow=26)\n",
    "                    gg[\"macd_signal\"] = fi_local.macd_signal(gg[\"close\"], window_fast=12, window_slow=26, window_sign=9)\n",
    "                    gg[\"macd_diff\"] = gg[\"macd\"] - gg[\"macd_signal\"]\n",
    "                    gg[\"boll_up\"] = fi_local.bollinger_hband(gg[\"close\"], window=20, window_dev=2)\n",
    "                    gg[\"boll_dn\"] = fi_local.bollinger_lband(gg[\"close\"], window=20, window_dev=2)\n",
    "                    gg[\"atr14\"] = fi_local.atr(gg[\"high\"], gg[\"low\"], gg[\"close\"], window=14)\n",
    "                    gg[\"obv\"] = fi_local.obv(gg[\"close\"], gg[\"volume\"])\n",
    "                    gg[\"vwap\"] = fi_local.vwap(gg[\"high\"], gg[\"low\"], gg[\"close\"], gg[\"volume\"], window=20)\n",
    "                    gg[\"vol_z\"] = (gg[\"volume\"] - gg[\"volume\"].rolling(20).mean()) / gg[\"volume\"].rolling(20).std()\n",
    "                else:\n",
    "                    gg[\"ema5\"] = gg[\"close\"].ewm(span=5).mean()\n",
    "                    gg[\"ema20\"] = gg[\"close\"].ewm(span=20).mean()\n",
    "                    gg[\"ema50\"] = gg[\"close\"].ewm(span=50).mean()\n",
    "                    gg[\"rsi14\"] = np.nan\n",
    "                    gg[\"macd\"] = gg[\"close\"].ewm(span=12).mean() - gg[\"close\"].ewm(span=26).mean()\n",
    "                    gg[\"macd_signal\"] = gg[\"macd\"].ewm(span=9).mean()\n",
    "                    gg[\"macd_diff\"] = gg[\"macd\"] - gg[\"macd_signal\"]\n",
    "                    gg[\"boll_up\"] = gg[\"close\"].rolling(20).mean() + 2 * gg[\"close\"].rolling(20).std()\n",
    "                    gg[\"boll_dn\"] = gg[\"close\"].rolling(20).mean() - 2 * gg[\"close\"].rolling(20).std()\n",
    "                    gg[\"vol_z\"] = (gg[\"volume\"] - gg[\"volume\"].rolling(20).mean()) / gg[\"volume\"].rolling(20).std()\n",
    "            except Exception as e:\n",
    "                print(f\"[features_master] TA error {tk}: {e}\")\n",
    "            frames.append(gg)\n",
    "        df_feat = pd.concat(frames, ignore_index=True)\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        timeseries_path = os.path.join(FEATURE_DIR, f\"features_timeseries_{ts}.parquet\")\n",
    "        df_feat.to_parquet(timeseries_path, index=False)\n",
    "        snapshot = df_feat.groupby(\"ticker\", as_index=False).last()\n",
    "        snap_path = os.path.join(FEATURE_DIR, f\"features_today_{ts}.csv\")\n",
    "        snapshot.to_csv(snap_path, index=False)\n",
    "        print(\"[features_master] saved timeseries & snapshot\")\n",
    "        return snap_path\n",
    "    except Exception as e:\n",
    "        print(\"[features_master] error:\", e)\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Block 5: A3C inference (fallback heuristic)\n",
    "# ---------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# --- Model A3C (gi·ªëng block training) ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "class DummyA3C:\n",
    "    \"\"\"\n",
    "    If you have per-cluster A3C models, load them externally and change a3c_infer to use them.\n",
    "    This pipeline prefers a precomputed signals file 'signals/a3c_signals_infer.csv' if present.\n",
    "    Otherwise fallback to heuristic rule-based signals.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def a3c_infer(features_today_csv: str, a3c_models: Optional[dict] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Return CSV path with columns: date, ticker, action (int: -1/0/1), prob_buy, prob_hold, prob_sell\n",
    "    \"\"\"\n",
    "    if features_today_csv is None or not os.path.exists(features_today_csv):\n",
    "        print(\"[a3c] features missing -> skip\")\n",
    "        return None\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    out_path = os.path.join(SIGNAL_DIR, f\"a3c_signals_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    precomputed = os.path.join(SIGNAL_DIR, \"a3c_signals_infer.csv\")\n",
    "\n",
    "    # --- Case 1: n·∫øu c√≥ file infer s·∫µn th√¨ d√πng ---\n",
    "    if os.path.exists(precomputed):\n",
    "        try:\n",
    "            df = pd.read_csv(precomputed)\n",
    "            if \"signal\" in df.columns and \"action\" not in df.columns:\n",
    "                df = df.rename(columns={\"signal\":\"action\"})\n",
    "            if \"action\" not in df.columns:\n",
    "                if \"pred\" in df.columns:\n",
    "                    df[\"action\"] = df[\"pred\"].astype(int)\n",
    "                else:\n",
    "                    df[\"action\"] = 0\n",
    "            for c,default in [(\"prob_buy\",0.2),(\"prob_hold\",0.6),(\"prob_sell\",0.2)]:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = default\n",
    "            df[[\"date\",\"ticker\",\"action\",\"prob_buy\",\"prob_hold\",\"prob_sell\"]].to_csv(out_path, index=False)\n",
    "            print(\"[a3c] used precomputed infer file ->\", out_path)\n",
    "            return out_path\n",
    "        except Exception as e:\n",
    "            print(\"[a3c] precomputed load error:\", e)\n",
    "\n",
    "    # --- Case 2: n·∫øu c√≥ checkpoint actor th√¨ load ---\n",
    "    actor_path = os.path.join(MODEL_DIR, \"a3c_actor.pth\")\n",
    "    if os.path.exists(actor_path):\n",
    "        try:\n",
    "            # ƒê·ªãnh nghƒ©a s·ªë feature = s·ªë c·ªôt c·ªßa features_today_csv\n",
    "            feats = pd.read_csv(features_today_csv)\n",
    "            F = feats.shape[1] - 1  # tr·ª´ c·ªôt ticker\n",
    "            model = A3CNet(F).to(device)\n",
    "            model.load_state_dict(torch.load(actor_path, map_location=device))\n",
    "            model.eval()\n",
    "\n",
    "            rows=[]\n",
    "            with torch.no_grad():\n",
    "                for _, r in feats.iterrows():\n",
    "                    x = torch.tensor(r.drop(\"ticker\").values, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    logits, _ = model(x)\n",
    "                    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                    act = int(np.argmax(probs)-1)  # (-1,0,1)\n",
    "                    rows.append([date_str, r[\"ticker\"], act, probs[2], probs[1], probs[0]])\n",
    "            odf = pd.DataFrame(rows, columns=[\"date\",\"ticker\",\"action\",\"prob_buy\",\"prob_hold\",\"prob_sell\"])\n",
    "            odf.to_csv(out_path, index=False)\n",
    "            print(\"[a3c] used trained actor checkpoint ->\", out_path)\n",
    "            return out_path\n",
    "        except Exception as e:\n",
    "            print(\"[a3c] checkpoint load error, fallback:\", e)\n",
    "\n",
    "    # --- Case 3: fallback rule-based ---\n",
    "    feats = pd.read_csv(features_today_csv)\n",
    "    rows=[]\n",
    "    for _, r in feats.iterrows():\n",
    "        ema5 = gfloat(r.get(\"ema5\")); ema20 = gfloat(r.get(\"ema20\"))\n",
    "        rsi = gfloat(r.get(\"rsi14\"))\n",
    "        action = 1 if (not math.isnan(ema5) and not math.isnan(ema20) and ema5>ema20 and not math.isnan(rsi) and rsi<RSI_TH) else 0\n",
    "        if action == 1:\n",
    "            pb,ph,ps = 0.75,0.2,0.05\n",
    "        else:\n",
    "            pb,ph,ps = 0.1,0.8,0.1\n",
    "        rows.append([date_str, r[\"ticker\"], int(action), pb, ph, ps])\n",
    "    odf = pd.DataFrame(rows, columns=[\"date\",\"ticker\",\"action\",\"prob_buy\",\"prob_hold\",\"prob_sell\"])\n",
    "    odf.to_csv(out_path, index=False)\n",
    "    print(\"[a3c] fallback saved:\", out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Block 6: Rule-based scoring\n",
    "# ---------------------------\n",
    "def rule_scoring(features_today_csv: str) -> Optional[str]:\n",
    "    if features_today_csv is None or not os.path.exists(features_today_csv):\n",
    "        print(\"[rule] features missing\")\n",
    "        return None\n",
    "    feats = pd.read_csv(features_today_csv)\n",
    "    rows=[]\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for _, r in feats.iterrows():\n",
    "        ticker = r[\"ticker\"]\n",
    "        ema5 = gfloat(r.get(\"ema5\")); ema20 = gfloat(r.get(\"ema20\"))\n",
    "        rsi = gfloat(r.get(\"rsi14\")); macd_diff = gfloat(r.get(\"macd_diff\"))\n",
    "        close = gfloat(r.get(\"close\")); boll_up = gfloat(r.get(\"boll_up\")); vol_z = gfloat(r.get(\"vol_z\"))\n",
    "        score=0; pass_list=[]; fail_list=[]\n",
    "        if not math.isnan(ema5) and not math.isnan(ema20) and ema5>ema20:\n",
    "            score+=1; pass_list.append(\"EMA5>EMA20\")\n",
    "        else:\n",
    "            fail_list.append(\"EMA<=20\")\n",
    "        if not math.isnan(rsi) and rsi < RSI_TH:\n",
    "            score+=1; pass_list.append(\"RSI<TH\")\n",
    "        else:\n",
    "            fail_list.append(\"RSI>=\")\n",
    "        if not math.isnan(macd_diff) and macd_diff > 0:\n",
    "            score+=1; pass_list.append(\"MACD>0\")\n",
    "        else:\n",
    "            fail_list.append(\"MACD<=0\")\n",
    "        if not math.isnan(close) and not math.isnan(boll_up) and close > boll_up:\n",
    "            score+=1; pass_list.append(\"BollBreak\")\n",
    "        else:\n",
    "            fail_list.append(\"BollNo\")\n",
    "        if not math.isnan(vol_z) and vol_z > VOL_SPIKE_MULT:\n",
    "            score+=1; pass_list.append(\"VolSpike\")\n",
    "        else:\n",
    "            fail_list.append(\"VolNormal\")\n",
    "        details = {\"ema5\":ema5,\"ema20\":ema20,\"rsi\":rsi,\"macd_diff\":macd_diff,\"close\":close,\"boll_up\":boll_up,\"vol_z\":vol_z}\n",
    "        rows.append([date_str, ticker, int(score), \";\".join(pass_list), \";\".join(fail_list), json.dumps(details), float(r.get(\"close\", np.nan))])\n",
    "    df = pd.DataFrame(rows, columns=[\"date\",\"ticker\",\"rule_score\",\"pass_list\",\"fail_list\",\"detail_values\",\"close\"])\n",
    "    out_path = os.path.join(SIGNAL_DIR, f\"rule_scores_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"[rule] saved:\", out_path)\n",
    "    return out_path\n",
    "\n",
    "# ---------------------------\n",
    "# Block 7: DDPG hybrid allocation (fallback equal-weight)\n",
    "# ---------------------------\n",
    "def run_ddpg_hybrid(a3c_csv: Optional[str], rule_csv: Optional[str]) -> Tuple[Optional[str], Optional[str], str]:\n",
    "    \"\"\"\n",
    "    Try:\n",
    "      1) use ddpg_actor.pth if present (trained in notebook)\n",
    "      2) use historical weights_by_ticker.csv if exists\n",
    "      3) fallback to equal-weight among active tickers (action==1 & rule_score>=RULE_MIN_SCORE)\n",
    "    Returns (alloc_path, top10_path, alloc_source)\n",
    "    \"\"\"\n",
    "    if a3c_csv is None or not os.path.exists(a3c_csv) or rule_csv is None or not os.path.exists(rule_csv):\n",
    "        print(\"[ddpg_hybrid] missing inputs\")\n",
    "        return (None, None, \"no_input\")\n",
    "\n",
    "    sig = pd.read_csv(a3c_csv)\n",
    "    rule = pd.read_csv(rule_csv)\n",
    "    if \"signal\" in sig.columns and \"action\" not in sig.columns:\n",
    "        sig = sig.rename(columns={\"signal\": \"action\"})\n",
    "    if \"action\" not in sig.columns:\n",
    "        sig[\"action\"] = sig.get(\"action\", 0).fillna(0).astype(int)\n",
    "\n",
    "    merged = sig.merge(rule, on=[\"date\", \"ticker\"], how=\"left\")\n",
    "    merged[\"rule_score\"] = merged[\"rule_score\"].fillna(0).astype(int)\n",
    "    active = merged[(merged[\"action\"] == 1) & (merged[\"rule_score\"] >= RULE_MIN_SCORE)]\n",
    "    active_tickers = sorted(active[\"ticker\"].unique().tolist())\n",
    "    if not active_tickers:\n",
    "        print(\"[ddpg_hybrid] no active tickers -> no allocation\")\n",
    "        return (None, None, \"no_active\")\n",
    "\n",
    "    # --- Try ddpg actor (saved from training notebook) ---\n",
    "    actor_path = os.path.join(MODEL_DIR, \"ddpg_actor.pth\")\n",
    "    if os.path.exists(actor_path) and TORCH:\n",
    "        try:\n",
    "            # ƒë·ªãnh nghƒ©a l·∫°i Actor gi·ªëng notebook training\n",
    "            class Actor(nn.Module):\n",
    "                def __init__(self, s_dim, a_dim, hidden=128):\n",
    "                    super().__init__()\n",
    "                    self.net = nn.Sequential(\n",
    "                        nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "                        nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "                        nn.Linear(hidden, a_dim)\n",
    "                    )\n",
    "                def forward(self, s):\n",
    "                    return torch.softmax(self.net(s), dim=-1)\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # --- load actor ---\n",
    "            s_dim = len(active_tickers)    # t·∫°m l·∫•y s·ªë m√£ active l√†m state_dim\n",
    "            a_dim = len(active_tickers)    # s·ªë action = s·ªë m√£\n",
    "            ddpg_actor = Actor(s_dim, a_dim).to(device)\n",
    "            ddpg_actor.load_state_dict(torch.load(actor_path, map_location=device))\n",
    "            ddpg_actor.eval()\n",
    "\n",
    "            # --- state hi·ªán t·∫°i: ƒë∆°n gi·∫£n = identity (1 n·∫øu active) ---\n",
    "            state_today = np.zeros((s_dim,), dtype=\"float32\")\n",
    "            for i, tk in enumerate(active_tickers):\n",
    "                state_today[i] = 1.0  # ƒë√°nh d·∫•u m√£ active\n",
    "\n",
    "            s = torch.tensor(state_today, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                w = ddpg_actor(s).cpu().numpy()[0]\n",
    "\n",
    "            # normalize\n",
    "            w = np.clip(w, 0, 1)\n",
    "            if w.sum() <= 1e-12:\n",
    "                w = np.ones_like(w) / len(w)\n",
    "            else:\n",
    "                w /= w.sum()\n",
    "\n",
    "            # build alloc df\n",
    "            date_label = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            rows = [[date_label, 0, tk, 1.0, float(w[i])] for i, tk in enumerate(active_tickers)]\n",
    "            df_alloc = pd.DataFrame(rows, columns=[\"date\",\"cluster\",\"ticker\",\"cluster_weight\",\"ticker_weight\"])\n",
    "\n",
    "            alloc_path = os.path.join(ALLOC_DIR, f\"alloc_ddpg_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "            df_alloc.to_csv(alloc_path, index=False)\n",
    "\n",
    "            top10 = df_alloc.sort_values(\"ticker_weight\", ascending=False).head(TOPK)\n",
    "            top10_path = os.path.join(ALLOC_DIR, f\"top10_ddpg_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "            top10.to_csv(top10_path, index=False)\n",
    "\n",
    "            print(\"[ddpg_hybrid] used ddpg_actor ->\", alloc_path)\n",
    "            return (alloc_path, top10_path, \"DDPG\")\n",
    "        except Exception as e:\n",
    "            print(\"[ddpg_hybrid] ddpg actor error:\", e)\n",
    "\n",
    "    # --- Try historical weights_by_ticker.csv ---\n",
    "    hist_weights_path = os.path.join(MONITOR_DIR, \"weights_by_ticker.csv\")\n",
    "    if os.path.exists(hist_weights_path):\n",
    "        try:\n",
    "            dfw = pd.read_csv(hist_weights_path, index_col=0)\n",
    "            if not dfw.empty:\n",
    "                last = dfw.iloc[-1]\n",
    "                rows = []\n",
    "                date_label = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                total_raw = 0.0\n",
    "                for t in active_tickers:\n",
    "                    w = float(last.get(t, 0.0)) if t in last.index else 0.0\n",
    "                    rows.append([date_label, 0, t, None, w])\n",
    "                    total_raw += w\n",
    "                if total_raw > 1e-12:\n",
    "                    df_alloc = pd.DataFrame(rows, columns=[\"date\",\"cluster\",\"ticker\",\"cluster_weight\",\"ticker_weight\"])\n",
    "                    df_alloc[\"ticker_weight\"] = df_alloc[\"ticker_weight\"] / df_alloc[\"ticker_weight\"].sum()\n",
    "                    alloc_path = os.path.join(ALLOC_DIR, f\"alloc_hist_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "                    df_alloc.to_csv(alloc_path, index=False)\n",
    "                    top10 = df_alloc.sort_values(\"ticker_weight\", ascending=False).head(TOPK)\n",
    "                    top10_path = os.path.join(ALLOC_DIR, f\"top10_hist_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "                    top10.to_csv(top10_path, index=False)\n",
    "                    print(\"[ddpg_hybrid] using historical weights saved:\", alloc_path)\n",
    "                    return (alloc_path, top10_path, \"HistAlloc\")\n",
    "        except Exception as e:\n",
    "            print(\"[ddpg_hybrid] historical weights load error:\", e)\n",
    "\n",
    "    # --- fallback equal-weight ---\n",
    "    per = 1.0 / len(active_tickers)\n",
    "    rows = []\n",
    "    date_label = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for t in active_tickers:\n",
    "        rows.append([date_label, 0, t, 1.0, round(per, 6)])\n",
    "    df_alloc = pd.DataFrame(rows, columns=[\"date\",\"cluster\",\"ticker\",\"cluster_weight\",\"ticker_weight\"])\n",
    "    alloc_path = os.path.join(ALLOC_DIR, f\"alloc_equal_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    df_alloc.to_csv(alloc_path, index=False)\n",
    "    top10_path = os.path.join(ALLOC_DIR, f\"top10_equal_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    df_alloc.sort_values(\"ticker_weight\", ascending=False).head(TOPK).to_csv(top10_path, index=False)\n",
    "    print(\"[ddpg_hybrid] equal-weight allocation saved:\", alloc_path)\n",
    "    return (alloc_path, top10_path, \"EqualWeight\")\n",
    "\n",
    "# ---------------------------\n",
    "# Block 8: Decision engine & Alerts\n",
    "# ---------------------------\n",
    "EMOJI = {\"BUY\":\"üü¢\",\"SELL\":\"üî¥\",\"HOLD\":\"üü°\",\"PASS\":\"‚úîÔ∏è\",\"FAIL\":\"‚úñÔ∏è\"}\n",
    "\n",
    "def decision_and_alerts(a3c_csv: Optional[str], rule_csv: Optional[str], alloc_pair: Tuple[Optional[str],Optional[str]], alloc_source: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Create alerts CSV and send Telegram messages + push to Google Sheets.\n",
    "    \"\"\"\n",
    "    if a3c_csv is None or not os.path.exists(a3c_csv) or rule_csv is None or not os.path.exists(rule_csv):\n",
    "        print(\"[alerts] missing inputs\")\n",
    "        return None\n",
    "    sig = pd.read_csv(a3c_csv)\n",
    "    rule = pd.read_csv(rule_csv)\n",
    "    if \"signal\" in sig.columns and \"action\" not in sig.columns:\n",
    "        sig = sig.rename(columns={\"signal\":\"action\"})\n",
    "    if \"action\" not in sig.columns:\n",
    "        sig[\"action\"] = sig.get(\"action\", 0).fillna(0).astype(int)\n",
    "    merged = sig.merge(rule, on=[\"date\",\"ticker\"], how=\"left\")\n",
    "    # attach allocation weights if present\n",
    "    if alloc_pair and alloc_pair[0] and os.path.exists(alloc_pair[0]):\n",
    "        alloc_df = pd.read_csv(alloc_pair[0])\n",
    "        merged = merged.merge(alloc_df[[\"ticker\",\"ticker_weight\"]], on=\"ticker\", how=\"left\")\n",
    "    else:\n",
    "        merged[\"ticker_weight\"] = 0.0\n",
    "\n",
    "    ts_now = datetime.now()\n",
    "    alerts=[]\n",
    "    for _, r in merged.iterrows():\n",
    "        ticker = r[\"ticker\"]\n",
    "        action_code = int(r.get(\"action\",0))\n",
    "        rule_score = int(r.get(\"rule_score\",0)) if not pd.isna(r.get(\"rule_score\", np.nan)) else 0\n",
    "        alloc_pct = float(r.get(\"ticker_weight\", 0.0)) if not pd.isna(r.get(\"ticker_weight\", np.nan)) else 0.0\n",
    "        price = r.get(\"close\", None)\n",
    "        decided = None\n",
    "        if action_code == 1 and rule_score >= RULE_MIN_SCORE and alloc_pct >= MIN_ALLOC:\n",
    "            decided = \"BUY\"\n",
    "        elif action_code == -1 and rule_score >= RULE_MIN_SCORE:\n",
    "            decided = \"SELL\"\n",
    "        else:\n",
    "            decided = None\n",
    "\n",
    "        if decided is None:\n",
    "            continue\n",
    "\n",
    "        # debounce\n",
    "        last = runtime_state.get(\"last_alerts\", {}).get(ticker)\n",
    "        can_send = True\n",
    "        if last:\n",
    "            try:\n",
    "                last_time = datetime.fromisoformat(last.get(\"time\"))\n",
    "                last_action = last.get(\"action\")\n",
    "                if last_action == decided and (ts_now - last_time) < timedelta(hours=COOLDOWN_HOURS):\n",
    "                    can_send = False\n",
    "            except Exception:\n",
    "                can_send = True\n",
    "        if not can_send:\n",
    "            print(f\"[alerts] debounce skip {ticker} {decided}\")\n",
    "            continue\n",
    "\n",
    "        alert_id = f\"{ticker}_{ts_now.strftime('%Y%m%d_%H%M%S')}_{np.random.randint(0,999999):06d}\"\n",
    "        exec_date = (ts_now + timedelta(days=EXEC_LAG)).strftime(\"%Y-%m-%d\")\n",
    "        indicators = r.get(\"pass_list\", \"\") or \"\"\n",
    "        source_txt = alloc_source or \"Unknown\"\n",
    "\n",
    "        row = {\n",
    "            \"log_time\": ts_now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"alert_id\": alert_id,\n",
    "            \"ticker\": ticker,\n",
    "            \"action\": decided,\n",
    "            \"price\": price if not pd.isna(price) else None,\n",
    "            \"a3c\": {1:\"BUY\",0:\"HOLD\",-1:\"SELL\"}.get(action_code,\"HOLD\"),\n",
    "            \"rule_score\": rule_score,\n",
    "            \"indicators\": indicators,\n",
    "            \"alloc_pct\": alloc_pct,\n",
    "            \"alloc_source\": source_txt,\n",
    "            \"exec_date\": exec_date,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "\n",
    "        # build Telegram message (Markdown)\n",
    "        emoji = EMOJI.get(decided, \"\")\n",
    "        em_a3c = EMOJI.get(\"BUY\",\"\") if action_code==1 else (EMOJI.get(\"SELL\",\"\") if action_code==-1 else EMOJI.get(\"HOLD\",\"\"))\n",
    "        msg = (\n",
    "            f\"{emoji} *C·∫¢NH B√ÅO M·ªöI*\\n\"\n",
    "            f\"Th·ªùi gian: {ts_now.strftime('%Y-%m-%d %H:%M')}\\n\"\n",
    "            f\"M√£: *{ticker}*\\n\"\n",
    "            f\"H√†nh ƒë·ªông: *{decided}* {emoji}\\n\"\n",
    "            f\"Gi√°: {row['price'] if row['price'] is not None else 'N/A'}\\n\"\n",
    "            f\"A3C: {em_a3c} {row['a3c']}\\n\"\n",
    "            f\"Rule-based: {row['rule_score']}/5 ‚Äî {indicators}\\n\"\n",
    "            f\"Ph√¢n b·ªï v·ªën: {round(alloc_pct*100,4)}%  (Ngu·ªìn: {source_txt})\\n\"\n",
    "            f\"Exec: T+{EXEC_LAG} ({exec_date})\\n\"\n",
    "            f\"AlertID: `{alert_id}`\"\n",
    "        )\n",
    "        sent = send_telegram_text(msg, thread_id=TG_THREAD_ID)\n",
    "        time.sleep(0.5)\n",
    "        row[\"status\"] = \"SENT\" if sent else \"NOT_SENT\"\n",
    "        alerts.append(row)\n",
    "        runtime_state.setdefault(\"last_alerts\", {})[ticker] = {\"time\": ts_now.isoformat(), \"action\": decided}\n",
    "\n",
    "    if not alerts:\n",
    "        # send HOLD summary (top candidates)\n",
    "        merged_local = merged.copy()\n",
    "        merged_local[\"rule_score\"] = merged_local[\"rule_score\"].fillna(0)\n",
    "        top = merged_local.sort_values(\"rule_score\", ascending=False).drop_duplicates(\"ticker\").head(TOPK)\n",
    "        top_text = \"\"\n",
    "        for idx, rowt in top.iterrows():\n",
    "            top_text += f\"{rowt['ticker']} ‚Äî rule {int(rowt['rule_score'])}\\n\"\n",
    "        hold_msg = (\n",
    "            f\"{EMOJI['HOLD']} *NO TRADE SIGNAL (HOLD)*\\n\"\n",
    "            f\"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\"\n",
    "            f\"Top {TOPK} candidates by rule:\\n{top_text}\"\n",
    "        )\n",
    "        send_telegram_text(hold_msg, thread_id=TG_THREAD_ID)\n",
    "        # optionally send top10 file if present\n",
    "        if alloc_pair and alloc_pair[1] and os.path.exists(alloc_pair[1]):\n",
    "            send_telegram_file(alloc_pair[1], caption=f\"Top {TOPK} allocations\")\n",
    "        print(\"[alerts] no trade alerts this run (HOLD summary sent if applicable)\")\n",
    "        save_runtime_state()\n",
    "        return None\n",
    "\n",
    "    df_alerts = pd.DataFrame(alerts)\n",
    "    out_path = os.path.join(ALERT_DIR, f\"alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    df_alerts.to_csv(out_path, index=False)\n",
    "    # push to gsheet\n",
    "    try:\n",
    "        append_to_gsheet(df_alerts, worksheet_name=\"Alerts\")\n",
    "    except Exception as e:\n",
    "        print(\"[alerts] push to gsheet error:\", e)\n",
    "    master_log = os.path.join(ALERT_DIR, \"alerts_log.csv\")\n",
    "    df_alerts.to_csv(master_log, mode=\"a\", header=not os.path.exists(master_log), index=False)\n",
    "    print(\"[alerts] saved & sent:\", out_path)\n",
    "    save_runtime_state()\n",
    "    return out_path\n",
    "\n",
    "# ---------------------------\n",
    "# Block 9: Monitoring & artifact logging\n",
    "# ---------------------------\n",
    "def log_weights_returns_turnover(alloc_csv: Optional[str], alloc_source: str):\n",
    "    if alloc_csv is None or not os.path.exists(alloc_csv):\n",
    "        print(\"[monitor] no alloc to log\")\n",
    "        return\n",
    "    alloc = pd.read_csv(alloc_csv)\n",
    "    date_label = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    w_series = alloc.set_index(\"ticker\")[\"ticker_weight\"].astype(float)\n",
    "    weights_path = os.path.join(MONITOR_DIR, \"weights_by_ticker.csv\")\n",
    "    prev_weights = None\n",
    "    if os.path.exists(weights_path):\n",
    "        prev_df = pd.read_csv(weights_path, index_col=0)\n",
    "        if not prev_df.empty:\n",
    "            prev_weights = prev_df.iloc[-1].dropna().astype(float)\n",
    "    all_tickers = sorted(list(set(list(w_series.index) + ([] if prev_weights is None else list(prev_weights.index)))))\n",
    "    row = {t: float(w_series.get(t, 0.0)) for t in all_tickers}\n",
    "    df_row = pd.DataFrame([row], index=[date_label])\n",
    "    df_row.to_csv(weights_path, mode=\"a\", header=not os.path.exists(weights_path))\n",
    "    if prev_weights is not None:\n",
    "        prev = prev_weights.reindex(all_tickers).fillna(0.0)\n",
    "        curr = pd.Series(row).reindex(all_tickers).fillna(0.0)\n",
    "        turnover = float(np.sum(np.abs(curr.values - prev.values)))\n",
    "    else:\n",
    "        turnover = float(np.sum(np.abs(list(row.values()))))\n",
    "    trades_log_path = os.path.join(MONITOR_DIR, \"trades_log.csv\")\n",
    "    pd.DataFrame([{\"date\": date_label, \"turnover\": turnover, \"alloc_source\": alloc_source}]).to_csv(trades_log_path, mode=\"a\", header=not os.path.exists(trades_log_path), index=False)\n",
    "    # naive returns using last two dates in master parquet\n",
    "    master_path = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "    returns_path = os.path.join(MONITOR_DIR, \"returns.csv\")\n",
    "    if os.path.exists(master_path):\n",
    "        px = pd.read_parquet(master_path)\n",
    "        px[\"timestamp\"] = pd.to_datetime(px[\"timestamp\"])\n",
    "        dates_unique = pd.Series(px[\"timestamp\"].dt.date.unique())\n",
    "        if len(dates_unique) >= 2:\n",
    "            d1 = pd.to_datetime(dates_unique.iloc[-2])\n",
    "            d2 = pd.to_datetime(dates_unique.iloc[-1])\n",
    "            px_wide = px.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\")\n",
    "            try:\n",
    "                p1 = px_wide.loc[px_wide.index.date == d1.date()].iloc[-1]\n",
    "                p2 = px_wide.loc[px_wide.index.date == d2.date()].iloc[-1]\n",
    "                pct = (p2 - p1) / p1\n",
    "                if prev_weights is None:\n",
    "                    port_ret = float(np.dot(list(row.values()), pct.reindex(all_tickers).fillna(0.0).values))\n",
    "                else:\n",
    "                    port_ret = float(np.dot(prev.reindex(all_tickers).fillna(0.0).values, pct.reindex(all_tickers).fillna(0.0).values))\n",
    "                pd.DataFrame([{\"date\": date_label, \"daily_return\": port_ret, \"alloc_source\": alloc_source}]).to_csv(returns_path, mode=\"a\", header=not os.path.exists(returns_path), index=False)\n",
    "            except Exception as e:\n",
    "                print(\"[monitor] compute returns error:\", e)\n",
    "        else:\n",
    "            print(\"[monitor] not enough history to compute returns\")\n",
    "    else:\n",
    "        print(\"[monitor] ohlcv master missing -> cannot compute returns\")\n",
    "\n",
    "def monitoring_stats_update():\n",
    "    api_file = os.path.join(MONITOR_DIR, \"api_usage.csv\")\n",
    "    pd.DataFrame([{\"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"api_used\": runtime_state.get(\"api_used\",0)}]).to_csv(api_file, mode=\"a\", header=not os.path.exists(api_file), index=False)\n",
    "    master_log = os.path.join(ALERT_DIR, \"alerts_log.csv\")\n",
    "    if os.path.exists(master_log):\n",
    "        df = pd.read_csv(master_log, parse_dates=[\"log_time\"])\n",
    "        df[\"date_only\"] = df[\"log_time\"].dt.date\n",
    "        stats = df.groupby(\"date_only\").agg(total=(\"alert_id\",\"count\"), buys=(\"action\", lambda s: (s==\"BUY\").sum()), sells=(\"action\", lambda s: (s==\"SELL\").sum()))\n",
    "        stats.to_csv(os.path.join(MONITOR_DIR, \"alert_stats.csv\"))\n",
    "\n",
    "# ---------------------------\n",
    "# Orchestration: run_cycle (one-run)\n",
    "# ---------------------------\n",
    "def run_cycle() -> dict:\n",
    "    artifacts = {}\n",
    "    univ_path = os.path.join(UNIVERSE_DIR, \"universe_list.csv\")\n",
    "    if not os.path.exists(univ_path):\n",
    "        raise FileNotFoundError(\"universe/universe_list.csv missing. Run Block 2 offline.\")\n",
    "    universe = pd.read_csv(univ_path)[\"ticker\"].tolist()\n",
    "    print(f\"[run_cycle] universe size: {len(universe)}\")\n",
    "    # Block3: fetch snapshot (stream)\n",
    "    incr = None\n",
    "    try:\n",
    "        incr = fetch_snapshot(universe, seconds_snapshot=SNAPSHOT_SECONDS)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] fetch_snapshot error:\", e)\n",
    "    artifacts[\"incremental_csv\"] = incr\n",
    "    master = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "    # Backfill master if missing\n",
    "    if not os.path.exists(master):\n",
    "        try:\n",
    "            print(\"[run_cycle] master missing -> backfilling 6 months\")\n",
    "            from_date = (datetime.now() - pd.DateOffset(months=6)).strftime(\"%Y-%m-%d\")\n",
    "            all_frames=[]\n",
    "            for i in range(0, len(universe), BATCH_SIZE):\n",
    "                batch = universe[i:i+BATCH_SIZE]\n",
    "                try:\n",
    "                    dfb = client_fq.Fetch_Trading_Data(realtime=False, tickers=batch,\n",
    "                              fields=['open','high','low','close','volume'], adjusted=True, by='1d', from_date=from_date).get_data()\n",
    "                    all_frames.append(dfb)\n",
    "                    time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    print(\"[backfill] batch error:\", e)\n",
    "            if len(all_frames) > 0:\n",
    "                df_all = pd.concat(all_frames, ignore_index=True)\n",
    "                df_all.to_parquet(master, index=False)\n",
    "                print(\"[backfill] saved master parquet\", master, \"rows=\", len(df_all))\n",
    "        except Exception as e:\n",
    "            print(\"[run_cycle] backfill exception:\", e)\n",
    "    # Push master to GSheet if present\n",
    "    try:\n",
    "        if os.path.exists(master):\n",
    "            push_ohlcv_master_to_gsheet(master)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] push master to gsheet failed:\", e)\n",
    "    # push incremental to gsheet as well\n",
    "    try:\n",
    "        if incr and os.path.exists(incr):\n",
    "            push_csv_to_gsheet(incr, worksheet_name_prefix=\"INCR\")\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] push incremental to gsheet failed:\", e)\n",
    "\n",
    "    # Block 4: features\n",
    "    features_csv = None\n",
    "    try:\n",
    "        if os.path.exists(master):\n",
    "            features_csv = build_features_from_master(master)\n",
    "        else:\n",
    "            features_csv = build_features_from_incr(incr)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] build features error:\", e)\n",
    "    artifacts[\"features_csv\"] = features_csv\n",
    "\n",
    "    # Block 5: A3C inference\n",
    "    try:\n",
    "        a3c_csv = a3c_infer(features_csv, None)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] a3c infer error:\", e)\n",
    "        a3c_csv = None\n",
    "    artifacts[\"a3c_csv\"] = a3c_csv\n",
    "\n",
    "    # Block 6: rule scoring\n",
    "    try:\n",
    "        rule_csv = rule_scoring(features_csv)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] rule scoring error:\", e)\n",
    "        rule_csv = None\n",
    "    artifacts[\"rule_csv\"] = rule_csv\n",
    "\n",
    "    # Block 7: allocation\n",
    "    try:\n",
    "        alloc_path, top10_path, alloc_source = run_ddpg_hybrid(a3c_csv, rule_csv)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] run_ddpg_hybrid error:\", e)\n",
    "        alloc_path, top10_path, alloc_source = (None, None, \"no_input\")\n",
    "    artifacts[\"alloc_path\"] = alloc_path\n",
    "    artifacts[\"top10_path\"] = top10_path\n",
    "    artifacts[\"alloc_source\"] = alloc_source\n",
    "\n",
    "    # Block 8: decision & alerts\n",
    "    try:\n",
    "        alert_file = decision_and_alerts(a3c_csv, rule_csv, (alloc_path, top10_path), alloc_source)\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] decision_and_alerts error:\", e)\n",
    "        alert_file = None\n",
    "    artifacts[\"alert_file\"] = alert_file\n",
    "\n",
    "    # Block 9: monitoring\n",
    "    try:\n",
    "        log_weights_returns_turnover(alloc_path, alloc_source)\n",
    "        monitoring_stats_update()\n",
    "    except Exception as e:\n",
    "        print(\"[run_cycle] monitoring error:\", e)\n",
    "\n",
    "    print(\"[run_cycle] Done. Artifacts:\", artifacts)\n",
    "    return artifacts\n",
    "\n",
    "# ---------------------------\n",
    "# Historical replay / stress-test\n",
    "# ---------------------------\n",
    "def run_historical_inference(start_date: str, end_date: str, push_gsheet: bool = True, send_telegram: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Replay inference on historical master parquet between start_date and end_date inclusive.\n",
    "    For each trading date, compute features snapshot (up to that date), run a3c_infer & rule_scoring,\n",
    "    push results to google sheets and send telegram summary (if requested).\n",
    "    \"\"\"\n",
    "    master = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "    if not os.path.exists(master):\n",
    "        raise FileNotFoundError(\"ohlcv_master.parquet missing for historical inference\")\n",
    "    px = pd.read_parquet(master)\n",
    "    px[\"timestamp\"] = pd.to_datetime(px[\"timestamp\"])\n",
    "    dates_all = np.unique(px[\"timestamp\"].dt.date)\n",
    "    s_date = pd.to_datetime(start_date).date()\n",
    "    e_date = pd.to_datetime(end_date).date()\n",
    "    run_dates = [d for d in sorted(dates_all) if (d >= s_date and d <= e_date)]\n",
    "    summary_rows=[]\n",
    "    for d in run_dates:\n",
    "        date_label = pd.to_datetime(d).strftime(\"%Y-%m-%d\")\n",
    "        print(f\"[replay] running for {date_label}\")\n",
    "        try:\n",
    "            df_until = px[px[\"timestamp\"].dt.date <= pd.to_datetime(date_label).date()].copy()\n",
    "            frames=[]\n",
    "            fi_local = None\n",
    "            try:\n",
    "                fi_local = client_fq.FiinIndicator()\n",
    "            except Exception:\n",
    "                fi_local = fi\n",
    "            for tk, g in df_until.groupby(\"ticker\"):\n",
    "                gg = g.sort_values(\"timestamp\").copy()\n",
    "                try:\n",
    "                    if fi_local is not None:\n",
    "                        gg_feat = gg.copy()\n",
    "                        gg_feat['ema5'] = fi_local.ema(gg_feat['close'], window=5)\n",
    "                        gg_feat['ema20'] = fi_local.ema(gg_feat['close'], window=20)\n",
    "                        gg_feat['rsi14'] = fi_local.rsi(gg_feat['close'], window=14)\n",
    "                        gg_feat['macd'] = fi_local.macd(gg_feat['close'], window_fast=12, window_slow=26)\n",
    "                        gg_feat['macd_signal'] = fi_local.macd_signal(gg_feat['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "                        gg_feat['macd_diff'] = gg_feat['macd'] - gg_feat['macd_signal']\n",
    "                        gg_feat['boll_up'] = fi_local.bollinger_hband(gg_feat['close'], window=20, window_dev=2)\n",
    "                        gg_feat['boll_dn'] = fi_local.bollinger_lband(gg_feat['close'], window=20, window_dev=2)\n",
    "                        gg_feat['vol_z'] = (gg_feat['volume'] - gg_feat['volume'].rolling(20).mean())/gg_feat['volume'].rolling(20).std()\n",
    "                    else:\n",
    "                        gg_feat = gg.copy()\n",
    "                        gg_feat['ema5'] = gg_feat['close'].ewm(span=5).mean()\n",
    "                        gg_feat['ema20'] = gg_feat['close'].ewm(span=20).mean()\n",
    "                        gg_feat['rsi14'] = np.nan\n",
    "                        gg_feat['macd'] = gg_feat['close'].ewm(span=12).mean() - gg_feat['close'].ewm(span=26).mean()\n",
    "                        gg_feat['macd_signal'] = gg_feat['macd'].ewm(span=9).mean()\n",
    "                        gg_feat['macd_diff'] = gg_feat['macd'] - gg_feat['macd_signal']\n",
    "                        gg_feat['boll_up'] = gg_feat['close'].rolling(20).mean() + 2 * gg_feat['close'].rolling(20).std()\n",
    "                        gg_feat['boll_dn'] = gg_feat['close'].rolling(20).mean() - 2 * gg_feat['close'].rolling(20).std()\n",
    "                        gg_feat['vol_z'] = (gg_feat['volume'] - gg_feat['volume'].rolling(20).mean())/gg_feat['volume'].rolling(20).std()\n",
    "                except Exception as e:\n",
    "                    print(f\"[replay] TA error {tk} {e}\")\n",
    "                    gg_feat = gg.copy()\n",
    "                frames.append(gg_feat)\n",
    "            if len(frames) == 0:\n",
    "                print(\"[replay] no data for date\", date_label)\n",
    "                summary_rows.append({\"date\": date_label, \"a3c\": None, \"rule\": None, \"error\": \"no_data\"})\n",
    "                continue\n",
    "            df_all = pd.concat(frames, ignore_index=True)\n",
    "            snapshot = df_all.sort_values(\"timestamp\").groupby(\"ticker\").last().reset_index()\n",
    "            snapshot_path = os.path.join(FEATURE_DIR, f\"features_replay_{date_label}.csv\")\n",
    "            snapshot.to_csv(snapshot_path, index=False)\n",
    "            a3c_path = a3c_infer(snapshot_path, None)\n",
    "            rule_path = rule_scoring(snapshot_path)\n",
    "            # push to gsheet\n",
    "            if push_gsheet:\n",
    "                try:\n",
    "                    push_csv_to_gsheet(snapshot_path, worksheet_name_prefix=f\"OHLCV_REPLAY_{date_label}\")\n",
    "                except Exception as e:\n",
    "                    print(\"[replay][gs] push prices error:\", e)\n",
    "                try:\n",
    "                    if a3c_path and os.path.exists(a3c_path):\n",
    "                        push_csv_to_gsheet(a3c_path, worksheet_name_prefix=f\"A3C_REPLAY_{date_label}\")\n",
    "                    if rule_path and os.path.exists(rule_path):\n",
    "                        push_csv_to_gsheet(rule_path, worksheet_name_prefix=f\"RULE_REPLAY_{date_label}\")\n",
    "                except Exception as e:\n",
    "                    print(\"[replay][gs] push signals error:\", e)\n",
    "            # send telegram summary\n",
    "            if send_telegram:\n",
    "                top = None\n",
    "                if rule_path and os.path.exists(rule_path):\n",
    "                    topdf = pd.read_csv(rule_path)\n",
    "                    top = topdf.sort_values(\"rule_score\", ascending=False).head(TOPK)\n",
    "                msg = f\"üìÖ Replay {date_label} ‚Äî signals summary\\n\"\n",
    "                if top is None or top.empty:\n",
    "                    msg += \"No strong signals.\\n\"\n",
    "                else:\n",
    "                    for i, row in top.iterrows():\n",
    "                        msg += f\"{row['ticker']} ‚Äî rule {int(row['rule_score'])}\\n\"\n",
    "                send_telegram_text(msg)\n",
    "            summary_rows.append({\"date\": date_label, \"a3c\": a3c_path, \"rule\": rule_path, \"error\": None})\n",
    "        except Exception as e:\n",
    "            print(f\"[replay] exception for {date_label}\", e)\n",
    "            summary_rows.append({\"date\": date_label, \"a3c\": None, \"rule\": None, \"error\": str(e)})\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    out_summary = os.path.join(MONITOR_DIR, f\"replay_summary_{start_date.replace('-','')}_{end_date.replace('-','')}.csv\")\n",
    "    summary_df.to_csv(out_summary, index=False)\n",
    "    print(\"[replay] done. summary saved:\", out_summary)\n",
    "    return out_summary\n",
    "\n",
    "# ---------------------------\n",
    "# Main loop helper\n",
    "# ---------------------------\n",
    "def main_loop(interval_seconds: int = 6*60*60):\n",
    "    print(\"[main_loop] starting with interval\", interval_seconds)\n",
    "    while True:\n",
    "        try:\n",
    "            run_cycle()\n",
    "            print(f\"[main_loop] sleeping for {interval_seconds} seconds...\")\n",
    "            time.sleep(interval_seconds)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"[main_loop] stopped by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"[main_loop] exception:\", e)\n",
    "            traceback.print_exc()\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa924f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_cycle] universe size: 200\n",
      "[fetch] [Batch 1/4] Fetching 50 tickers...\n",
      "Connection established. Waiting for server processing...\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VIX\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.CII\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.SSI\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.GEX\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VPB\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VIC\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VHM\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.SHB\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.DXG\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.DIG\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VND\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VSC\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.PDR\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.MBB\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.NKG\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.ANV\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.BSR\n",
      "2025-09-21 16:12:36 Joined group: Realtime.Ticker.VJC\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.KBC\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.CTS\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.ORS\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.VDS\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.TPB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.STB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.HPG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.HDC\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.DXS\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.HDB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.MSN\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.SCR\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.GMD\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.HDG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.VCG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.MSB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.TAL\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.LDG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.TCH\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.DPG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.EVG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.TCB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.EVF\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.EIB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.CTG\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.LPB\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.NVL\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.PAN\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.DPM\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.IJC\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.CDC\n",
      "2025-09-21 16:12:37 Joined group: Realtime.Ticker.HSL\n",
      "Disconnecting...\n",
      "[fetch] [Batch 2/4] Fetching 50 tickers...\n",
      "Connection established. Waiting for server processing...\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.MWG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.OCB\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HCM\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.KDH\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.GEE\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.NAF\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HT1\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.SJS\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.KHG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.VIB\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.VAB\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.PET\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HHV\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.SBT\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.DHC\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HVN\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.ACB\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.MHC\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.E1VFVN30\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.CRE\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.DGW\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.KSB\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.LCG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HSG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.SMC\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HHS\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.VGC\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.VRE\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.VNE\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.NLG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.POW\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.PVD\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.TNI\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HAR\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.JVC\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.C47\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.NT2\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.HAG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.VCI\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.TCI\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.PC1\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.NNC\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.DCM\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.DSE\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.QCG\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.TNT\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.IDI\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.CSM\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.FUEVFVND\n",
      "2025-09-21 16:12:53 Joined group: Realtime.Ticker.CTI\n",
      "Disconnecting...\n",
      "[fetch] [Batch 3/4] Fetching 50 tickers...\n",
      "Connection established. Waiting for server processing...\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.FCN\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.AGR\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.FUEMAV30\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.NTL\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.HAH\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.VHC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.HPX\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DHA\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.BIC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.VCB\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.ITC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.VNM\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DBC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.PHR\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DSC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.AGG\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.ICT\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.FUESSVFL\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.YEG\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DLG\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.FUEVN100\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.ABT\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.TEG\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.BID\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.SAM\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.VPI\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.TN1\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.BVH\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.TTF\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.BSI\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.HTN\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.NHA\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.HQC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.TVB\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.CMG\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.HTI\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DCL\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.BBC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.BCM\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.CRC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.PAC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.SRC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DRH\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.AAA\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DGC\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.GIL\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.PVT\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.REE\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.DAH\n",
      "2025-09-21 16:13:09 Joined group: Realtime.Ticker.TCM\n",
      "Disconnecting...\n",
      "[fetch] [Batch 4/4] Fetching 50 tickers...\n",
      "Connection established. Waiting for server processing...\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.SVC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TDC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.VOS\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.PLP\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FUESSV50\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.GAS\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.ASM\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.PTL\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.DAT\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TTA\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TVS\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.ACL\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.VDP\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.NAB\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FTS\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.SVD\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.VPS\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FUESSV30\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.PNJ\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.PTC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FUEKIVFS\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.DTT\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TCD\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FUETCC50\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TLD\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.SZC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.GVR\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.PLX\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TLG\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FPT\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.LHG\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.LBM\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.LIX\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.LGL\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.BFC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.HTV\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TRC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.TIP\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FUEDCMID\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FUCVREIT\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.FIT\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.PMG\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.SSB\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.GEG\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.MIG\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.DPR\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.DBD\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.DRC\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.BMP\n",
      "2025-09-21 16:13:25 Joined group: Realtime.Ticker.HCD\n",
      "Disconnecting...\n",
      "[fetch] snapshot complete rows=0\n",
      "Connection established. Waiting for server processing...\n",
      "[gs] pushed ohlcv master to sheet: OHLCV_Master\n",
      "[features_master] saved timeseries & snapshot\n",
      "[a3c] used precomputed infer file -> ./signals\\a3c_signals_20250921_161354.csv\n",
      "[rule] saved: ./signals\\rule_scores_20250921_161359.csv\n",
      "[ddpg_hybrid] no active tickers -> no allocation\n",
      "[alerts] no trade alerts this run (HOLD summary sent if applicable)\n",
      "[monitor] no alloc to log\n",
      "[run_cycle] Done. Artifacts: {'incremental_csv': None, 'features_csv': './features\\\\features_today_20250921_161353.csv', 'a3c_csv': './signals\\\\a3c_signals_20250921_161354.csv', 'rule_csv': './signals\\\\rule_scores_20250921_161359.csv', 'alloc_path': None, 'top10_path': None, 'alloc_source': 'no_active', 'alert_file': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'incremental_csv': None,\n",
       " 'features_csv': './features\\\\features_today_20250921_161353.csv',\n",
       " 'a3c_csv': './signals\\\\a3c_signals_20250921_161354.csv',\n",
       " 'rule_csv': './signals\\\\rule_scores_20250921_161359.csv',\n",
       " 'alloc_path': None,\n",
       " 'top10_path': None,\n",
       " 'alloc_source': 'no_active',\n",
       " 'alert_file': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cycle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1938f22",
   "metadata": {},
   "source": [
    "**TEST TR√äN STRESS TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fe0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Backtest / Historical Replay standalone cell ===\n",
    "# Paste to√†n b·ªô cell n√†y v√†o 1 √¥ trong notebook v√† ch·∫°y.\n",
    "# M·∫∑c ƒë·ªãnh: s·∫Ω ch·∫°y replay t·ª´ START_DATE -> END_DATE v√† g·ª≠i Telegram m·ªói ng√†y.\n",
    "# N·∫øu mu·ªën ƒë·ªìng th·ªùi ghi Google Sheet, set PUSH_GSHEET = True (c·∫ßn service JSON & sheet)\n",
    "# Ng∆∞·ªùi d√πng c√≥ th·ªÉ ch·ªânh bi·∫øn c·∫•u h√¨nh b√™n d∆∞·ªõi\n",
    "\n",
    "import os, time, json, math, glob, traceback\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# ====== CONFIG (ch·ªânh ·ªü ƒë√¢y) ======\n",
    "FQ_USERNAME = \"DSTC_18@fiinquant.vn\"        # b·∫°n cung c·∫•p\n",
    "FQ_PASSWORD = \"Fiinquant0606\"              # b·∫°n cung c·∫•p\n",
    "\n",
    "# Google Sheets (optional)\n",
    "PUSH_GSHEET = False                         # default: False (khuy√™n b·∫≠t False khi debug)\n",
    "GS_SERVICE_JSON = \"dstround3.json\"          # service account JSON path\n",
    "GS_SHEET_KEY = \"17FRrF63TFE3bmAseoV4vQK5EA9nYaTaRRnLyd1F8MGU\"\n",
    "GS_SHEET_NAME = \"DSTRound3\"\n",
    "\n",
    "# Telegram config (edit if c·∫ßn)\n",
    "TG_TOKEN = \"8454050043:AAG_quR7eSALqh9WVRvx6DRZVxtRde_OpFQ\"\n",
    "TG_CHAT_ID = \"-1002692813170\"   # chat id as string, e.g. \"-2692813170\"\n",
    "TG_THREAD_ID = 2             # set thread id you want, change if needed\n",
    "\n",
    "# Replay period (stress test default)\n",
    "START_DATE = \"2025-03-25\"\n",
    "END_DATE   = \"2025-04-16\"\n",
    "\n",
    "# OHLCV backfill window length (if master not found)\n",
    "BACKFILL_MONTHS = 6   # fetch 6 months history if needed\n",
    "\n",
    "# Rule thresholds\n",
    "RSI_TH = 35\n",
    "VOL_SPIKE_MULT = 1.5\n",
    "RULE_MIN_SCORE = 3\n",
    "MIN_ALLOC = 0.005   # not used here, just for reference\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"./data\"\n",
    "FEATURE_DIR = \"./features\"\n",
    "SIGNAL_DIR = \"./signals\"\n",
    "MONITOR_DIR = \"./monitor\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "os.makedirs(SIGNAL_DIR, exist_ok=True)\n",
    "os.makedirs(MONITOR_DIR, exist_ok=True)\n",
    "\n",
    "# ===== Helpers: Telegram & Google Sheets =====\n",
    "def send_telegram_text(msg, thread_id=None):\n",
    "    url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendMessage\"\n",
    "    payload = {\"chat_id\": TG_CHAT_ID, \"text\": msg, \"parse_mode\": \"Markdown\"}\n",
    "    if thread_id is not None:\n",
    "        payload[\"message_thread_id\"] = int(thread_id)\n",
    "    try:\n",
    "        r = requests.post(url, json=payload, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[tg] send failed: {r.status_code} {r.text}\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[tg] exception:\", e)\n",
    "        return False\n",
    "\n",
    "def send_telegram_file(path, caption=\"\", thread_id=None):\n",
    "    url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendDocument\"\n",
    "    data = {\"chat_id\": TG_CHAT_ID, \"caption\": caption}\n",
    "    if thread_id is not None:\n",
    "        data[\"message_thread_id\"] = int(thread_id)\n",
    "    try:\n",
    "        with open(path,\"rb\") as fh:\n",
    "            files = {\"document\": fh}\n",
    "            r = requests.post(url, data=data, files=files, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            print(\"[tg] sendDocument failed:\", r.status_code, r.text)\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[tg] sendDocument exception:\", e)\n",
    "        return False\n",
    "\n",
    "# Google Sheets helper (optional)\n",
    "if PUSH_GSHEET:\n",
    "    try:\n",
    "        import gspread\n",
    "        from gspread_dataframe import set_with_dataframe\n",
    "        from google.oauth2.service_account import Credentials\n",
    "        GS_CREDS = Credentials.from_service_account_file(GS_SERVICE_JSON, scopes=[\n",
    "            \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "            \"https://www.googleapis.com/auth/drive\"\n",
    "        ])\n",
    "        GS_CLIENT = gspread.authorize(GS_CREDS)\n",
    "        GS_SHEET = GS_CLIENT.open_by_key(GS_SHEET_KEY)\n",
    "        try:\n",
    "            GS_WS = GS_SHEET.worksheet(GS_SHEET_NAME)\n",
    "        except Exception:\n",
    "            GS_WS = GS_SHEET.add_worksheet(title=GS_SHEET_NAME, rows=\"1000\", cols=\"20\")\n",
    "        print(\"[gs] Google Sheets ready:\", GS_SHEET_NAME)\n",
    "    except Exception as e:\n",
    "        print(\"[gs] init failed, disabling PUSH_GSHEET. Error:\", e)\n",
    "        PUSH_GSHEET = False\n",
    "\n",
    "def append_df_to_gsheet(df, worksheet, batch_size=500):\n",
    "    # Clean NaN -> empty string before push to avoid JSON NaN error\n",
    "    df2 = df.copy()\n",
    "    df2 = df2.replace([np.inf, -np.inf], np.nan).fillna(\"\")\n",
    "    # append rows in batches\n",
    "    rows = df2.values.tolist()\n",
    "    try:\n",
    "        # gspread append_rows may accept batches\n",
    "        for i in range(0, len(rows), batch_size):\n",
    "            chunk = rows[i:i+batch_size]\n",
    "            worksheet.append_rows(chunk, value_input_option=\"RAW\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[gs] append error:\", e)\n",
    "        return False\n",
    "\n",
    "# ===== FiinQuant client helpers =====\n",
    "from FiinQuantX import FiinSession, RealTimeData   # must be installed in environment\n",
    "\n",
    "def init_fiin_client():\n",
    "    try:\n",
    "        client = FiinSession(username=FQ_USERNAME, password=FQ_PASSWORD).login()\n",
    "        print(\"[init] Fiin client logged in\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(\"[init] Fiin login failed:\", e)\n",
    "        return None\n",
    "\n",
    "# TA builder using fiin indicator from client\n",
    "def add_ta_indicators_for_df(df, fi):\n",
    "    # df should have columns: timestamp, open, high, low, close, volume\n",
    "    df = df.sort_values(\"timestamp\").copy().reset_index(drop=True)\n",
    "    try:\n",
    "        df['ema_5'] = fi.ema(df['close'], window=5)\n",
    "        df['ema_20'] = fi.ema(df['close'], window=20)\n",
    "        df['ema_50'] = fi.ema(df['close'], window=50)\n",
    "        df['macd'] = fi.macd(df['close'], window_fast=12, window_slow=26)\n",
    "        df['macd_signal'] = fi.macd_signal(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "        df['macd_diff'] = df['macd'] - df['macd_signal']\n",
    "        df['rsi_14'] = fi.rsi(df['close'], window=14)\n",
    "        df['boll_up'] = fi.bollinger_hband(df['close'], window=20, window_dev=2)\n",
    "        df['boll_dn'] = fi.bollinger_lband(df['close'], window=20, window_dev=2)\n",
    "        df['atr_14'] = fi.atr(df['high'], df['low'], df['close'], window=14)\n",
    "        df['obv'] = fi.obv(df['close'], df['volume'])\n",
    "        df['vwap_20'] = fi.vwap(df['high'], df['low'], df['close'], df['volume'], window=20)\n",
    "        # volume z-score over 20\n",
    "        df['vol_z'] = (df['volume'] - df['volume'].rolling(20).mean()) / (df['volume'].rolling(20).std().replace(0, np.nan))\n",
    "    except Exception as e:\n",
    "        # If some fi functions not available or error, fill NaN\n",
    "        print(\"[TA] indicator compute error:\", e)\n",
    "    return df\n",
    "\n",
    "# ===== Backfill / ensure master OHLCV present =====\n",
    "def backfill_master(client, universe, months=BACKFILL_MONTHS):\n",
    "    master_path = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "    if os.path.exists(master_path):\n",
    "        print(\"[backfill] master exists:\", master_path)\n",
    "        return master_path\n",
    "    # compute from_date\n",
    "    to_date = datetime.now().date()\n",
    "    from_date = (datetime.now() - pd.DateOffset(months=months)).date()\n",
    "    print(f\"[backfill] requesting historical OHLCV for {len(universe)} tickers from {from_date}\")\n",
    "    # Fetch in batches (Fiin API may accept many tickers)\n",
    "    all_rows=[]\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(universe), batch_size):\n",
    "        batch = universe[i:i+batch_size]\n",
    "        try:\n",
    "            data = client.Fetch_Trading_Data(\n",
    "                realtime=False,\n",
    "                tickers=batch,\n",
    "                fields=['open','high','low','close','volume'],\n",
    "                adjusted=True,\n",
    "                by='1d',\n",
    "                from_date=str(from_date),\n",
    "                to_date=str(to_date)\n",
    "            ).get_data()\n",
    "            # data expected DataFrame with timestamp,ticker,close,...\n",
    "            if data is not None and not data.empty:\n",
    "                all_rows.append(data)\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(\"[backfill] batch fetch error:\", e)\n",
    "    if all_rows:\n",
    "        df_master = pd.concat(all_rows, ignore_index=True)\n",
    "        df_master.to_parquet(master_path, index=False)\n",
    "        print(f\"[backfill] saved master parquet {master_path} rows={len(df_master)}\")\n",
    "        return master_path\n",
    "    else:\n",
    "        print(\"[backfill] no data fetched\")\n",
    "        return None\n",
    "\n",
    "# ===== A3C inference fallback (if precomputed file not available) =====\n",
    "def a3c_fallback_signals_from_snapshot(snapshot_df):\n",
    "    # snapshot_df: one-row-per-ticker with TA columns\n",
    "    rows=[]\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for _, r in snapshot_df.iterrows():\n",
    "        ema5 = r.get('ema_5', np.nan); ema20 = r.get('ema_20', np.nan); rsi = r.get('rsi_14', np.nan)\n",
    "        buy = False\n",
    "        if not math.isnan(ema5) and not math.isnan(ema20) and ema5 > ema20 and (not math.isnan(rsi) and rsi < RSI_TH):\n",
    "            buy = True\n",
    "        action = 1 if buy else 0\n",
    "        pb, ph, ps = (0.75, 0.2, 0.05) if buy else (0.1, 0.85, 0.05)\n",
    "        rows.append([date_str, r['ticker'], int(action), float(pb), float(ph), float(ps)])\n",
    "    df = pd.DataFrame(rows, columns=[\"date\",\"ticker\",\"action\",\"prob_buy\",\"prob_hold\",\"prob_sell\"])\n",
    "    return df\n",
    "\n",
    "# ===== Rule scoring (per snapshot) =====\n",
    "def rule_scoring_from_snapshot(snapshot_df):\n",
    "    rows=[]\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for _, r in snapshot_df.iterrows():\n",
    "        ticker = r['ticker']\n",
    "        ema5 = float(r.get('ema_5', np.nan)) if not pd.isna(r.get('ema_5', np.nan)) else np.nan\n",
    "        ema20 = float(r.get('ema_20', np.nan)) if not pd.isna(r.get('ema_20', np.nan)) else np.nan\n",
    "        rsi = float(r.get('rsi_14', np.nan)) if not pd.isna(r.get('rsi_14', np.nan)) else np.nan\n",
    "        macd_diff = float(r.get('macd_diff', np.nan)) if not pd.isna(r.get('macd_diff', np.nan)) else np.nan\n",
    "        close = float(r.get('close', np.nan)) if not pd.isna(r.get('close', np.nan)) else np.nan\n",
    "        boll_up = float(r.get('boll_up', np.nan)) if not pd.isna(r.get('boll_up', np.nan)) else np.nan\n",
    "        vol_z = float(r.get('vol_z', np.nan)) if not pd.isna(r.get('vol_z', np.nan)) else np.nan\n",
    "\n",
    "        score=0; pass_list=[]; fail_list=[]\n",
    "        if not math.isnan(ema5) and not math.isnan(ema20) and ema5 > ema20:\n",
    "            score += 1; pass_list.append(\"EMA5>EMA20\")\n",
    "        else:\n",
    "            fail_list.append(\"EMA<=20\")\n",
    "        if not math.isnan(rsi) and rsi < RSI_TH:\n",
    "            score += 1; pass_list.append(\"RSI<TH\")\n",
    "        else:\n",
    "            fail_list.append(\"RSI>=\")\n",
    "        if not math.isnan(macd_diff) and macd_diff > 0:\n",
    "            score += 1; pass_list.append(\"MACD>0\")\n",
    "        else:\n",
    "            fail_list.append(\"MACD<=0\")\n",
    "        if not math.isnan(close) and not math.isnan(boll_up) and close > boll_up:\n",
    "            score += 1; pass_list.append(\"BollBreak\")\n",
    "        else:\n",
    "            fail_list.append(\"BollNo\")\n",
    "        if not math.isnan(vol_z) and vol_z > VOL_SPIKE_MULT:\n",
    "            score += 1; pass_list.append(\"VolSpike\")\n",
    "        else:\n",
    "            fail_list.append(\"VolNormal\")\n",
    "\n",
    "        details = {\"ema5\":ema5,\"ema20\":ema20,\"rsi\":rsi,\"macd_diff\":macd_diff,\"close\":close,\"boll_up\":boll_up,\"vol_z\":vol_z}\n",
    "        rows.append([date_str, ticker, int(score), \";\".join(pass_list), \";\".join(fail_list), json.dumps(details)])\n",
    "    df = pd.DataFrame(rows, columns=[\"date\",\"ticker\",\"rule_score\",\"pass_list\",\"fail_list\",\"detail_values\"])\n",
    "    return df\n",
    "\n",
    "# ===== Single-day processing (core of replay) =====\n",
    "def process_one_replay_day(client, day_date, universe, master_df=None, use_precomputed_a3c=False):\n",
    "    \"\"\"\n",
    "    day_date: pd.Timestamp or 'YYYY-MM-DD'\n",
    "    universe: list of tickers\n",
    "    master_df: full ohlcv_master DataFrame (optional, if provided reuse)\n",
    "    use_precomputed_a3c: if True and signals/a3c_signals_infer.csv present, will use it.\n",
    "    Returns: dict with paths/dataframes for a3c, rule, snapshot\n",
    "    \"\"\"\n",
    "    day = pd.to_datetime(day_date).date()\n",
    "    date_str = day.strftime(\"%Y-%m-%d\")\n",
    "    print(f\"[replay] running for {date_str}\")\n",
    "\n",
    "    # 1) prepare price data for day: we will derive snapshot = last available row <= day for each ticker\n",
    "    master_path = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "    if master_df is None:\n",
    "        if not os.path.exists(master_path):\n",
    "            # try backfill\n",
    "            print(\"[replay] ohlcv_master.parquet not found. Backfilling history...\")\n",
    "            backfill_master(client, universe, months=BACKFILL_MONTHS)\n",
    "        if not os.path.exists(master_path):\n",
    "            print(\"[replay] master missing -> abort day\")\n",
    "            return {\"error\":\"master_missing\"}\n",
    "        master_df = pd.read_parquet(master_path)\n",
    "        master_df[\"timestamp\"] = pd.to_datetime(master_df[\"timestamp\"])\n",
    "\n",
    "    # filter rows up to day (inclusive)\n",
    "    df_up_to = master_df[master_df[\"timestamp\"].dt.date <= day].copy()\n",
    "    if df_up_to.empty:\n",
    "        print(\"[replay] no historical rows <= day\")\n",
    "        return {\"error\":\"no_rows\"}\n",
    "\n",
    "    # Build snapshot: last row per ticker\n",
    "    last_rows = df_up_to.sort_values([\"ticker\",\"timestamp\"]).groupby(\"ticker\").tail(1).reset_index(drop=True)\n",
    "    # compute TA per ticker: we may need timeseries longer than 1 row to compute indicators.\n",
    "    # For robust TA we compute per-ticker on its timeseries up to day\n",
    "    snapshot_frames=[]\n",
    "    fi = client.FiinIndicator()  # IMPORTANT: use client.FiinIndicator() per your correct sample\n",
    "    for tk in universe:\n",
    "        tk_df = df_up_to[df_up_to[\"ticker\"]==tk].sort_values(\"timestamp\").copy()\n",
    "        if tk_df.empty:\n",
    "            continue\n",
    "        tk_df = add_ta_indicators_for_df(tk_df, fi)\n",
    "        # pick last row (snapshot)\n",
    "        last = tk_df.tail(1).copy()\n",
    "        last[\"ticker\"] = tk\n",
    "        snapshot_frames.append(last)\n",
    "    if not snapshot_frames:\n",
    "        print(\"[replay] no ticker snapshots computed\")\n",
    "        return {\"error\":\"no_snapshots\"}\n",
    "    snapshot_df = pd.concat(snapshot_frames, ignore_index=True)\n",
    "    # Keep only desired columns and ensure 'ticker' present\n",
    "    if \"ticker\" not in snapshot_df.columns:\n",
    "        snapshot_df[\"ticker\"] = snapshot_df.get(\"ticker\", np.nan)\n",
    "    # Save snapshot CSV for debugging\n",
    "    snap_path = os.path.join(FEATURE_DIR, f\"features_today_{date_str}.csv\")\n",
    "    snapshot_df.to_csv(snap_path, index=False)\n",
    "\n",
    "    # 2) A3C inference: try precomputed file first\n",
    "    a3c_out = None\n",
    "    if use_precomputed_a3c:\n",
    "        infer_files = sorted(glob.glob(os.path.join(SIGNAL_DIR, \"a3c_signals_*.csv\")))\n",
    "        if infer_files:\n",
    "            # choose the latest precomputed file and filter by date if available\n",
    "            a3c_df_pre = pd.read_csv(infer_files[-1])\n",
    "            if 'date' in a3c_df_pre.columns:\n",
    "                a3c_df_pre['date'] = pd.to_datetime(a3c_df_pre['date']).dt.strftime(\"%Y-%m-%d\")\n",
    "                a3c_sel = a3c_df_pre[a3c_df_pre['date'] == date_str].copy()\n",
    "                if not a3c_sel.empty:\n",
    "                    a3c_out = a3c_sel\n",
    "    if a3c_out is None:\n",
    "        # fallback heuristic\n",
    "        a3c_out = a3c_fallback_signals_from_snapshot(snapshot_df)\n",
    "    # save a3c signals\n",
    "    a3c_path = os.path.join(SIGNAL_DIR, f\"a3c_signals_{date_str}.csv\")\n",
    "    a3c_out.to_csv(a3c_path, index=False)\n",
    "\n",
    "    # 3) rule scoring\n",
    "    rule_df = rule_scoring_from_snapshot(snapshot_df)\n",
    "    rule_path = os.path.join(SIGNAL_DIR, f\"rule_scores_{date_str}.csv\")\n",
    "    rule_df.to_csv(rule_path, index=False)\n",
    "\n",
    "    # 4) Merge and prepare alerts summary (no execution)\n",
    "    merged = a3c_out.merge(rule_df, on=[\"date\",\"ticker\"], how=\"left\")\n",
    "    merged[\"rule_score\"] = merged[\"rule_score\"].fillna(0).astype(int)\n",
    "\n",
    "    # 5) Build alert list per our decision rules (A3C==1 & rule_score >= RULE_MIN_SCORE)\n",
    "    alerts = merged[(merged[\"action\"]==1) & (merged[\"rule_score\"] >= RULE_MIN_SCORE)].copy()\n",
    "    # If no alerts but you want rule-only alerts, create those where rule_score >= RULE_MIN_SCORE\n",
    "    # (we will still send hold summary if no alerts)\n",
    "    return {\n",
    "        \"date\": date_str,\n",
    "        \"snapshot_path\": snap_path,\n",
    "        \"a3c_path\": a3c_path,\n",
    "        \"rule_path\": rule_path,\n",
    "        \"snapshot_df\": snapshot_df,\n",
    "        \"a3c_df\": a3c_out,\n",
    "        \"rule_df\": rule_df,\n",
    "        \"merged\": merged,\n",
    "        \"alerts\": alerts\n",
    "    }\n",
    "\n",
    "# ===== Replay driver: run across date range and send telegram (and optional GSheet) =====\n",
    "def run_historical_replay(start_date=START_DATE, end_date=END_DATE, push_gsheet=PUSH_GSHEET, send_telegram=True, use_precomputed_a3c=True, universe_override=None, allow_rule_only=False):\n",
    "    client = init_fiin_client()\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"Fiin login failed; cannot proceed\")\n",
    "\n",
    "    # Determine universe: read universe/universe_list.csv if exists, else use top tickers from master\n",
    "    universe_path = os.path.join(\"universe\",\"universe_list.csv\")\n",
    "    universe=[]\n",
    "    if universe_override:\n",
    "        universe = universe_override\n",
    "    elif os.path.exists(universe_path):\n",
    "        try:\n",
    "            df_univ = pd.read_csv(universe_path)\n",
    "            universe = df_univ[\"ticker\"].astype(str).tolist()\n",
    "        except Exception:\n",
    "            universe = []\n",
    "    # fallback: take top 200 tickers present in ohlcv_master\n",
    "    master_path = os.path.join(DATA_DIR, \"ohlcv_master.parquet\")\n",
    "    if not universe and os.path.exists(master_path):\n",
    "        px = pd.read_parquet(master_path)\n",
    "        universe = px[\"ticker\"].unique().tolist()[:200]\n",
    "    if not universe:\n",
    "        raise RuntimeError(\"Universe empty: please provide universe_list.csv or ensure ohlcv_master.parquet is present\")\n",
    "\n",
    "    # Preload master once to speed up\n",
    "    master_df = None\n",
    "    if os.path.exists(master_path):\n",
    "        master_df = pd.read_parquet(master_path)\n",
    "        master_df[\"timestamp\"] = pd.to_datetime(master_df[\"timestamp\"])\n",
    "\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    day = start\n",
    "    summary_rows=[]\n",
    "    while day <= end:\n",
    "        try:\n",
    "            res = process_one_replay_day(client, day, universe, master_df=master_df, use_precomputed_a3c=use_precomputed_a3c)\n",
    "            if res.get(\"error\"):\n",
    "                print(f\"[replay] error for {day.date()}: {res.get('error')}\")\n",
    "                summary_rows.append({\"date\": day.strftime(\"%Y-%m-%d\"), \"status\": \"error\", \"note\": res.get(\"error\")})\n",
    "                day += pd.Timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            merged = res[\"merged\"]\n",
    "            alerts = res[\"alerts\"]\n",
    "\n",
    "            # Build telegram message\n",
    "            date_label = res[\"date\"]\n",
    "            if not alerts.empty:\n",
    "                # list alerts\n",
    "                lines=[]\n",
    "                for _, r in alerts.iterrows():\n",
    "                    t = r[\"ticker\"]\n",
    "                    a3c_label = {1:\"BUY\",0:\"HOLD\",-1:\"SELL\"}.get(int(r.get(\"action\",0)),\"HOLD\")\n",
    "                    score = int(r.get(\"rule_score\",0))\n",
    "                    lines.append(f\"- {t} | A3C:{a3c_label} | rule:{score}\")\n",
    "                body = \"\\n\".join(lines)\n",
    "                msg = f\"üü¢ *REPLAY ALERTS* {date_label}\\n{body}\\n(Replay log)\"\n",
    "                if send_telegram:\n",
    "                    ok = send_telegram_text(msg, thread_id=TG_THREAD_ID)\n",
    "                    if not ok:\n",
    "                        # try without thread id (compatibility)\n",
    "                        send_telegram_text(msg, thread_id=None)\n",
    "                summary_rows.append({\"date\": date_label, \"status\":\"alerts_sent\", \"n_alerts\": len(alerts)})\n",
    "            else:\n",
    "                # no alerts -> send HOLD summary or rule-only if allowed\n",
    "                if allow_rule_only:\n",
    "                    rule_candidates = merged[merged[\"rule_score\"] >= RULE_MIN_SCORE].sort_values(\"rule_score\", ascending=False).head(10)\n",
    "                    if not rule_candidates.empty:\n",
    "                        lines=[]\n",
    "                        for i, r in enumerate(rule_candidates.itertuples(),1):\n",
    "                            lines.append(f\"{i}. {r.ticker} ‚Äî rule {int(r.rule_score)}\")\n",
    "                        msg = f\"üü° *REPLAY RULE-ONLY SUMMARY* {date_label}\\nTop {len(rule_candidates)}:\\n\" + \"\\n\".join(lines)\n",
    "                        if send_telegram:\n",
    "                            ok = send_telegram_text(msg, thread_id=TG_THREAD_ID)\n",
    "                            if not ok:\n",
    "                                send_telegram_text(msg, thread_id=None)\n",
    "                        summary_rows.append({\"date\": date_label, \"status\":\"rule_only_sent\", \"n_candidates\": len(rule_candidates)})\n",
    "                    else:\n",
    "                        msg = f\"üü° *REPLAY HOLD* {date_label} ‚Äî no signals.\"\n",
    "                        if send_telegram:\n",
    "                            send_telegram_text(msg, thread_id=TG_THREAD_ID)\n",
    "                        summary_rows.append({\"date\": date_label, \"status\":\"hold\", \"n_candidates\": 0})\n",
    "                else:\n",
    "                    msg = f\"üü° *REPLAY HOLD* {date_label} ‚Äî no signals.\"\n",
    "                    if send_telegram:\n",
    "                        send_telegram_text(msg, thread_id=TG_THREAD_ID)\n",
    "                    summary_rows.append({\"date\": date_label, \"status\":\"hold\", \"n_candidates\": 0})\n",
    "\n",
    "            # Optional: push snapshot or alerts to Google Sheets (if turned on)\n",
    "            if push_gsheet:\n",
    "                try:\n",
    "                    # push merged (select columns to avoid huge sheet)\n",
    "                    push_df = merged[[\"date\",\"ticker\",\"action\",\"prob_buy\",\"rule_score\"]].copy()\n",
    "                    append_df_to_gsheet(push_df, GS_WS)\n",
    "                except Exception as e:\n",
    "                    print(\"[gs] append error:\", e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[replay] exception for\", day.strftime(\"%Y-%m-%d\"), e)\n",
    "            traceback.print_exc()\n",
    "            summary_rows.append({\"date\": day.strftime(\"%Y-%m-%d\"), \"status\":\"exception\", \"note\": str(e)})\n",
    "\n",
    "        day += pd.Timedelta(days=1)\n",
    "\n",
    "    # Save replay summary CSV\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_path = os.path.join(MONITOR_DIR, f\"replay_summary_{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(\"[replay] done. summary saved:\", summary_path)\n",
    "    return summary_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2de8e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Fiin client logged in\n",
      "[replay] running for 2025-03-25\n",
      "[replay] running for 2025-03-26\n",
      "[replay] running for 2025-03-27\n",
      "[replay] running for 2025-03-28\n",
      "[replay] running for 2025-03-29\n",
      "[replay] running for 2025-03-30\n",
      "[replay] running for 2025-03-31\n",
      "[replay] running for 2025-04-01\n",
      "[replay] running for 2025-04-02\n",
      "[replay] running for 2025-04-03\n",
      "[replay] running for 2025-04-04\n",
      "[replay] running for 2025-04-05\n",
      "[replay] running for 2025-04-06\n",
      "[replay] running for 2025-04-07\n",
      "[replay] running for 2025-04-08\n",
      "[replay] running for 2025-04-09\n",
      "[replay] running for 2025-04-10\n",
      "[replay] running for 2025-04-11\n",
      "[replay] running for 2025-04-12\n",
      "[replay] running for 2025-04-13\n",
      "[replay] running for 2025-04-14\n",
      "[replay] running for 2025-04-15\n",
      "[replay] running for 2025-04-16\n",
      "[replay] done. summary saved: ./monitor\\replay_summary_20250325_20250416.csv\n"
     ]
    }
   ],
   "source": [
    "summary = run_historical_replay(start_date=START_DATE, end_date=END_DATE, push_gsheet=PUSH_GSHEET, send_telegram=True, use_precomputed_a3c=True, allow_rule_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
