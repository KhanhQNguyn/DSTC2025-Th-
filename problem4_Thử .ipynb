{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4d6cad",
   "metadata": {},
   "source": [
    "**HƯỚNG DẪN CHẠY**\n",
    "\n",
    "*Nhóm chạy code theo thứ tự từng cell từ trên xuống xuống dưới*\n",
    "\n",
    "**Một số điểm lưu ý:**\n",
    "\n",
    "- *Thời gian chạy các block đa số lâu (Khoảng 10 phút riêng block 6 khoảng 25 phút)*\n",
    "\n",
    "- *Các block 8,9,10,11 có lưu kết quả file csv* \n",
    "\n",
    "- **Các file csv kết quả nhóm có upload lên github**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bbe97",
   "metadata": {},
   "source": [
    "**Tải các thư viện cần thiết** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch scikit-learn matplotlib\n",
    "!pip install --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx\n",
    "!pip install --upgrade --extra-index-url https://fiinquant.github.io/fiinquantx/simple fiinquantx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565db80b",
   "metadata": {},
   "source": [
    "**Block 1: tải dữ liệu lịch sử và realtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a2bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mã HOSE: 413\n",
      "Fetching data, it may take a while. Please wait...\n",
      "History ban đầu:   ticker         timestamp      open      high       low     close     volume  \\\n",
      "0    AAA  2023-01-03 00:00  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA  2023-01-04 00:00  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA  2023-01-05 00:00  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA  2023-01-06 00:00  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA  2023-01-09 00:00  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs           fn  \n",
      "0  938600.0  504700.0   40579000.0  899404000.0  \n",
      "1  462900.0  780600.0  151639000.0   36850000.0  \n",
      "2  487200.0  473700.0  343911000.0  -59103000.0  \n",
      "3  564300.0  828300.0  345999000.0 -294312000.0  \n",
      "4  414000.0  631800.0  514557000.0 -483197000.0  \n"
     ]
    }
   ],
   "source": [
    "# Block 1 — Login & Lấy dữ liệu tất cả HOSE/HNX/UPCOM\n",
    "import pandas as pd\n",
    "from FiinQuantX import FiinSession, BarDataUpdate\n",
    "# --- Login ---\n",
    "username = \"DSTC_18@fiinquant.vn\"\n",
    "password = \"Fiinquant0606\"\n",
    "\n",
    "client = FiinSession(\n",
    "    username=username,\n",
    "    password=password\n",
    ").login()\n",
    "\n",
    "# --- Lấy danh sách cổ phiếu từng sàn ---\n",
    "tickers_hose  = list(client.TickerList(ticker=\"VNINDEX\"))     # HOSE\n",
    "print(f\"Số mã HOSE: {len(tickers_hose)}\")\n",
    "\n",
    "# --- Lấy dữ liệu lịch sử toàn bộ ---\n",
    "event_history = client.Fetch_Trading_Data(\n",
    "    realtime=False,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    from_date=\"2023-01-01\"   # Lấy dữ liệu từ 2023 tới nay\n",
    ")\n",
    "\n",
    "df_all = event_history.get_data()\n",
    "print(\"History ban đầu:\", df_all.head())\n",
    "\n",
    "# --- Callback realtime ---\n",
    "def onDataUpdate(data: BarDataUpdate):\n",
    "    global df_all\n",
    "    df_update = data.to_dataFrame()\n",
    "    df_all = pd.concat([df_all, df_update])\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    print(\"Realtime update:\")\n",
    "    print(df_update.head())\n",
    "\n",
    "# --- Bật realtime nối tiếp dữ liệu ---\n",
    "event_realtime = client.Fetch_Trading_Data(\n",
    "    realtime=True,\n",
    "    tickers=tickers_hose,\n",
    "    fields=['open','high','low','close','volume','bu','sd','fs','fn'], \n",
    "    adjusted=True,\n",
    "    by=\"1d\",\n",
    "    period=1,\n",
    "    callback=onDataUpdate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1122224",
   "metadata": {},
   "source": [
    "**Block 2: lấy dữ liệu FA, lọc các mã không hợp lệ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c93b463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Lỗi khi lấy FA cho FUETPVND: 'FUETPVND'\n",
      "Số mã HOSE ban đầu: 413\n",
      "Số mã có dữ liệu FA: 391\n",
      "FA Data sample:\n",
      "   organizationId ticker  year  quarter  \\\n",
      "0          894364    CCC  2023        4   \n",
      "1          894364    CCC  2024        1   \n",
      "2          894364    CCC  2024        2   \n",
      "3          894364    CCC  2024        3   \n",
      "4          894364    CCC  2024        4   \n",
      "\n",
      "                                              ratios ReportDate  \n",
      "0  {'SolvencyRatio': {'DebtToEquityRatio': 1.5102...        NaT  \n",
      "1  {'SolvencyRatio': {'DebtToEquityRatio': 0.7722...        NaT  \n",
      "2  {'SolvencyRatio': {'DebtToEquityRatio': 0.7357...        NaT  \n",
      "3  {'SolvencyRatio': {'DebtToEquityRatio': 0.7914...        NaT  \n",
      "4  {'SolvencyRatio': {'DebtToEquityRatio': 0.6437...        NaT  \n"
     ]
    }
   ],
   "source": [
    "# Block 2 — Lấy dữ liệu FA theo quý (HOSE only)\n",
    "\n",
    "def fetch_fa_quarterly(ticker, latest_year=2025, n_periods=32):\n",
    "    try:\n",
    "        fi_list = client.FundamentalAnalysis().get_ratios(\n",
    "            tickers=[ticker],\n",
    "            TimeFilter=\"Quarterly\",\n",
    "            LatestYear=latest_year,\n",
    "            NumberOfPeriod=n_periods,\n",
    "            Consolidated=True\n",
    "        )\n",
    "\n",
    "        # Nếu không có dữ liệu thì bỏ qua\n",
    "        if not fi_list or not isinstance(fi_list, list):\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(fi_list)\n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df[\"ticker\"] = ticker\n",
    "        if \"ReportDate\" in df.columns:\n",
    "            df[\"ReportDate\"] = pd.to_datetime(df[\"ReportDate\"])\n",
    "        else:\n",
    "            # Nếu không có ReportDate thì tạo cột null để tránh lỗi concat\n",
    "            df[\"ReportDate\"] = pd.NaT\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi lấy FA cho {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- Lọc danh sách: chỉ giữ những mã có dữ liệu FA ---\n",
    "fa_list = []\n",
    "valid_tickers = []\n",
    "\n",
    "for t in tickers_hose:   # lấy theo danh sách HOSE từ Block 1\n",
    "    df_fa = fetch_fa_quarterly(t, latest_year=2025, n_periods=32)\n",
    "    if not df_fa.empty:\n",
    "        fa_list.append(df_fa)\n",
    "        valid_tickers.append(t)\n",
    "\n",
    "# --- Gộp DataFrame ---\n",
    "if fa_list:\n",
    "    fa_data = pd.concat(fa_list, ignore_index=True)\n",
    "else:\n",
    "    fa_data = pd.DataFrame()\n",
    "\n",
    "print(f\"Số mã HOSE ban đầu: {len(tickers_hose)}\")\n",
    "print(f\"Số mã có dữ liệu FA: {len(valid_tickers)}\")\n",
    "print(\"FA Data sample:\")\n",
    "print(fa_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9ef83",
   "metadata": {},
   "source": [
    "**Block 3: Chuẩn hóa FA và gộp dữ liệu với giá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "831a3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample merged:\n",
      "  ticker  timestamp      open      high       low     close     volume  \\\n",
      "0    AAA 2023-01-03  6539.643  6866.145  6539.643  6866.145  1543984.0   \n",
      "1    AAA 2023-01-04  6866.145  7000.587  6827.733  6827.733  1302505.0   \n",
      "2    AAA 2023-01-05  6866.145  6904.557  6808.527  6885.351   980473.0   \n",
      "3    AAA 2023-01-06  6885.351  6990.984  6818.130  6856.542  1431699.0   \n",
      "4    AAA 2023-01-09  6914.160  6962.175  6760.512  6789.321  1121385.0   \n",
      "\n",
      "         bu        sd           fs  ...  DebtToEquityRatio  EBITMargin  \\\n",
      "0  938600.0  504700.0   40579000.0  ...           0.507521   -0.049731   \n",
      "1  462900.0  780600.0  151639000.0  ...           0.507521   -0.049731   \n",
      "2  487200.0  473700.0  343911000.0  ...           0.507521   -0.049731   \n",
      "3  564300.0  828300.0  345999000.0  ...           0.507521   -0.049731   \n",
      "4  414000.0  631800.0  514557000.0  ...           0.507521   -0.049731   \n",
      "\n",
      "        ROA       ROE      ROIC    BasicEPS  PriceToBook  PriceToEarning  \\\n",
      "0  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "1  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "2  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "3  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "4  0.014669  0.029589  0.014784 -255.644791     0.737556       24.741322   \n",
      "\n",
      "   NetRevenueGrowthYoY  GrossProfitGrowthYoY  \n",
      "0             -0.18869             -0.910016  \n",
      "1             -0.18869             -0.910016  \n",
      "2             -0.18869             -0.910016  \n",
      "3             -0.18869             -0.910016  \n",
      "4             -0.18869             -0.910016  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Số mã merge thành công: 391\n"
     ]
    }
   ],
   "source": [
    "# Block 3 — Chuẩn hoá FA + Merge với giá (HOSE only, dựa theo Block 2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Các chỉ số FA cần lấy ---\n",
    "fa_fields = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "]\n",
    "\n",
    "# --- Hàm nổ ratios ---\n",
    "def explode_ratios(df, fa_fields):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        d = {\n",
    "            \"ticker\": row[\"ticker\"],\n",
    "            \"fa_year\": int(row[\"year\"]),\n",
    "            \"fa_quarter\": int(row[\"quarter\"])\n",
    "        }\n",
    "        ratios = row.get(\"ratios\", {})\n",
    "        if isinstance(ratios, dict):   # ✅ fix chỗ lỗi\n",
    "            for f in fa_fields:\n",
    "                val = None\n",
    "                for section in ratios.values():\n",
    "                    if isinstance(section, dict) and f in section:\n",
    "                        val = section[f]\n",
    "                d[f] = val\n",
    "        else:\n",
    "            # nếu ratios không phải dict thì gán NaN hết\n",
    "            for f in fa_fields:\n",
    "                d[f] = None\n",
    "        records.append(d)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --- Chuẩn hoá FA ---\n",
    "fa_clean = explode_ratios(fa_data, fa_fields)\n",
    "\n",
    "# --- Chuẩn hoá giá ---\n",
    "df_price = df_all[df_all[\"ticker\"].isin(valid_tickers)].copy()\n",
    "df_price[\"timestamp\"] = pd.to_datetime(df_price[\"timestamp\"])\n",
    "df_price = df_price.sort_values([\"ticker\",\"timestamp\"])\n",
    "\n",
    "# tạo key (fa_year, fa_quarter) = quý trước\n",
    "pi = df_price[\"timestamp\"].dt.to_period(\"Q\")\n",
    "prev_pi = pi - 1\n",
    "df_price[\"fa_year\"] = prev_pi.dt.year.astype(int)\n",
    "df_price[\"fa_quarter\"] = prev_pi.dt.quarter.astype(int)\n",
    "\n",
    "# --- Xử lý FA: giữ duy nhất bản cuối cùng mỗi quý\n",
    "fa_clean = (\n",
    "    fa_clean.sort_values([\"ticker\",\"fa_year\",\"fa_quarter\"])\n",
    "            .drop_duplicates(subset=[\"ticker\",\"fa_year\",\"fa_quarter\"], keep=\"last\")\n",
    ")\n",
    "\n",
    "# --- Merge giá + FA ---\n",
    "df_merged = df_price.merge(\n",
    "    fa_clean,\n",
    "    on=[\"ticker\",\"fa_year\",\"fa_quarter\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# FFill theo thời gian trong từng ticker để lấp chỗ trống\n",
    "df_merged = df_merged.sort_values([\"ticker\",\"timestamp\"])\n",
    "df_merged[fa_fields] = df_merged.groupby(\"ticker\")[fa_fields].ffill()\n",
    "\n",
    "print(\"Sample merged:\")\n",
    "print(df_merged.head())\n",
    "print(\"Số mã merge thành công:\", df_merged[\"ticker\"].nunique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08faae38",
   "metadata": {},
   "source": [
    "**Xóa biến df_all không cần thiết nữa để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331523dd",
   "metadata": {},
   "source": [
    "**Block 4: Tính các chỉ số TA dựa vào thư viện FiinQuant và ghép dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d43b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with TA + Regime:\n",
      "  ticker  timestamp     close  ma9  ma21  adx    ret_3d   regime\n",
      "0    AAA 2023-01-03  6866.145  NaN   NaN  NaN       NaN  sideway\n",
      "1    AAA 2023-01-04  6827.733  NaN   NaN  NaN       NaN  sideway\n",
      "2    AAA 2023-01-05  6885.351  NaN   NaN  NaN       NaN  sideway\n",
      "3    AAA 2023-01-06  6856.542  NaN   NaN  NaN -0.139860  sideway\n",
      "4    AAA 2023-01-09  6789.321  NaN   NaN  NaN -0.562588  sideway\n",
      "Shape sau khi thêm TA: (263580, 40)\n"
     ]
    }
   ],
   "source": [
    "# Block 4 — Tính các chỉ số TA + Regime (trên df_merged từ Block 3)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Khởi tạo Indicator ---\n",
    "fi = client.FiinIndicator()\n",
    "\n",
    "# --- Hàm tính TA theo từng ticker ---\n",
    "def add_ta_indicators(df):\n",
    "    df = df.sort_values(\"timestamp\").copy()\n",
    "    df = df.reset_index(drop = True)\n",
    "    # EMA\n",
    "    df['ema_5']  = fi.ema(df['close'], window=5)\n",
    "    df['ema_20'] = fi.ema(df['close'], window=20)\n",
    "    df['ema_50'] = fi.ema(df['close'], window=50)\n",
    "\n",
    "    # MACD\n",
    "    df['macd']        = fi.macd(df['close'], window_fast=12, window_slow=26)\n",
    "    df['macd_signal'] = fi.macd_signal(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "    df['macd_diff']   = fi.macd_diff(df['close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "\n",
    "    # RSI\n",
    "    df['rsi'] = fi.rsi(df['close'], window=14)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df['bollinger_hband'] = fi.bollinger_hband(df['close'], window=20, window_dev=2)\n",
    "    df['bollinger_lband'] = fi.bollinger_lband(df['close'], window=20, window_dev=2)\n",
    "\n",
    "    # ATR\n",
    "    df['atr'] = fi.atr(df['high'], df['low'], df['close'], window=14)\n",
    "\n",
    "    # OBV\n",
    "    df['obv'] = fi.obv(df['close'], df['volume'])\n",
    "\n",
    "    # VWAP\n",
    "    df['vwap'] = fi.vwap(df['high'], df['low'], df['close'], df['volume'], window=14)\n",
    "\n",
    "    # ----------------- Chỉ báo cho Regime -----------------\n",
    "    df['ma9']  = df['close'].rolling(window=9).mean()\n",
    "    df['ma21'] = df['close'].rolling(window=21).mean()\n",
    "    df['adx']  = fi.adx(df['high'], df['low'], df['close'], window=14)\n",
    "    df['ret_3d'] = df['close'].pct_change(periods=3) * 100\n",
    "\n",
    "    # Regime: bull/bear/sideway\n",
    "    cond_bull = (df['ma9'] > df['ma21']) & (df['adx'] > 20)\n",
    "    cond_bear = ((df['ma9'] < df['ma21']) & (df['adx'] > 20)) | (df['ret_3d'] < -7)\n",
    "    df['regime'] = np.where(cond_bull, 'bull', np.where(cond_bear, 'bear', 'sideway'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Áp dụng cho toàn bộ df_merged ---\n",
    "df_with_ta = df_merged.groupby(\"ticker\", group_keys=False).apply(add_ta_indicators)\n",
    "\n",
    "print(\"Sample with TA + Regime:\")\n",
    "print(df_with_ta[['ticker','timestamp','close','ma9','ma21','adx','ret_3d','regime']].head())\n",
    "print(\"Shape sau khi thêm TA:\", df_with_ta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e424a305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regime distribution (overall) ===\n",
      "regime\n",
      "sideway    98723\n",
      "bull       90301\n",
      "bear       74556\n",
      "Name: count, dtype: int64\n",
      "regime\n",
      "sideway    37.45\n",
      "bull       34.26\n",
      "bear       28.29\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Regime by ticker (top 5) ===\n",
      "regime  bear  bull  sideway\n",
      "ticker                     \n",
      "AAA      210   264      207\n",
      "AAM      218   104      359\n",
      "AAT      235   194      252\n",
      "ABR      225   201      255\n",
      "ABS      224   166      291\n"
     ]
    }
   ],
   "source": [
    "# --- Lưu kết quả Block 4 ---\n",
    "#df_with_ta.to_parquet(\"df_with_ta.parquet\", index=False)\n",
    "df_with_ta.to_csv(\"df_with_ta.csv\", index=False)\n",
    "\n",
    "# --- Thống kê phân phối regime ---\n",
    "print(\"=== Regime distribution (overall) ===\")\n",
    "print(df_with_ta['regime'].value_counts(normalize=False))\n",
    "print(df_with_ta['regime'].value_counts(normalize=True).mul(100).round(2))\n",
    "\n",
    "print(\"\\n=== Regime by ticker (top 5) ===\")\n",
    "print(df_with_ta.groupby(\"ticker\")['regime'].value_counts().unstack(fill_value=0).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235303e",
   "metadata": {},
   "source": [
    "**Xóa bớt biến df_merged không còn cần thiết để giảm dung lượng RAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26204a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df_merged\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614073e",
   "metadata": {},
   "source": [
    "**Block 5: Chuẩn hóa dữ liệu FA và TA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59579dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features snapshot -> df_features_20250930_165456.parquet\n",
      "Sample features (head):\n",
      "  ticker  timestamp  DebtToEquityRatio  EBITMargin       ROA       ROE  \\\n",
      "0    AAA 2023-06-14           0.559115    0.978956  0.368731  0.415728   \n",
      "1    AAT 2023-06-14           0.511748    0.980206  0.463952  0.482511   \n",
      "2    ABS 2023-06-14           0.572974    0.977818  0.380350  0.424487   \n",
      "3    ABT 2023-06-14           0.505407    0.980548  0.482215  0.495237   \n",
      "4    ACC 2023-06-14           0.592866    0.980726  0.403270  0.446116   \n",
      "\n",
      "       ROIC  BasicEPS  PriceToBook  PriceToEarning  ...  macd_diff_z  \\\n",
      "0  0.753300  0.158046     0.377640        0.537353  ...    -1.164222   \n",
      "1  0.778758  0.145325     0.349144        0.521837  ...     2.231773   \n",
      "2  0.757676  0.156000     0.357324        0.528204  ...     1.191728   \n",
      "3  0.776716  0.211753     0.372187        0.522248  ...     1.217966   \n",
      "4  0.758087  0.153041     0.415297        0.528052  ...    -0.111005   \n",
      "\n",
      "      rsi_z  bollinger_hband_z  bollinger_lband_z     atr_z     obv_z  \\\n",
      "0 -1.530688           1.363061           1.839777  0.589827  1.405288   \n",
      "1  1.285717           3.255245          -2.341308  3.090152  2.465147   \n",
      "2  0.818367           2.953700           1.355816  2.352162  2.390705   \n",
      "3  0.240402          -2.307010          -1.300076 -0.966512 -0.930970   \n",
      "4 -2.131000          -0.560416           0.232599 -1.192828 -0.313724   \n",
      "\n",
      "     vwap_z  regime_bear  regime_bull  regime_sideway  \n",
      "0  1.603355        False         True           False  \n",
      "1  3.222970        False         True           False  \n",
      "2  2.919060        False         True           False  \n",
      "3 -2.073728        False        False            True  \n",
      "4 -0.355070        False        False            True  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Shape after scaling & dropna: (188243, 27)\n",
      "Original rows: 263580, Kept after dropna: 188243 (kept 71.42%)\n"
     ]
    }
   ],
   "source": [
    "# Block 5 — Feature engineering & scaling (with regime as feature)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Config ----------\n",
    "SAVE_SNAPSHOT = True\n",
    "SNAPSHOT_PATH = f\"df_features_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet\"\n",
    "\n",
    "SAVE_GLOBAL_SCALER = False\n",
    "GLOBAL_SCALER_PATH = \"global_fa_scaler.json\"\n",
    "\n",
    "# ---------- Feature lists ----------\n",
    "fa_features = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "]\n",
    "\n",
    "ta_features = [\n",
    "    \"ema_5\",\"ema_20\",\"ema_50\",\"macd\",\"macd_signal\",\"macd_diff\",\n",
    "    \"rsi\",\"bollinger_hband\",\"bollinger_lband\",\"atr\",\"obv\",\"vwap\"\n",
    "]\n",
    "\n",
    "# ---------- FA: cross-section min-max per day ----------\n",
    "def scale_fa_minmax(df):\n",
    "    df_scaled = df.copy()\n",
    "    for f in fa_features:\n",
    "        vals = pd.to_numeric(df[f], errors=\"coerce\")\n",
    "        vmin, vmax = vals.min(), vals.max()\n",
    "        if np.isfinite(vmin) and np.isfinite(vmax) and vmax > vmin:\n",
    "            df_scaled[f] = (vals - vmin) / (vmax - vmin)\n",
    "        else:\n",
    "            df_scaled[f] = np.nan\n",
    "    return df_scaled\n",
    "\n",
    "df_scaled_fa = df_with_ta.groupby(\"timestamp\", group_keys=False).apply(scale_fa_minmax)\n",
    "\n",
    "# ---------- TA: rolling z-score per ticker ----------\n",
    "def zscore_rolling(series, window=60):\n",
    "    return (series - series.rolling(window).mean()) / series.rolling(window).std()\n",
    "\n",
    "df_scaled = df_scaled_fa.groupby(\"ticker\", group_keys=False).apply(\n",
    "    lambda g: g.assign(**{f\"{col}_z\": zscore_rolling(g[col], 60) for col in ta_features})\n",
    ")\n",
    "\n",
    "# ---------- Encode regime (one-hot) ----------\n",
    "# bull=1, bear=0, sideway=0 -> regime_bull\n",
    "# bull=0, bear=1, sideway=0 -> regime_bear\n",
    "# bull=0, bear=0, sideway=1 -> regime_sideway\n",
    "regime_dummies = pd.get_dummies(df_scaled[\"regime\"], prefix=\"regime\")\n",
    "df_scaled = pd.concat([df_scaled, regime_dummies], axis=1)\n",
    "\n",
    "regime_features = list(regime_dummies.columns)  # [\"regime_bear\",\"regime_bull\",\"regime_sideway\"]\n",
    "\n",
    "# ---------- Keep features ----------\n",
    "keep_cols = [\"ticker\",\"timestamp\"] + fa_features + [f\"{col}_z\" for col in ta_features] + regime_features\n",
    "\n",
    "df_features = df_scaled[keep_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "# ---------- Save snapshot ----------\n",
    "if SAVE_SNAPSHOT:\n",
    "    try:\n",
    "        df_features.to_parquet(SNAPSHOT_PATH, index=False)\n",
    "        print(f\"Saved features snapshot -> {SNAPSHOT_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot save snapshot:\", e)\n",
    "\n",
    "# ---------- Optionally persist a global FA scaler ----------\n",
    "if SAVE_GLOBAL_SCALER:\n",
    "    global_scaler = {}\n",
    "    for f in fa_features:\n",
    "        vals = pd.to_numeric(df_with_ta[f], errors=\"coerce\")\n",
    "        vmin, vmax = float(vals.min()) if vals.notna().any() else None, float(vals.max()) if vals.notna().any() else None\n",
    "        global_scaler[f] = {\"vmin\": vmin, \"vmax\": vmax}\n",
    "    try:\n",
    "        with open(GLOBAL_SCALER_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(global_scaler, fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved global FA scaler -> {GLOBAL_SCALER_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot save global scaler:\", e)\n",
    "\n",
    "# ---------- Diagnostics ----------\n",
    "print(\"Sample features (head):\")\n",
    "print(df_features.head())\n",
    "print(\"Shape after scaling & dropna:\", df_features.shape)\n",
    "try:\n",
    "    orig_rows = len(df_with_ta)\n",
    "    kept_rows = len(df_features)\n",
    "    print(f\"Original rows: {orig_rows}, Kept after dropna: {kept_rows} (kept {kept_rows/orig_rows:.2%})\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f5d94",
   "metadata": {},
   "source": [
    "**Xóa các biến df_with_ta, df_scaled, df_scaled_fa không cần thiết nữa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee3d19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_with_ta, df_scaled, df_scaled_fa\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245a6ca",
   "metadata": {},
   "source": [
    "**Block 6: Giảm chiều dữ liệu bằng t-SNE và phân cụm bằng DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "466d47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sample:\n",
      "  ticker  timestamp  cluster     tsne_x     tsne_y    month\n",
      "0    AAA 2023-06-14       -1  11.322961 -39.312492  2023-06\n",
      "1    AAT 2023-06-14       -1 -52.104614  35.878654  2023-06\n",
      "2    ABS 2023-06-14       -1 -53.525627  12.820326  2023-06\n",
      "3    ABT 2023-06-14       -1  43.397667  56.436115  2023-06\n",
      "4    ACC 2023-06-14       -1  68.141510   5.876695  2023-06\n",
      "Số cụm mỗi tháng:\n",
      "month\n",
      "2023-06     22\n",
      "2023-07     74\n",
      "2023-08     77\n",
      "2023-09     41\n",
      "2023-10     69\n",
      "2023-11     54\n",
      "2023-12     80\n",
      "2024-01     84\n",
      "2024-02     32\n",
      "2024-03     70\n",
      "2024-04     45\n",
      "2024-05     65\n",
      "2024-06     84\n",
      "2024-07     97\n",
      "2024-08     68\n",
      "2024-09     81\n",
      "2024-10    112\n",
      "2024-11     84\n",
      "2024-12     67\n",
      "2025-01     41\n",
      "2025-02     71\n",
      "2025-03     93\n",
      "2025-04     50\n",
      "2025-05     74\n",
      "2025-06     96\n",
      "2025-07    103\n",
      "2025-08     83\n",
      "2025-09     65\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Block 6 — Giảm chiều dữ liệu & phân cụm (t-SNE + DBSCAN)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# --- Chọn các cột features để phân cụm ---\n",
    "feature_cols = [\n",
    "    \"DebtToEquityRatio\",\"EBITMargin\",\"ROA\",\"ROE\",\"ROIC\",\n",
    "    \"BasicEPS\",\"PriceToBook\",\"PriceToEarning\",\n",
    "    \"NetRevenueGrowthYoY\",\"GrossProfitGrowthYoY\"\n",
    "] + [c for c in df_features.columns if c.endswith(\"_z\")]\n",
    "\n",
    "# --- Thêm cột tháng để snapshot ---\n",
    "df_features[\"month\"] = df_features[\"timestamp\"].dt.to_period(\"M\")\n",
    "\n",
    "cluster_results = []\n",
    "\n",
    "for (month, g) in df_features.groupby(\"month\"):\n",
    "    if len(g) < 10:   # quá ít cổ phiếu thì bỏ\n",
    "        continue\n",
    "\n",
    "    X = g[feature_cols].values\n",
    "\n",
    "    # --- t-SNE giảm chiều còn 2D ---\n",
    "    tsne = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\", init=\"random\", random_state=42)\n",
    "    X_emb = tsne.fit_transform(X)\n",
    "\n",
    "    # --- DBSCAN phân cụm ---\n",
    "    db = DBSCAN(eps=0.5, min_samples=5).fit(X_emb)\n",
    "    labels = db.labels_\n",
    "\n",
    "    temp = g[[\"ticker\",\"timestamp\"]].copy()\n",
    "    temp[\"cluster\"] = labels\n",
    "    temp[\"tsne_x\"] = X_emb[:,0]\n",
    "    temp[\"tsne_y\"] = X_emb[:,1]\n",
    "    temp[\"month\"]  = str(month)\n",
    "\n",
    "    cluster_results.append(temp)\n",
    "\n",
    "df_clusters = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(\"Cluster sample:\")\n",
    "print(df_clusters.head())\n",
    "print(\"Số cụm mỗi tháng:\")\n",
    "print(df_clusters.groupby(\"month\")[\"cluster\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde0916",
   "metadata": {},
   "source": [
    "**Block 7: Xây tensors (clusters mapping) và masks (active stocks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d88b8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 1: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 2: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 3: tensor (509, 64, 33, 25), mask (509, 64, 33, 25) saved.\n",
      "Cluster 4: tensor (509, 64, 31, 25), mask (509, 64, 31, 25) saved.\n",
      "Cluster 5: tensor (509, 64, 30, 25), mask (509, 64, 30, 25) saved.\n",
      "Cluster 6: tensor (509, 64, 30, 25), mask (509, 64, 30, 25) saved.\n",
      "Cluster 7: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 8: tensor (509, 64, 27, 25), mask (509, 64, 27, 25) saved.\n",
      "Cluster 9: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 10: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 11: tensor (509, 64, 30, 25), mask (509, 64, 30, 25) saved.\n",
      "Cluster 12: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 13: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 14: tensor (509, 64, 31, 25), mask (509, 64, 31, 25) saved.\n",
      "Cluster 15: tensor (509, 64, 30, 25), mask (509, 64, 30, 25) saved.\n",
      "Cluster 16: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 17: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 18: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 19: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 20: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 21: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 22: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 23: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 24: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 25: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 26: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 27: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 28: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 29: tensor (509, 64, 25, 25), mask (509, 64, 25, 25) saved.\n",
      "Cluster 30: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 31: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 32: tensor (509, 64, 29, 25), mask (509, 64, 29, 25) saved.\n",
      "Cluster 33: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 34: tensor (509, 64, 25, 25), mask (509, 64, 25, 25) saved.\n",
      "Cluster 35: tensor (509, 64, 24, 25), mask (509, 64, 24, 25) saved.\n",
      "Cluster 36: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 37: tensor (509, 64, 25, 25), mask (509, 64, 25, 25) saved.\n",
      "Cluster 38: tensor (509, 64, 27, 25), mask (509, 64, 27, 25) saved.\n",
      "Cluster 39: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 40: tensor (509, 64, 24, 25), mask (509, 64, 24, 25) saved.\n",
      "Cluster 41: tensor (509, 64, 28, 25), mask (509, 64, 28, 25) saved.\n",
      "Cluster 42: tensor (509, 64, 27, 25), mask (509, 64, 27, 25) saved.\n",
      "Cluster 43: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 44: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 45: tensor (509, 64, 22, 25), mask (509, 64, 22, 25) saved.\n",
      "Cluster 46: tensor (509, 64, 25, 25), mask (509, 64, 25, 25) saved.\n",
      "Cluster 47: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 48: tensor (509, 64, 24, 25), mask (509, 64, 24, 25) saved.\n",
      "Cluster 49: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 50: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 51: tensor (509, 64, 26, 25), mask (509, 64, 26, 25) saved.\n",
      "Cluster 52: tensor (509, 64, 22, 25), mask (509, 64, 22, 25) saved.\n",
      "Cluster 53: tensor (509, 64, 25, 25), mask (509, 64, 25, 25) saved.\n",
      "Cluster 54: tensor (509, 64, 18, 25), mask (509, 64, 18, 25) saved.\n",
      "Cluster 55: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 56: tensor (509, 64, 24, 25), mask (509, 64, 24, 25) saved.\n",
      "Cluster 57: tensor (509, 64, 24, 25), mask (509, 64, 24, 25) saved.\n",
      "Cluster 58: tensor (509, 64, 22, 25), mask (509, 64, 22, 25) saved.\n",
      "Cluster 59: tensor (509, 64, 21, 25), mask (509, 64, 21, 25) saved.\n",
      "Cluster 60: tensor (509, 64, 20, 25), mask (509, 64, 20, 25) saved.\n",
      "Cluster 61: tensor (509, 64, 25, 25), mask (509, 64, 25, 25) saved.\n",
      "Cluster 62: tensor (509, 64, 22, 25), mask (509, 64, 22, 25) saved.\n",
      "Cluster 63: tensor (509, 64, 23, 25), mask (509, 64, 23, 25) saved.\n",
      "Cluster 64: tensor (509, 64, 24, 25), mask (509, 64, 24, 25) saved.\n",
      "Cluster 65: tensor (509, 64, 21, 25), mask (509, 64, 21, 25) saved.\n",
      "Cluster 66: tensor (509, 64, 21, 25), mask (509, 64, 21, 25) saved.\n",
      "Cluster 67: tensor (509, 64, 17, 25), mask (509, 64, 17, 25) saved.\n",
      "Cluster 68: tensor (509, 64, 19, 25), mask (509, 64, 19, 25) saved.\n",
      "Cluster 69: tensor (509, 64, 16, 25), mask (509, 64, 16, 25) saved.\n",
      "Cluster 70: tensor (509, 64, 16, 25), mask (509, 64, 16, 25) saved.\n",
      "Cluster 71: tensor (509, 64, 15, 25), mask (509, 64, 15, 25) saved.\n",
      "Cluster 72: tensor (509, 64, 14, 25), mask (509, 64, 14, 25) saved.\n",
      "Cluster 73: tensor (509, 64, 13, 25), mask (509, 64, 13, 25) saved.\n",
      "Cluster 74: tensor (509, 64, 12, 25), mask (509, 64, 12, 25) saved.\n",
      "Cluster 75: tensor (509, 64, 15, 25), mask (509, 64, 15, 25) saved.\n",
      "Cluster 76: tensor (509, 64, 11, 25), mask (509, 64, 11, 25) saved.\n",
      "Cluster 77: tensor (509, 64, 10, 25), mask (509, 64, 10, 25) saved.\n",
      "Cluster 78: tensor (509, 64, 10, 25), mask (509, 64, 10, 25) saved.\n",
      "Cluster 79: tensor (509, 64, 10, 25), mask (509, 64, 10, 25) saved.\n",
      "Cluster 80: tensor (509, 64, 12, 25), mask (509, 64, 12, 25) saved.\n",
      "Cluster 81: tensor (509, 64, 11, 25), mask (509, 64, 11, 25) saved.\n",
      "Cluster 82: tensor (509, 64, 8, 25), mask (509, 64, 8, 25) saved.\n",
      "Cluster 83: tensor (509, 64, 5, 25), mask (509, 64, 5, 25) saved.\n",
      "Cluster 84: tensor (509, 64, 6, 25), mask (509, 64, 6, 25) saved.\n",
      "Cluster 85: tensor (509, 64, 6, 25), mask (509, 64, 6, 25) saved.\n",
      "Cluster 86: tensor (509, 64, 5, 25), mask (509, 64, 5, 25) saved.\n",
      "Cluster 87: tensor (509, 64, 5, 25), mask (509, 64, 5, 25) saved.\n",
      "Cluster 88: tensor (509, 64, 5, 25), mask (509, 64, 5, 25) saved.\n",
      "Cluster 89: tensor (509, 64, 7, 25), mask (509, 64, 7, 25) saved.\n",
      "Cluster 90: tensor (509, 64, 6, 25), mask (509, 64, 6, 25) saved.\n",
      "Cluster 91: tensor (509, 64, 6, 25), mask (509, 64, 6, 25) saved.\n",
      "Cluster 92: tensor (509, 64, 4, 25), mask (509, 64, 4, 25) saved.\n",
      "Cluster 93: tensor (509, 64, 6, 25), mask (509, 64, 6, 25) saved.\n",
      "Cluster 94: tensor (509, 64, 6, 25), mask (509, 64, 6, 25) saved.\n",
      "Cluster 95: tensor (509, 64, 3, 25), mask (509, 64, 3, 25) saved.\n",
      "Cluster 96: tensor (509, 64, 3, 25), mask (509, 64, 3, 25) saved.\n",
      "Cluster 97: tensor (509, 64, 2, 25), mask (509, 64, 2, 25) saved.\n",
      "Cluster 98: tensor (509, 64, 2, 25), mask (509, 64, 2, 25) saved.\n",
      "Cluster 99: tensor (509, 64, 2, 25), mask (509, 64, 2, 25) saved.\n",
      "Cluster 100: tensor (509, 64, 3, 25), mask (509, 64, 3, 25) saved.\n",
      "Cluster 101: tensor (509, 64, 3, 25), mask (509, 64, 3, 25) saved.\n",
      "Cluster 102: tensor (509, 64, 2, 25), mask (509, 64, 2, 25) saved.\n",
      "Cluster 103: tensor (509, 64, 1, 25), mask (509, 64, 1, 25) saved.\n",
      "Cluster 104: tensor (509, 64, 1, 25), mask (509, 64, 1, 25) saved.\n",
      "Cluster 105: tensor (509, 64, 1, 25), mask (509, 64, 1, 25) saved.\n",
      "Cluster 106: tensor (509, 64, 1, 25), mask (509, 64, 1, 25) saved.\n",
      "Cluster 107: tensor (509, 64, 1, 25), mask (509, 64, 1, 25) saved.\n",
      "Cluster 108: tensor (309, 64, 1, 25), mask (309, 64, 1, 25) saved.\n",
      "Cluster 109: tensor (509, 64, 1, 25), mask (509, 64, 1, 25) saved.\n",
      "Cluster 110: tensor (509, 64, 2, 25), mask (509, 64, 2, 25) saved.\n",
      "✅ Done Block 7: tensors + masks saved for all clusters.\n"
     ]
    }
   ],
   "source": [
    "# Block 7 — Tensors & Masks (updated, include regime one-hot)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, json\n",
    "\n",
    "LOOKBACK = 64   # window size\n",
    "DATA_DIR = \"./tensors/\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- Feature columns: giữ tất cả trừ mấy cột meta ---\n",
    "feature_cols = [c for c in df_features.columns \n",
    "                if c not in [\"ticker\",\"timestamp\",\"cluster\",\"month\"]]\n",
    "\n",
    "tensor_index = []\n",
    "\n",
    "for c_id, g in df_clusters.groupby(\"cluster\"):\n",
    "    if c_id == -1:   # noise bỏ qua\n",
    "        continue\n",
    "\n",
    "    tickers = sorted(g[\"ticker\"].unique())\n",
    "    g_feat = df_features[df_features[\"ticker\"].isin(tickers)].copy()\n",
    "\n",
    "    # Pivot: index = timestamp, columns = (ticker, feature)\n",
    "    pivoted = g_feat.pivot(index=\"timestamp\", columns=\"ticker\", values=feature_cols)\n",
    "    pivoted.columns = pd.MultiIndex.from_product([tickers, feature_cols])\n",
    "\n",
    "    # Mask\n",
    "    mask_df = ~pivoted.isna()\n",
    "    pivoted_filled = pivoted.ffill().bfill()\n",
    "\n",
    "    T, N, F = len(pivoted_filled.index), len(tickers), len(feature_cols)\n",
    "    X = pivoted_filled.values.reshape(T, N, F)\n",
    "    M = mask_df.values.reshape(T, N, F).astype(np.int8)\n",
    "\n",
    "    cluster_tensors, cluster_masks, cluster_dates = [], [], []\n",
    "    for i in range(LOOKBACK, T):\n",
    "        cluster_tensors.append(X[i-LOOKBACK:i])\n",
    "        cluster_masks.append(M[i-LOOKBACK:i])\n",
    "        cluster_dates.append(pivoted_filled.index[i])  # ngày cuối của window\n",
    "\n",
    "    if cluster_tensors:\n",
    "        X_arr = np.array(cluster_tensors, dtype=np.float16)  # tiết kiệm RAM\n",
    "        M_arr = np.array(cluster_masks, dtype=np.int8)\n",
    "\n",
    "        tensor_file = f\"cluster_{c_id}_tensor.npy\"\n",
    "        mask_file   = f\"cluster_{c_id}_mask.npy\"\n",
    "        np.save(os.path.join(DATA_DIR, tensor_file), X_arr)\n",
    "        np.save(os.path.join(DATA_DIR, mask_file), M_arr)\n",
    "\n",
    "        tensor_index.append({\n",
    "            \"cluster\": int(c_id),\n",
    "            \"tickers\": tickers,\n",
    "            \"dates\": [str(d) for d in cluster_dates],   # ngày T\n",
    "            \"dates_shifted\": [str(d+pd.Timedelta(days=2)) for d in cluster_dates],  # T+1 (reward align, đổi T+2 ở Block 😎\n",
    "            \"tensor_file\": tensor_file,\n",
    "            \"mask_file\": mask_file,\n",
    "            \"n_features\": F\n",
    "        })\n",
    "\n",
    "        print(f\"Cluster {c_id}: tensor {X_arr.shape}, mask {M_arr.shape} saved.\")\n",
    "\n",
    "    del g_feat, pivoted, pivoted_filled, mask_df, X, M, cluster_tensors, cluster_masks\n",
    "    gc.collect()\n",
    "\n",
    "# Save metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"w\") as f:\n",
    "    json.dump(tensor_index, f, indent=2)\n",
    "\n",
    "print(\"✅ Done Block 7: tensors + masks saved for all clusters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95417aff",
   "metadata": {},
   "source": [
    "**Block 7.5: Chuẩn bị dữ liệu backtest(loại bỏ các cột dữ liệu không cần thiết nữa)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e9bfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done Block 7.5 (improved, ATR từ df_with_ta): df_backtest sẵn sàng.\n",
      "Kích thước: (263580, 20)\n",
      "Sample:\n",
      "  ticker  timestamp     close      high       low   regime  atr  exit_price  \\\n",
      "0    AAA 2023-01-03  6866.145  6866.145  6539.643  sideway  NaN  6885.35100   \n",
      "1    AAA 2023-01-04  6827.733  7000.587  6827.733  sideway  NaN  6856.54200   \n",
      "2    AAA 2023-01-05  6885.351  6904.557  6808.527  sideway  NaN  6789.32100   \n",
      "3    AAA 2023-01-06  6856.542  6990.984  6818.130  sideway  NaN  6719.41116   \n",
      "4    AAA 2023-01-09  6789.321  6962.175  6760.512  sideway  NaN  6904.55700   \n",
      "5    AAA 2023-01-10  6789.321  6885.351  6693.291  sideway  NaN  6904.55700   \n",
      "6    AAA 2023-01-11  6904.557  6962.175  6818.130  sideway  NaN  6837.33600   \n",
      "7    AAA 2023-01-12  6904.557  6962.175  6866.145  sideway  NaN  6875.74800   \n",
      "8    AAA 2023-01-13  6837.336  6990.984  6837.336  sideway  NaN  7076.64276   \n",
      "9    AAA 2023-01-16  6875.748  6904.557  6818.130  sideway  NaN  7116.39918   \n",
      "\n",
      "   exit_date  realized_return   exit_type     tp_level    sl_level  \\\n",
      "0 2023-01-05        -0.000207  time_limit  7106.460075  6728.82210   \n",
      "1 2023-01-06         0.001211  time_limit  7066.703655  6691.17834   \n",
      "2 2023-01-09        -0.017045  time_limit  7126.338285  6747.64398   \n",
      "3 2023-01-10        -0.023203          sl  7096.520970  6719.41116   \n",
      "4 2023-01-11         0.013831  time_limit  7026.947235  6653.53458   \n",
      "5 2023-01-12         0.013831  time_limit  7026.947235  6653.53458   \n",
      "6 2023-01-13        -0.012783  time_limit  7146.216495  6766.46586   \n",
      "7 2023-01-16        -0.007181  time_limit  7146.216495  6766.46586   \n",
      "8 2023-01-17         0.031401          tp  7076.642760  6700.58928   \n",
      "9 2023-01-18         0.031401          tp  7116.399180  6738.23304   \n",
      "\n",
      "  entry_regime exit_regime  horizon_days    fee  regime_bear  regime_bull  \\\n",
      "0      sideway     sideway           2.0  0.003        False        False   \n",
      "1      sideway     sideway           2.0  0.003        False        False   \n",
      "2      sideway     sideway           4.0  0.003        False        False   \n",
      "3      sideway     sideway           4.0  0.003        False        False   \n",
      "4      sideway     sideway           2.0  0.003        False        False   \n",
      "5      sideway     sideway           2.0  0.003        False        False   \n",
      "6      sideway     sideway           2.0  0.003        False        False   \n",
      "7      sideway     sideway           4.0  0.003        False        False   \n",
      "8      sideway     sideway           4.0  0.003        False        False   \n",
      "9      sideway     sideway           2.0  0.003        False        False   \n",
      "\n",
      "   regime_sideway  \n",
      "0            True  \n",
      "1            True  \n",
      "2            True  \n",
      "3            True  \n",
      "4            True  \n",
      "5            True  \n",
      "6            True  \n",
      "7            True  \n",
      "8            True  \n",
      "9            True  \n"
     ]
    }
   ],
   "source": [
    "# Block 7.5 — Chuẩn bị dữ liệu backtest cho reward (TP/SL động + regime flip + phí)\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Tham số cấu hình ---\n",
    "COST_BPS = 30  # phí round-trip (0.3%)\n",
    "ATR_MULT = 1.0 # hệ số nhân ATR để điều chỉnh SL\n",
    "TP_SL_RULES = {\n",
    "    \"bull\":    {\"tp\": 0.06,  \"sl\": -0.04},\n",
    "    \"sideway\":{\"tp\": 0.035, \"sl\": -0.02},\n",
    "    \"bear\":   {\"tp\": 0.02,  \"sl\": -0.015},\n",
    "}\n",
    "\n",
    "# --- Chuẩn bị dữ liệu giá cơ bản ---\n",
    "df_backtest = df_price[[\"ticker\", \"timestamp\", \"close\", \"high\", \"low\"]].copy()\n",
    "df_backtest[\"timestamp\"] = pd.to_datetime(df_backtest[\"timestamp\"])\n",
    "\n",
    "# --- Merge regime + ATR từ df_with_ta ---\n",
    "df_ta = df_with_ta[[\"ticker\",\"timestamp\",\"regime\",\"atr\"]].copy()\n",
    "df_ta[\"timestamp\"] = pd.to_datetime(df_ta[\"timestamp\"])\n",
    "df_backtest = pd.merge(df_backtest, df_ta, on=[\"ticker\",\"timestamp\"], how=\"left\")\n",
    "\n",
    "# --- Hàm tính exit cho từng ticker ---\n",
    "def compute_exit(df):\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    exit_price, exit_date = [], []\n",
    "    realized_return, exit_type = [], []\n",
    "    tp_level_list, sl_level_list = [], []\n",
    "    entry_regime_list, exit_regime_list = [], []\n",
    "    horizon_list, fee_list = [], []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        entry_price = df.loc[i, \"close\"]\n",
    "        entry_date  = df.loc[i, \"timestamp\"]\n",
    "        entry_regime= df.loc[i, \"regime\"]\n",
    "        atr_val     = df.loc[i, \"atr\"]\n",
    "\n",
    "        if pd.isna(entry_price) or pd.isna(entry_regime):\n",
    "            exit_price.append(np.nan); exit_date.append(pd.NaT)\n",
    "            realized_return.append(np.nan); exit_type.append(\"no_data\")\n",
    "            tp_level_list.append(np.nan); sl_level_list.append(np.nan)\n",
    "            entry_regime_list.append(entry_regime); exit_regime_list.append(np.nan)\n",
    "            horizon_list.append(np.nan); fee_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # --- Thiết lập TP/SL theo regime + ATR ---\n",
    "        rule = TP_SL_RULES.get(entry_regime, {\"tp\":0.03,\"sl\":-0.02})\n",
    "        tp_pct, sl_pct = rule[\"tp\"], rule[\"sl\"]\n",
    "\n",
    "        # điều chỉnh SL bằng ATR (%)\n",
    "        if atr_val and entry_price > 0:\n",
    "            atr_pct = atr_val / entry_price\n",
    "            sl_pct = min(sl_pct, -ATR_MULT * atr_pct)\n",
    "\n",
    "        tp_level = entry_price * (1 + tp_pct)\n",
    "        sl_level = entry_price * (1 + sl_pct)\n",
    "\n",
    "        tp_level_list.append(tp_level)\n",
    "        sl_level_list.append(sl_level)\n",
    "        entry_regime_list.append(entry_regime)\n",
    "\n",
    "        # --- Kiểm tra trong T+1, T+2 ---\n",
    "        future = df.loc[i+1:i+2].copy()\n",
    "        if future.empty:\n",
    "            exit_price.append(np.nan); exit_date.append(pd.NaT)\n",
    "            realized_return.append(np.nan); exit_type.append(\"censored\")\n",
    "            exit_regime_list.append(np.nan); horizon_list.append(np.nan); fee_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        decided = False\n",
    "        for j in future.index:\n",
    "            px_high, px_low = future.loc[j, \"high\"], future.loc[j, \"low\"]\n",
    "            reg_j   = future.loc[j, \"regime\"]\n",
    "            date_j  = future.loc[j, \"timestamp\"]\n",
    "\n",
    "            # 1. Regime flip sang bear -> exit close ngay lập tức\n",
    "            if reg_j == \"bear\" and entry_regime != \"bear\":\n",
    "                px, dt, etype = future.loc[j, \"close\"], date_j, \"regime_flip\"\n",
    "                decided = True; break\n",
    "\n",
    "            # 2. SL precedence\n",
    "            if px_low <= sl_level:\n",
    "                px, dt, etype = sl_level, date_j, \"sl\"\n",
    "                decided = True; break\n",
    "\n",
    "            # 3. TP sau đó\n",
    "            if px_high >= tp_level:\n",
    "                px, dt, etype = tp_level, date_j, \"tp\"\n",
    "                decided = True; break\n",
    "\n",
    "        # 4. Nếu chưa exit trong 2 ngày -> exit cuối cùng T+2 close\n",
    "        if not decided:\n",
    "            if i+2 < len(df):\n",
    "                px, dt, etype = df.loc[i+2, \"close\"], df.loc[i+2, \"timestamp\"], \"time_limit\"\n",
    "            else:\n",
    "                px, dt, etype = np.nan, pd.NaT, \"censored\"\n",
    "\n",
    "        exit_price.append(px); exit_date.append(dt); exit_type.append(etype)\n",
    "        exit_regime_list.append(reg_j if \"reg_j\" in locals() else np.nan)\n",
    "\n",
    "        # --- Realized return (log) sau khi trừ phí ---\n",
    "        if px and entry_price>0:\n",
    "            gross_ret = np.log(px/entry_price)\n",
    "            fee = COST_BPS/1e4   # phí round-trip\n",
    "            realized_return.append(gross_ret - fee)\n",
    "            fee_list.append(fee)\n",
    "        else:\n",
    "            realized_return.append(np.nan); fee_list.append(np.nan)\n",
    "\n",
    "        horizon = (dt - entry_date).days if pd.notna(dt) else np.nan\n",
    "        horizon_list.append(horizon)\n",
    "\n",
    "    df[\"exit_price\"] = exit_price\n",
    "    df[\"exit_date\"] = exit_date\n",
    "    df[\"realized_return\"] = realized_return\n",
    "    df[\"exit_type\"] = exit_type\n",
    "    df[\"tp_level\"] = tp_level_list\n",
    "    df[\"sl_level\"] = sl_level_list\n",
    "    df[\"entry_regime\"] = entry_regime_list\n",
    "    df[\"exit_regime\"] = exit_regime_list\n",
    "    df[\"horizon_days\"] = horizon_list\n",
    "    df[\"fee\"] = fee_list\n",
    "    return df\n",
    "\n",
    "# --- Áp dụng toàn bộ ticker ---\n",
    "df_backtest = df_backtest.groupby(\"ticker\", group_keys=False).apply(compute_exit)\n",
    "\n",
    "# --- One-hot regime tại entry ---\n",
    "df_regime_dummies = pd.get_dummies(df_backtest[\"entry_regime\"], prefix=\"regime\")\n",
    "df_backtest = pd.concat([df_backtest, df_regime_dummies], axis=1)\n",
    "\n",
    "print(\"✅ Done Block 7.5 (improved, ATR từ df_with_ta): df_backtest sẵn sàng.\")\n",
    "print(\"Kích thước:\", df_backtest.shape)\n",
    "print(\"Sample:\")\n",
    "print(df_backtest.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7aa4e",
   "metadata": {},
   "source": [
    "**Block 8: Huấn luyện A3C theo từng cụm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e993a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cluster 0] loaded tensor ./tensors/cluster_0_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 0 | Epoch 1/3 | AvgLoss -0.011300 | batches 46\n",
      "[A3C] Cluster 0 | Epoch 2/3 | AvgLoss -0.011976 | batches 46\n",
      "[A3C] Cluster 0 | Epoch 3/3 | AvgLoss -0.011060 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_0.pt\n",
      "[Cluster 1] loaded tensor ./tensors/cluster_1_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 1 | Epoch 1/3 | AvgLoss -0.012400 | batches 56\n",
      "[A3C] Cluster 1 | Epoch 2/3 | AvgLoss -0.011375 | batches 56\n",
      "[A3C] Cluster 1 | Epoch 3/3 | AvgLoss -0.011279 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_1.pt\n",
      "[Cluster 2] loaded tensor ./tensors/cluster_2_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 2 | Epoch 1/3 | AvgLoss -0.011559 | batches 52\n",
      "[A3C] Cluster 2 | Epoch 2/3 | AvgLoss -0.011941 | batches 52\n",
      "[A3C] Cluster 2 | Epoch 3/3 | AvgLoss -0.011691 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_2.pt\n",
      "[Cluster 3] loaded tensor ./tensors/cluster_3_tensor.npy -> (B,T,N,F)=(509,64,33,25)\n",
      "[A3C] Cluster 3 | Epoch 1/3 | AvgLoss -0.015455 | batches 66\n",
      "[A3C] Cluster 3 | Epoch 2/3 | AvgLoss -0.010604 | batches 66\n",
      "[A3C] Cluster 3 | Epoch 3/3 | AvgLoss -0.011397 | batches 66\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_3.pt\n",
      "[Cluster 4] loaded tensor ./tensors/cluster_4_tensor.npy -> (B,T,N,F)=(509,64,31,25)\n",
      "[A3C] Cluster 4 | Epoch 1/3 | AvgLoss -0.012686 | batches 62\n",
      "[A3C] Cluster 4 | Epoch 2/3 | AvgLoss -0.010470 | batches 62\n",
      "[A3C] Cluster 4 | Epoch 3/3 | AvgLoss -0.011019 | batches 62\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_4.pt\n",
      "[Cluster 5] loaded tensor ./tensors/cluster_5_tensor.npy -> (B,T,N,F)=(509,64,30,25)\n",
      "[A3C] Cluster 5 | Epoch 1/3 | AvgLoss -0.021586 | batches 60\n",
      "[A3C] Cluster 5 | Epoch 2/3 | AvgLoss -0.011187 | batches 60\n",
      "[A3C] Cluster 5 | Epoch 3/3 | AvgLoss -0.011001 | batches 60\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_5.pt\n",
      "[Cluster 6] loaded tensor ./tensors/cluster_6_tensor.npy -> (B,T,N,F)=(509,64,30,25)\n",
      "[A3C] Cluster 6 | Epoch 1/3 | AvgLoss -0.015056 | batches 60\n",
      "[A3C] Cluster 6 | Epoch 2/3 | AvgLoss -0.009397 | batches 60\n",
      "[A3C] Cluster 6 | Epoch 3/3 | AvgLoss -0.011569 | batches 60\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_6.pt\n",
      "[Cluster 7] loaded tensor ./tensors/cluster_7_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 7 | Epoch 1/3 | AvgLoss -0.011866 | batches 52\n",
      "[A3C] Cluster 7 | Epoch 2/3 | AvgLoss -0.010749 | batches 52\n",
      "[A3C] Cluster 7 | Epoch 3/3 | AvgLoss -0.011610 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_7.pt\n",
      "[Cluster 8] loaded tensor ./tensors/cluster_8_tensor.npy -> (B,T,N,F)=(509,64,27,25)\n",
      "[A3C] Cluster 8 | Epoch 1/3 | AvgLoss -0.006595 | batches 54\n",
      "[A3C] Cluster 8 | Epoch 2/3 | AvgLoss -0.010634 | batches 54\n",
      "[A3C] Cluster 8 | Epoch 3/3 | AvgLoss -0.011182 | batches 54\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_8.pt\n",
      "[Cluster 9] loaded tensor ./tensors/cluster_9_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 9 | Epoch 1/3 | AvgLoss -0.007507 | batches 58\n",
      "[A3C] Cluster 9 | Epoch 2/3 | AvgLoss -0.011157 | batches 58\n",
      "[A3C] Cluster 9 | Epoch 3/3 | AvgLoss -0.011654 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_9.pt\n",
      "[Cluster 10] loaded tensor ./tensors/cluster_10_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 10 | Epoch 1/3 | AvgLoss -0.011715 | batches 58\n",
      "[A3C] Cluster 10 | Epoch 2/3 | AvgLoss -0.010357 | batches 58\n",
      "[A3C] Cluster 10 | Epoch 3/3 | AvgLoss -0.011408 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_10.pt\n",
      "[Cluster 11] loaded tensor ./tensors/cluster_11_tensor.npy -> (B,T,N,F)=(509,64,30,25)\n",
      "[A3C] Cluster 11 | Epoch 1/3 | AvgLoss -0.008499 | batches 60\n",
      "[A3C] Cluster 11 | Epoch 2/3 | AvgLoss -0.010865 | batches 60\n",
      "[A3C] Cluster 11 | Epoch 3/3 | AvgLoss -0.011333 | batches 60\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_11.pt\n",
      "[Cluster 12] loaded tensor ./tensors/cluster_12_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 12 | Epoch 1/3 | AvgLoss -0.020619 | batches 52\n",
      "[A3C] Cluster 12 | Epoch 2/3 | AvgLoss -0.011108 | batches 52\n",
      "[A3C] Cluster 12 | Epoch 3/3 | AvgLoss -0.011437 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_12.pt\n",
      "[Cluster 13] loaded tensor ./tensors/cluster_13_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 13 | Epoch 1/3 | AvgLoss -0.009229 | batches 56\n",
      "[A3C] Cluster 13 | Epoch 2/3 | AvgLoss -0.011080 | batches 56\n",
      "[A3C] Cluster 13 | Epoch 3/3 | AvgLoss -0.011293 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_13.pt\n",
      "[Cluster 14] loaded tensor ./tensors/cluster_14_tensor.npy -> (B,T,N,F)=(509,64,31,25)\n",
      "[A3C] Cluster 14 | Epoch 1/3 | AvgLoss -0.002871 | batches 62\n",
      "[A3C] Cluster 14 | Epoch 2/3 | AvgLoss -0.011028 | batches 62\n",
      "[A3C] Cluster 14 | Epoch 3/3 | AvgLoss -0.011189 | batches 62\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_14.pt\n",
      "[Cluster 15] loaded tensor ./tensors/cluster_15_tensor.npy -> (B,T,N,F)=(509,64,30,25)\n",
      "[A3C] Cluster 15 | Epoch 1/3 | AvgLoss -0.004748 | batches 60\n",
      "[A3C] Cluster 15 | Epoch 2/3 | AvgLoss -0.011807 | batches 60\n",
      "[A3C] Cluster 15 | Epoch 3/3 | AvgLoss -0.011245 | batches 60\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_15.pt\n",
      "[Cluster 16] loaded tensor ./tensors/cluster_16_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 16 | Epoch 1/3 | AvgLoss -0.016575 | batches 56\n",
      "[A3C] Cluster 16 | Epoch 2/3 | AvgLoss -0.010250 | batches 56\n",
      "[A3C] Cluster 16 | Epoch 3/3 | AvgLoss -0.011287 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_16.pt\n",
      "[Cluster 17] loaded tensor ./tensors/cluster_17_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 17 | Epoch 1/3 | AvgLoss -0.009608 | batches 56\n",
      "[A3C] Cluster 17 | Epoch 2/3 | AvgLoss -0.011548 | batches 56\n",
      "[A3C] Cluster 17 | Epoch 3/3 | AvgLoss -0.011663 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_17.pt\n",
      "[Cluster 18] loaded tensor ./tensors/cluster_18_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 18 | Epoch 1/3 | AvgLoss -0.011493 | batches 58\n",
      "[A3C] Cluster 18 | Epoch 2/3 | AvgLoss -0.011139 | batches 58\n",
      "[A3C] Cluster 18 | Epoch 3/3 | AvgLoss -0.011471 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_18.pt\n",
      "[Cluster 19] loaded tensor ./tensors/cluster_19_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 19 | Epoch 1/3 | AvgLoss -0.011301 | batches 58\n",
      "[A3C] Cluster 19 | Epoch 2/3 | AvgLoss -0.011133 | batches 58\n",
      "[A3C] Cluster 19 | Epoch 3/3 | AvgLoss -0.011735 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_19.pt\n",
      "[Cluster 20] loaded tensor ./tensors/cluster_20_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 20 | Epoch 1/3 | AvgLoss -0.013625 | batches 52\n",
      "[A3C] Cluster 20 | Epoch 2/3 | AvgLoss -0.011106 | batches 52\n",
      "[A3C] Cluster 20 | Epoch 3/3 | AvgLoss -0.011418 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_20.pt\n",
      "[Cluster 21] loaded tensor ./tensors/cluster_21_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 21 | Epoch 1/3 | AvgLoss -0.014014 | batches 56\n",
      "[A3C] Cluster 21 | Epoch 2/3 | AvgLoss -0.010004 | batches 56\n",
      "[A3C] Cluster 21 | Epoch 3/3 | AvgLoss -0.011615 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_21.pt\n",
      "[Cluster 22] loaded tensor ./tensors/cluster_22_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 22 | Epoch 1/3 | AvgLoss -0.009127 | batches 56\n",
      "[A3C] Cluster 22 | Epoch 2/3 | AvgLoss -0.011596 | batches 56\n",
      "[A3C] Cluster 22 | Epoch 3/3 | AvgLoss -0.011509 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_22.pt\n",
      "[Cluster 23] loaded tensor ./tensors/cluster_23_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 23 | Epoch 1/3 | AvgLoss -0.007897 | batches 56\n",
      "[A3C] Cluster 23 | Epoch 2/3 | AvgLoss -0.010976 | batches 56\n",
      "[A3C] Cluster 23 | Epoch 3/3 | AvgLoss -0.011273 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_23.pt\n",
      "[Cluster 24] loaded tensor ./tensors/cluster_24_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 24 | Epoch 1/3 | AvgLoss -0.008949 | batches 56\n",
      "[A3C] Cluster 24 | Epoch 2/3 | AvgLoss -0.011350 | batches 56\n",
      "[A3C] Cluster 24 | Epoch 3/3 | AvgLoss -0.011692 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_24.pt\n",
      "[Cluster 25] loaded tensor ./tensors/cluster_25_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 25 | Epoch 1/3 | AvgLoss -0.013995 | batches 52\n",
      "[A3C] Cluster 25 | Epoch 2/3 | AvgLoss -0.010686 | batches 52\n",
      "[A3C] Cluster 25 | Epoch 3/3 | AvgLoss -0.011625 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_25.pt\n",
      "[Cluster 26] loaded tensor ./tensors/cluster_26_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 26 | Epoch 1/3 | AvgLoss -0.008012 | batches 52\n",
      "[A3C] Cluster 26 | Epoch 2/3 | AvgLoss -0.010864 | batches 52\n",
      "[A3C] Cluster 26 | Epoch 3/3 | AvgLoss -0.011080 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_26.pt\n",
      "[Cluster 27] loaded tensor ./tensors/cluster_27_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 27 | Epoch 1/3 | AvgLoss -0.012213 | batches 52\n",
      "[A3C] Cluster 27 | Epoch 2/3 | AvgLoss -0.010746 | batches 52\n",
      "[A3C] Cluster 27 | Epoch 3/3 | AvgLoss -0.011140 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_27.pt\n",
      "[Cluster 28] loaded tensor ./tensors/cluster_28_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 28 | Epoch 1/3 | AvgLoss -0.017935 | batches 58\n",
      "[A3C] Cluster 28 | Epoch 2/3 | AvgLoss -0.010822 | batches 58\n",
      "[A3C] Cluster 28 | Epoch 3/3 | AvgLoss -0.011519 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_28.pt\n",
      "[Cluster 29] loaded tensor ./tensors/cluster_29_tensor.npy -> (B,T,N,F)=(509,64,25,25)\n",
      "[A3C] Cluster 29 | Epoch 1/3 | AvgLoss -0.012713 | batches 50\n",
      "[A3C] Cluster 29 | Epoch 2/3 | AvgLoss -0.009681 | batches 50\n",
      "[A3C] Cluster 29 | Epoch 3/3 | AvgLoss -0.013639 | batches 50\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_29.pt\n",
      "[Cluster 30] loaded tensor ./tensors/cluster_30_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 30 | Epoch 1/3 | AvgLoss -0.012925 | batches 58\n",
      "[A3C] Cluster 30 | Epoch 2/3 | AvgLoss -0.011357 | batches 58\n",
      "[A3C] Cluster 30 | Epoch 3/3 | AvgLoss -0.011273 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_30.pt\n",
      "[Cluster 31] loaded tensor ./tensors/cluster_31_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 31 | Epoch 1/3 | AvgLoss -0.011692 | batches 52\n",
      "[A3C] Cluster 31 | Epoch 2/3 | AvgLoss -0.012072 | batches 52\n",
      "[A3C] Cluster 31 | Epoch 3/3 | AvgLoss -0.010402 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_31.pt\n",
      "[Cluster 32] loaded tensor ./tensors/cluster_32_tensor.npy -> (B,T,N,F)=(509,64,29,25)\n",
      "[A3C] Cluster 32 | Epoch 1/3 | AvgLoss -0.010445 | batches 58\n",
      "[A3C] Cluster 32 | Epoch 2/3 | AvgLoss -0.010631 | batches 58\n",
      "[A3C] Cluster 32 | Epoch 3/3 | AvgLoss -0.011301 | batches 58\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_32.pt\n",
      "[Cluster 33] loaded tensor ./tensors/cluster_33_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 33 | Epoch 1/3 | AvgLoss -0.013285 | batches 52\n",
      "[A3C] Cluster 33 | Epoch 2/3 | AvgLoss -0.010802 | batches 52\n",
      "[A3C] Cluster 33 | Epoch 3/3 | AvgLoss -0.011062 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_33.pt\n",
      "[Cluster 34] loaded tensor ./tensors/cluster_34_tensor.npy -> (B,T,N,F)=(509,64,25,25)\n",
      "[A3C] Cluster 34 | Epoch 1/3 | AvgLoss -0.021212 | batches 50\n",
      "[A3C] Cluster 34 | Epoch 2/3 | AvgLoss -0.006992 | batches 50\n",
      "[A3C] Cluster 34 | Epoch 3/3 | AvgLoss -0.017058 | batches 50\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_34.pt\n",
      "[Cluster 35] loaded tensor ./tensors/cluster_35_tensor.npy -> (B,T,N,F)=(509,64,24,25)\n",
      "[A3C] Cluster 35 | Epoch 1/3 | AvgLoss 0.001937 | batches 48\n",
      "[A3C] Cluster 35 | Epoch 2/3 | AvgLoss -0.010240 | batches 48\n",
      "[A3C] Cluster 35 | Epoch 3/3 | AvgLoss -0.010629 | batches 48\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_35.pt\n",
      "[Cluster 36] loaded tensor ./tensors/cluster_36_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 36 | Epoch 1/3 | AvgLoss -0.009640 | batches 56\n",
      "[A3C] Cluster 36 | Epoch 2/3 | AvgLoss -0.010872 | batches 56\n",
      "[A3C] Cluster 36 | Epoch 3/3 | AvgLoss -0.010966 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_36.pt\n",
      "[Cluster 37] loaded tensor ./tensors/cluster_37_tensor.npy -> (B,T,N,F)=(509,64,25,25)\n",
      "[A3C] Cluster 37 | Epoch 1/3 | AvgLoss -0.004563 | batches 50\n",
      "[A3C] Cluster 37 | Epoch 2/3 | AvgLoss -0.014956 | batches 50\n",
      "[A3C] Cluster 37 | Epoch 3/3 | AvgLoss -0.005581 | batches 50\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_37.pt\n",
      "[Cluster 38] loaded tensor ./tensors/cluster_38_tensor.npy -> (B,T,N,F)=(509,64,27,25)\n",
      "[A3C] Cluster 38 | Epoch 1/3 | AvgLoss -0.027363 | batches 54\n",
      "[A3C] Cluster 38 | Epoch 2/3 | AvgLoss -0.010432 | batches 54\n",
      "[A3C] Cluster 38 | Epoch 3/3 | AvgLoss -0.011432 | batches 54\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_38.pt\n",
      "[Cluster 39] loaded tensor ./tensors/cluster_39_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 39 | Epoch 1/3 | AvgLoss -0.009479 | batches 56\n",
      "[A3C] Cluster 39 | Epoch 2/3 | AvgLoss -0.010927 | batches 56\n",
      "[A3C] Cluster 39 | Epoch 3/3 | AvgLoss -0.011466 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_39.pt\n",
      "[Cluster 40] loaded tensor ./tensors/cluster_40_tensor.npy -> (B,T,N,F)=(509,64,24,25)\n",
      "[A3C] Cluster 40 | Epoch 1/3 | AvgLoss -0.009636 | batches 48\n",
      "[A3C] Cluster 40 | Epoch 2/3 | AvgLoss -0.011024 | batches 48\n",
      "[A3C] Cluster 40 | Epoch 3/3 | AvgLoss -0.011344 | batches 48\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_40.pt\n",
      "[Cluster 41] loaded tensor ./tensors/cluster_41_tensor.npy -> (B,T,N,F)=(509,64,28,25)\n",
      "[A3C] Cluster 41 | Epoch 1/3 | AvgLoss -0.013449 | batches 56\n",
      "[A3C] Cluster 41 | Epoch 2/3 | AvgLoss -0.010980 | batches 56\n",
      "[A3C] Cluster 41 | Epoch 3/3 | AvgLoss -0.011236 | batches 56\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_41.pt\n",
      "[Cluster 42] loaded tensor ./tensors/cluster_42_tensor.npy -> (B,T,N,F)=(509,64,27,25)\n",
      "[A3C] Cluster 42 | Epoch 1/3 | AvgLoss -0.003589 | batches 54\n",
      "[A3C] Cluster 42 | Epoch 2/3 | AvgLoss -0.010678 | batches 54\n",
      "[A3C] Cluster 42 | Epoch 3/3 | AvgLoss -0.011212 | batches 54\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_42.pt\n",
      "[Cluster 43] loaded tensor ./tensors/cluster_43_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 43 | Epoch 1/3 | AvgLoss -0.010179 | batches 46\n",
      "[A3C] Cluster 43 | Epoch 2/3 | AvgLoss -0.010890 | batches 46\n",
      "[A3C] Cluster 43 | Epoch 3/3 | AvgLoss -0.011684 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_43.pt\n",
      "[Cluster 44] loaded tensor ./tensors/cluster_44_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 44 | Epoch 1/3 | AvgLoss -0.008254 | batches 46\n",
      "[A3C] Cluster 44 | Epoch 2/3 | AvgLoss -0.010937 | batches 46\n",
      "[A3C] Cluster 44 | Epoch 3/3 | AvgLoss -0.011047 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_44.pt\n",
      "[Cluster 45] loaded tensor ./tensors/cluster_45_tensor.npy -> (B,T,N,F)=(509,64,22,25)\n",
      "[A3C] Cluster 45 | Epoch 1/3 | AvgLoss -0.008240 | batches 44\n",
      "[A3C] Cluster 45 | Epoch 2/3 | AvgLoss -0.010841 | batches 44\n",
      "[A3C] Cluster 45 | Epoch 3/3 | AvgLoss -0.010752 | batches 44\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_45.pt\n",
      "[Cluster 46] loaded tensor ./tensors/cluster_46_tensor.npy -> (B,T,N,F)=(509,64,25,25)\n",
      "[A3C] Cluster 46 | Epoch 1/3 | AvgLoss -0.009266 | batches 50\n",
      "[A3C] Cluster 46 | Epoch 2/3 | AvgLoss -0.013160 | batches 50\n",
      "[A3C] Cluster 46 | Epoch 3/3 | AvgLoss -0.011794 | batches 50\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_46.pt\n",
      "[Cluster 47] loaded tensor ./tensors/cluster_47_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 47 | Epoch 1/3 | AvgLoss -0.006949 | batches 46\n",
      "[A3C] Cluster 47 | Epoch 2/3 | AvgLoss -0.009978 | batches 46\n",
      "[A3C] Cluster 47 | Epoch 3/3 | AvgLoss -0.010845 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_47.pt\n",
      "[Cluster 48] loaded tensor ./tensors/cluster_48_tensor.npy -> (B,T,N,F)=(509,64,24,25)\n",
      "[A3C] Cluster 48 | Epoch 1/3 | AvgLoss -0.007167 | batches 48\n",
      "[A3C] Cluster 48 | Epoch 2/3 | AvgLoss -0.011200 | batches 48\n",
      "[A3C] Cluster 48 | Epoch 3/3 | AvgLoss -0.011233 | batches 48\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_48.pt\n",
      "[Cluster 49] loaded tensor ./tensors/cluster_49_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 49 | Epoch 1/3 | AvgLoss -0.021697 | batches 46\n",
      "[A3C] Cluster 49 | Epoch 2/3 | AvgLoss -0.011667 | batches 46\n",
      "[A3C] Cluster 49 | Epoch 3/3 | AvgLoss -0.011060 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_49.pt\n",
      "[Cluster 50] loaded tensor ./tensors/cluster_50_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 50 | Epoch 1/3 | AvgLoss -0.015368 | batches 52\n",
      "[A3C] Cluster 50 | Epoch 2/3 | AvgLoss -0.010230 | batches 52\n",
      "[A3C] Cluster 50 | Epoch 3/3 | AvgLoss -0.011595 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_50.pt\n",
      "[Cluster 51] loaded tensor ./tensors/cluster_51_tensor.npy -> (B,T,N,F)=(509,64,26,25)\n",
      "[A3C] Cluster 51 | Epoch 1/3 | AvgLoss -0.004079 | batches 52\n",
      "[A3C] Cluster 51 | Epoch 2/3 | AvgLoss -0.010777 | batches 52\n",
      "[A3C] Cluster 51 | Epoch 3/3 | AvgLoss -0.010979 | batches 52\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_51.pt\n",
      "[Cluster 52] loaded tensor ./tensors/cluster_52_tensor.npy -> (B,T,N,F)=(509,64,22,25)\n",
      "[A3C] Cluster 52 | Epoch 1/3 | AvgLoss -0.010660 | batches 44\n",
      "[A3C] Cluster 52 | Epoch 2/3 | AvgLoss -0.011576 | batches 44\n",
      "[A3C] Cluster 52 | Epoch 3/3 | AvgLoss -0.011485 | batches 44\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_52.pt\n",
      "[Cluster 53] loaded tensor ./tensors/cluster_53_tensor.npy -> (B,T,N,F)=(509,64,25,25)\n",
      "[A3C] Cluster 53 | Epoch 1/3 | AvgLoss -0.002787 | batches 50\n",
      "[A3C] Cluster 53 | Epoch 2/3 | AvgLoss -0.006722 | batches 50\n",
      "[A3C] Cluster 53 | Epoch 3/3 | AvgLoss -0.007800 | batches 50\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_53.pt\n",
      "[Cluster 54] loaded tensor ./tensors/cluster_54_tensor.npy -> (B,T,N,F)=(509,64,18,25)\n",
      "[A3C] Cluster 54 | Epoch 1/3 | AvgLoss -0.009489 | batches 36\n",
      "[A3C] Cluster 54 | Epoch 2/3 | AvgLoss -0.009713 | batches 36\n",
      "[A3C] Cluster 54 | Epoch 3/3 | AvgLoss -0.010897 | batches 36\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_54.pt\n",
      "[Cluster 55] loaded tensor ./tensors/cluster_55_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 55 | Epoch 1/3 | AvgLoss -0.012298 | batches 46\n",
      "[A3C] Cluster 55 | Epoch 2/3 | AvgLoss -0.010337 | batches 46\n",
      "[A3C] Cluster 55 | Epoch 3/3 | AvgLoss -0.011661 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_55.pt\n",
      "[Cluster 56] loaded tensor ./tensors/cluster_56_tensor.npy -> (B,T,N,F)=(509,64,24,25)\n",
      "[A3C] Cluster 56 | Epoch 1/3 | AvgLoss -0.007420 | batches 48\n",
      "[A3C] Cluster 56 | Epoch 2/3 | AvgLoss -0.009914 | batches 48\n",
      "[A3C] Cluster 56 | Epoch 3/3 | AvgLoss -0.011537 | batches 48\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_56.pt\n",
      "[Cluster 57] loaded tensor ./tensors/cluster_57_tensor.npy -> (B,T,N,F)=(509,64,24,25)\n",
      "[A3C] Cluster 57 | Epoch 1/3 | AvgLoss -0.010064 | batches 48\n",
      "[A3C] Cluster 57 | Epoch 2/3 | AvgLoss -0.010858 | batches 48\n",
      "[A3C] Cluster 57 | Epoch 3/3 | AvgLoss -0.010924 | batches 48\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_57.pt\n",
      "[Cluster 58] loaded tensor ./tensors/cluster_58_tensor.npy -> (B,T,N,F)=(509,64,22,25)\n",
      "[A3C] Cluster 58 | Epoch 1/3 | AvgLoss -0.008345 | batches 44\n",
      "[A3C] Cluster 58 | Epoch 2/3 | AvgLoss -0.010627 | batches 44\n",
      "[A3C] Cluster 58 | Epoch 3/3 | AvgLoss -0.011068 | batches 44\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_58.pt\n",
      "[Cluster 59] loaded tensor ./tensors/cluster_59_tensor.npy -> (B,T,N,F)=(509,64,21,25)\n",
      "[A3C] Cluster 59 | Epoch 1/3 | AvgLoss -0.015904 | batches 42\n",
      "[A3C] Cluster 59 | Epoch 2/3 | AvgLoss -0.011926 | batches 42\n",
      "[A3C] Cluster 59 | Epoch 3/3 | AvgLoss -0.011977 | batches 42\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_59.pt\n",
      "[Cluster 60] loaded tensor ./tensors/cluster_60_tensor.npy -> (B,T,N,F)=(509,64,20,25)\n",
      "[A3C] Cluster 60 | Epoch 1/3 | AvgLoss -0.010213 | batches 40\n",
      "[A3C] Cluster 60 | Epoch 2/3 | AvgLoss -0.009958 | batches 40\n",
      "[A3C] Cluster 60 | Epoch 3/3 | AvgLoss -0.011343 | batches 40\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_60.pt\n",
      "[Cluster 61] loaded tensor ./tensors/cluster_61_tensor.npy -> (B,T,N,F)=(509,64,25,25)\n",
      "[A3C] Cluster 61 | Epoch 1/3 | AvgLoss -0.005445 | batches 50\n",
      "[A3C] Cluster 61 | Epoch 2/3 | AvgLoss -0.008658 | batches 50\n",
      "[A3C] Cluster 61 | Epoch 3/3 | AvgLoss -0.007120 | batches 50\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_61.pt\n",
      "[Cluster 62] loaded tensor ./tensors/cluster_62_tensor.npy -> (B,T,N,F)=(509,64,22,25)\n",
      "[A3C] Cluster 62 | Epoch 1/3 | AvgLoss -0.010961 | batches 44\n",
      "[A3C] Cluster 62 | Epoch 2/3 | AvgLoss -0.011120 | batches 44\n",
      "[A3C] Cluster 62 | Epoch 3/3 | AvgLoss -0.010807 | batches 44\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_62.pt\n",
      "[Cluster 63] loaded tensor ./tensors/cluster_63_tensor.npy -> (B,T,N,F)=(509,64,23,25)\n",
      "[A3C] Cluster 63 | Epoch 1/3 | AvgLoss -0.011761 | batches 46\n",
      "[A3C] Cluster 63 | Epoch 2/3 | AvgLoss -0.009384 | batches 46\n",
      "[A3C] Cluster 63 | Epoch 3/3 | AvgLoss -0.011012 | batches 46\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_63.pt\n",
      "[Cluster 64] loaded tensor ./tensors/cluster_64_tensor.npy -> (B,T,N,F)=(509,64,24,25)\n",
      "[A3C] Cluster 64 | Epoch 1/3 | AvgLoss -0.004144 | batches 48\n",
      "[A3C] Cluster 64 | Epoch 2/3 | AvgLoss -0.010695 | batches 48\n",
      "[A3C] Cluster 64 | Epoch 3/3 | AvgLoss -0.010655 | batches 48\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_64.pt\n",
      "[Cluster 65] loaded tensor ./tensors/cluster_65_tensor.npy -> (B,T,N,F)=(509,64,21,25)\n",
      "[A3C] Cluster 65 | Epoch 1/3 | AvgLoss -0.009425 | batches 42\n",
      "[A3C] Cluster 65 | Epoch 2/3 | AvgLoss -0.009857 | batches 42\n",
      "[A3C] Cluster 65 | Epoch 3/3 | AvgLoss -0.011254 | batches 42\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_65.pt\n",
      "[Cluster 66] loaded tensor ./tensors/cluster_66_tensor.npy -> (B,T,N,F)=(509,64,21,25)\n",
      "[A3C] Cluster 66 | Epoch 1/3 | AvgLoss -0.006792 | batches 42\n",
      "[A3C] Cluster 66 | Epoch 2/3 | AvgLoss -0.012133 | batches 42\n",
      "[A3C] Cluster 66 | Epoch 3/3 | AvgLoss -0.010969 | batches 42\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_66.pt\n",
      "[Cluster 67] loaded tensor ./tensors/cluster_67_tensor.npy -> (B,T,N,F)=(509,64,17,25)\n",
      "[A3C] Cluster 67 | Epoch 1/3 | AvgLoss -0.010860 | batches 34\n",
      "[A3C] Cluster 67 | Epoch 2/3 | AvgLoss -0.010376 | batches 34\n",
      "[A3C] Cluster 67 | Epoch 3/3 | AvgLoss -0.011671 | batches 34\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_67.pt\n",
      "[Cluster 68] loaded tensor ./tensors/cluster_68_tensor.npy -> (B,T,N,F)=(509,64,19,25)\n",
      "[A3C] Cluster 68 | Epoch 1/3 | AvgLoss -0.025528 | batches 38\n",
      "[A3C] Cluster 68 | Epoch 2/3 | AvgLoss -0.013221 | batches 38\n",
      "[A3C] Cluster 68 | Epoch 3/3 | AvgLoss -0.011151 | batches 38\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_68.pt\n",
      "[Cluster 69] loaded tensor ./tensors/cluster_69_tensor.npy -> (B,T,N,F)=(509,64,16,25)\n",
      "[A3C] Cluster 69 | Epoch 1/3 | AvgLoss -0.034411 | batches 32\n",
      "[A3C] Cluster 69 | Epoch 2/3 | AvgLoss -0.003414 | batches 32\n",
      "[A3C] Cluster 69 | Epoch 3/3 | AvgLoss -0.013095 | batches 32\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_69.pt\n",
      "[Cluster 70] loaded tensor ./tensors/cluster_70_tensor.npy -> (B,T,N,F)=(509,64,16,25)\n",
      "[A3C] Cluster 70 | Epoch 1/3 | AvgLoss 0.002910 | batches 32\n",
      "[A3C] Cluster 70 | Epoch 2/3 | AvgLoss -0.007273 | batches 32\n",
      "[A3C] Cluster 70 | Epoch 3/3 | AvgLoss -0.011012 | batches 32\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_70.pt\n",
      "[Cluster 71] loaded tensor ./tensors/cluster_71_tensor.npy -> (B,T,N,F)=(509,64,15,25)\n",
      "[A3C] Cluster 71 | Epoch 1/3 | AvgLoss -0.011283 | batches 30\n",
      "[A3C] Cluster 71 | Epoch 2/3 | AvgLoss -0.009183 | batches 30\n",
      "[A3C] Cluster 71 | Epoch 3/3 | AvgLoss -0.009563 | batches 30\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_71.pt\n",
      "[Cluster 72] loaded tensor ./tensors/cluster_72_tensor.npy -> (B,T,N,F)=(509,64,14,25)\n",
      "[A3C] Cluster 72 | Epoch 1/3 | AvgLoss -0.009049 | batches 28\n",
      "[A3C] Cluster 72 | Epoch 2/3 | AvgLoss -0.007526 | batches 28\n",
      "[A3C] Cluster 72 | Epoch 3/3 | AvgLoss -0.012999 | batches 28\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_72.pt\n",
      "[Cluster 73] loaded tensor ./tensors/cluster_73_tensor.npy -> (B,T,N,F)=(509,64,13,25)\n",
      "[A3C] Cluster 73 | Epoch 1/3 | AvgLoss -0.014348 | batches 26\n",
      "[A3C] Cluster 73 | Epoch 2/3 | AvgLoss -0.012020 | batches 26\n",
      "[A3C] Cluster 73 | Epoch 3/3 | AvgLoss -0.010411 | batches 26\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_73.pt\n",
      "[Cluster 74] loaded tensor ./tensors/cluster_74_tensor.npy -> (B,T,N,F)=(509,64,12,25)\n",
      "[A3C] Cluster 74 | Epoch 1/3 | AvgLoss -0.024594 | batches 24\n",
      "[A3C] Cluster 74 | Epoch 2/3 | AvgLoss -0.010962 | batches 24\n",
      "[A3C] Cluster 74 | Epoch 3/3 | AvgLoss -0.009690 | batches 24\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_74.pt\n",
      "[Cluster 75] loaded tensor ./tensors/cluster_75_tensor.npy -> (B,T,N,F)=(509,64,15,25)\n",
      "[A3C] Cluster 75 | Epoch 1/3 | AvgLoss -0.019722 | batches 30\n",
      "[A3C] Cluster 75 | Epoch 2/3 | AvgLoss -0.009804 | batches 30\n",
      "[A3C] Cluster 75 | Epoch 3/3 | AvgLoss -0.013660 | batches 30\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_75.pt\n",
      "[Cluster 76] loaded tensor ./tensors/cluster_76_tensor.npy -> (B,T,N,F)=(509,64,11,25)\n",
      "[A3C] Cluster 76 | Epoch 1/3 | AvgLoss -0.018907 | batches 22\n",
      "[A3C] Cluster 76 | Epoch 2/3 | AvgLoss -0.011948 | batches 22\n",
      "[A3C] Cluster 76 | Epoch 3/3 | AvgLoss -0.011075 | batches 22\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_76.pt\n",
      "[Cluster 77] loaded tensor ./tensors/cluster_77_tensor.npy -> (B,T,N,F)=(509,64,10,25)\n",
      "[A3C] Cluster 77 | Epoch 1/3 | AvgLoss -0.005019 | batches 20\n",
      "[A3C] Cluster 77 | Epoch 2/3 | AvgLoss -0.007827 | batches 20\n",
      "[A3C] Cluster 77 | Epoch 3/3 | AvgLoss -0.012307 | batches 20\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_77.pt\n",
      "[Cluster 78] loaded tensor ./tensors/cluster_78_tensor.npy -> (B,T,N,F)=(509,64,10,25)\n",
      "[A3C] Cluster 78 | Epoch 1/3 | AvgLoss 0.005477 | batches 20\n",
      "[A3C] Cluster 78 | Epoch 2/3 | AvgLoss -0.010388 | batches 20\n",
      "[A3C] Cluster 78 | Epoch 3/3 | AvgLoss -0.014095 | batches 20\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_78.pt\n",
      "[Cluster 79] loaded tensor ./tensors/cluster_79_tensor.npy -> (B,T,N,F)=(509,64,10,25)\n",
      "[A3C] Cluster 79 | Epoch 1/3 | AvgLoss -0.011317 | batches 20\n",
      "[A3C] Cluster 79 | Epoch 2/3 | AvgLoss -0.011960 | batches 20\n",
      "[A3C] Cluster 79 | Epoch 3/3 | AvgLoss -0.008563 | batches 20\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_79.pt\n",
      "[Cluster 80] loaded tensor ./tensors/cluster_80_tensor.npy -> (B,T,N,F)=(509,64,12,25)\n",
      "[A3C] Cluster 80 | Epoch 1/3 | AvgLoss -0.005446 | batches 24\n",
      "[A3C] Cluster 80 | Epoch 2/3 | AvgLoss -0.005868 | batches 24\n",
      "[A3C] Cluster 80 | Epoch 3/3 | AvgLoss -0.013308 | batches 24\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_80.pt\n",
      "[Cluster 81] loaded tensor ./tensors/cluster_81_tensor.npy -> (B,T,N,F)=(509,64,11,25)\n",
      "[A3C] Cluster 81 | Epoch 1/3 | AvgLoss -0.012921 | batches 22\n",
      "[A3C] Cluster 81 | Epoch 2/3 | AvgLoss -0.010250 | batches 22\n",
      "[A3C] Cluster 81 | Epoch 3/3 | AvgLoss -0.010461 | batches 22\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_81.pt\n",
      "[Cluster 82] loaded tensor ./tensors/cluster_82_tensor.npy -> (B,T,N,F)=(509,64,8,25)\n",
      "[A3C] Cluster 82 | Epoch 1/3 | AvgLoss -0.014493 | batches 16\n",
      "[A3C] Cluster 82 | Epoch 2/3 | AvgLoss -0.002717 | batches 16\n",
      "[A3C] Cluster 82 | Epoch 3/3 | AvgLoss -0.010178 | batches 16\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_82.pt\n",
      "[Cluster 83] loaded tensor ./tensors/cluster_83_tensor.npy -> (B,T,N,F)=(509,64,5,25)\n",
      "[A3C] Cluster 83 | Epoch 1/3 | AvgLoss -0.000990 | batches 10\n",
      "[A3C] Cluster 83 | Epoch 2/3 | AvgLoss -0.027931 | batches 10\n",
      "[A3C] Cluster 83 | Epoch 3/3 | AvgLoss -0.003597 | batches 10\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_83.pt\n",
      "[Cluster 84] loaded tensor ./tensors/cluster_84_tensor.npy -> (B,T,N,F)=(509,64,6,25)\n",
      "[A3C] Cluster 84 | Epoch 1/3 | AvgLoss -0.017898 | batches 12\n",
      "[A3C] Cluster 84 | Epoch 2/3 | AvgLoss -0.012492 | batches 12\n",
      "[A3C] Cluster 84 | Epoch 3/3 | AvgLoss -0.000859 | batches 12\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_84.pt\n",
      "[Cluster 85] loaded tensor ./tensors/cluster_85_tensor.npy -> (B,T,N,F)=(509,64,6,25)\n",
      "[A3C] Cluster 85 | Epoch 1/3 | AvgLoss -0.009774 | batches 12\n",
      "[A3C] Cluster 85 | Epoch 2/3 | AvgLoss -0.011686 | batches 12\n",
      "[A3C] Cluster 85 | Epoch 3/3 | AvgLoss -0.006466 | batches 12\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_85.pt\n",
      "[Cluster 86] loaded tensor ./tensors/cluster_86_tensor.npy -> (B,T,N,F)=(509,64,5,25)\n",
      "[A3C] Cluster 86 | Epoch 1/3 | AvgLoss -0.053401 | batches 10\n",
      "[A3C] Cluster 86 | Epoch 2/3 | AvgLoss 0.003107 | batches 10\n",
      "[A3C] Cluster 86 | Epoch 3/3 | AvgLoss -0.014589 | batches 10\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_86.pt\n",
      "[Cluster 87] loaded tensor ./tensors/cluster_87_tensor.npy -> (B,T,N,F)=(509,64,5,25)\n",
      "[A3C] Cluster 87 | Epoch 1/3 | AvgLoss 0.014564 | batches 10\n",
      "[A3C] Cluster 87 | Epoch 2/3 | AvgLoss -0.025146 | batches 10\n",
      "[A3C] Cluster 87 | Epoch 3/3 | AvgLoss -0.004954 | batches 10\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_87.pt\n",
      "[Cluster 88] loaded tensor ./tensors/cluster_88_tensor.npy -> (B,T,N,F)=(509,64,5,25)\n",
      "[A3C] Cluster 88 | Epoch 1/3 | AvgLoss -0.002991 | batches 10\n",
      "[A3C] Cluster 88 | Epoch 2/3 | AvgLoss -0.000932 | batches 10\n",
      "[A3C] Cluster 88 | Epoch 3/3 | AvgLoss -0.014455 | batches 10\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_88.pt\n",
      "[Cluster 89] loaded tensor ./tensors/cluster_89_tensor.npy -> (B,T,N,F)=(509,64,7,25)\n",
      "[A3C] Cluster 89 | Epoch 1/3 | AvgLoss -0.004378 | batches 14\n",
      "[A3C] Cluster 89 | Epoch 2/3 | AvgLoss -0.011110 | batches 14\n",
      "[A3C] Cluster 89 | Epoch 3/3 | AvgLoss -0.008122 | batches 14\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_89.pt\n",
      "[Cluster 90] loaded tensor ./tensors/cluster_90_tensor.npy -> (B,T,N,F)=(509,64,6,25)\n",
      "[A3C] Cluster 90 | Epoch 1/3 | AvgLoss -0.020005 | batches 12\n",
      "[A3C] Cluster 90 | Epoch 2/3 | AvgLoss -0.030166 | batches 12\n",
      "[A3C] Cluster 90 | Epoch 3/3 | AvgLoss 0.000265 | batches 12\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_90.pt\n",
      "[Cluster 91] loaded tensor ./tensors/cluster_91_tensor.npy -> (B,T,N,F)=(509,64,6,25)\n",
      "[A3C] Cluster 91 | Epoch 1/3 | AvgLoss -0.009424 | batches 12\n",
      "[A3C] Cluster 91 | Epoch 2/3 | AvgLoss -0.012214 | batches 12\n",
      "[A3C] Cluster 91 | Epoch 3/3 | AvgLoss -0.009666 | batches 12\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_91.pt\n",
      "[Cluster 92] loaded tensor ./tensors/cluster_92_tensor.npy -> (B,T,N,F)=(509,64,4,25)\n",
      "[A3C] Cluster 92 | Epoch 1/3 | AvgLoss -0.022244 | batches 8\n",
      "[A3C] Cluster 92 | Epoch 2/3 | AvgLoss -0.000519 | batches 8\n",
      "[A3C] Cluster 92 | Epoch 3/3 | AvgLoss -0.015909 | batches 8\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_92.pt\n",
      "[Cluster 93] loaded tensor ./tensors/cluster_93_tensor.npy -> (B,T,N,F)=(509,64,6,25)\n",
      "[A3C] Cluster 93 | Epoch 1/3 | AvgLoss -0.013623 | batches 12\n",
      "[A3C] Cluster 93 | Epoch 2/3 | AvgLoss -0.029954 | batches 12\n",
      "[A3C] Cluster 93 | Epoch 3/3 | AvgLoss 0.000957 | batches 12\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_93.pt\n",
      "[Cluster 94] loaded tensor ./tensors/cluster_94_tensor.npy -> (B,T,N,F)=(509,64,6,25)\n",
      "[A3C] Cluster 94 | Epoch 1/3 | AvgLoss 0.009341 | batches 12\n",
      "[A3C] Cluster 94 | Epoch 2/3 | AvgLoss -0.006429 | batches 12\n",
      "[A3C] Cluster 94 | Epoch 3/3 | AvgLoss -0.020628 | batches 12\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_94.pt\n",
      "[Cluster 95] loaded tensor ./tensors/cluster_95_tensor.npy -> (B,T,N,F)=(509,64,3,25)\n",
      "[A3C] Cluster 95 | Epoch 1/3 | AvgLoss -0.050777 | batches 6\n",
      "[A3C] Cluster 95 | Epoch 2/3 | AvgLoss -0.007084 | batches 6\n",
      "[A3C] Cluster 95 | Epoch 3/3 | AvgLoss 0.010248 | batches 6\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_95.pt\n",
      "[Cluster 96] loaded tensor ./tensors/cluster_96_tensor.npy -> (B,T,N,F)=(509,64,3,25)\n",
      "[A3C] Cluster 96 | Epoch 1/3 | AvgLoss 0.026120 | batches 6\n",
      "[A3C] Cluster 96 | Epoch 2/3 | AvgLoss -0.041643 | batches 6\n",
      "[A3C] Cluster 96 | Epoch 3/3 | AvgLoss -0.010313 | batches 6\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_96.pt\n",
      "[Cluster 97] loaded tensor ./tensors/cluster_97_tensor.npy -> (B,T,N,F)=(509,64,2,25)\n",
      "[A3C] Cluster 97 | Epoch 1/3 | AvgLoss 0.131929 | batches 4\n",
      "[A3C] Cluster 97 | Epoch 2/3 | AvgLoss 0.021206 | batches 4\n",
      "[A3C] Cluster 97 | Epoch 3/3 | AvgLoss -0.030631 | batches 4\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_97.pt\n",
      "[Cluster 98] loaded tensor ./tensors/cluster_98_tensor.npy -> (B,T,N,F)=(509,64,2,25)\n",
      "[A3C] Cluster 98 | Epoch 1/3 | AvgLoss -0.002790 | batches 4\n",
      "[A3C] Cluster 98 | Epoch 2/3 | AvgLoss 0.016535 | batches 4\n",
      "[A3C] Cluster 98 | Epoch 3/3 | AvgLoss -0.012839 | batches 4\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_98.pt\n",
      "[Cluster 99] loaded tensor ./tensors/cluster_99_tensor.npy -> (B,T,N,F)=(509,64,2,25)\n",
      "[A3C] Cluster 99 | Epoch 1/3 | AvgLoss -0.051797 | batches 4\n",
      "[A3C] Cluster 99 | Epoch 2/3 | AvgLoss -0.032623 | batches 4\n",
      "[A3C] Cluster 99 | Epoch 3/3 | AvgLoss -0.005317 | batches 4\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_99.pt\n",
      "[Cluster 100] loaded tensor ./tensors/cluster_100_tensor.npy -> (B,T,N,F)=(509,64,3,25)\n",
      "[A3C] Cluster 100 | Epoch 1/3 | AvgLoss 0.006112 | batches 6\n",
      "[A3C] Cluster 100 | Epoch 2/3 | AvgLoss -0.000699 | batches 6\n",
      "[A3C] Cluster 100 | Epoch 3/3 | AvgLoss -0.033711 | batches 6\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_100.pt\n",
      "[Cluster 101] loaded tensor ./tensors/cluster_101_tensor.npy -> (B,T,N,F)=(509,64,3,25)\n",
      "[A3C] Cluster 101 | Epoch 1/3 | AvgLoss 0.069462 | batches 6\n",
      "[A3C] Cluster 101 | Epoch 2/3 | AvgLoss -0.016413 | batches 6\n",
      "[A3C] Cluster 101 | Epoch 3/3 | AvgLoss -0.008147 | batches 6\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_101.pt\n",
      "[Cluster 102] loaded tensor ./tensors/cluster_102_tensor.npy -> (B,T,N,F)=(509,64,2,25)\n",
      "[A3C] Cluster 102 | Epoch 1/3 | AvgLoss -0.042800 | batches 4\n",
      "[A3C] Cluster 102 | Epoch 2/3 | AvgLoss -0.000549 | batches 4\n",
      "[A3C] Cluster 102 | Epoch 3/3 | AvgLoss -0.020327 | batches 4\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_102.pt\n",
      "[Cluster 103] loaded tensor ./tensors/cluster_103_tensor.npy -> (B,T,N,F)=(509,64,1,25)\n",
      "[A3C] Cluster 103 | Epoch 1/3 | AvgLoss 0.030083 | batches 2\n",
      "[A3C] Cluster 103 | Epoch 2/3 | AvgLoss -0.023374 | batches 2\n",
      "[A3C] Cluster 103 | Epoch 3/3 | AvgLoss -0.033763 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_103.pt\n",
      "[Cluster 104] loaded tensor ./tensors/cluster_104_tensor.npy -> (B,T,N,F)=(509,64,1,25)\n",
      "[A3C] Cluster 104 | Epoch 1/3 | AvgLoss 0.029432 | batches 2\n",
      "[A3C] Cluster 104 | Epoch 2/3 | AvgLoss -0.029487 | batches 2\n",
      "[A3C] Cluster 104 | Epoch 3/3 | AvgLoss -0.043498 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_104.pt\n",
      "[Cluster 105] loaded tensor ./tensors/cluster_105_tensor.npy -> (B,T,N,F)=(509,64,1,25)\n",
      "[A3C] Cluster 105 | Epoch 1/3 | AvgLoss -0.102863 | batches 2\n",
      "[A3C] Cluster 105 | Epoch 2/3 | AvgLoss -0.022026 | batches 2\n",
      "[A3C] Cluster 105 | Epoch 3/3 | AvgLoss 0.032945 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_105.pt\n",
      "[Cluster 106] loaded tensor ./tensors/cluster_106_tensor.npy -> (B,T,N,F)=(509,64,1,25)\n",
      "[A3C] Cluster 106 | Epoch 1/3 | AvgLoss 0.069570 | batches 2\n",
      "[A3C] Cluster 106 | Epoch 2/3 | AvgLoss -0.017020 | batches 2\n",
      "[A3C] Cluster 106 | Epoch 3/3 | AvgLoss -0.046101 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_106.pt\n",
      "[Cluster 107] loaded tensor ./tensors/cluster_107_tensor.npy -> (B,T,N,F)=(509,64,1,25)\n",
      "[A3C] Cluster 107 | Epoch 1/3 | AvgLoss -0.002696 | batches 2\n",
      "[A3C] Cluster 107 | Epoch 2/3 | AvgLoss -0.005469 | batches 2\n",
      "[A3C] Cluster 107 | Epoch 3/3 | AvgLoss -0.027979 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_107.pt\n",
      "[Cluster 108] loaded tensor ./tensors/cluster_108_tensor.npy -> (B,T,N,F)=(309,64,1,25)\n",
      "[A3C] Cluster 108 | Epoch 1/3 | AvgLoss 0.067384 | batches 2\n",
      "[A3C] Cluster 108 | Epoch 2/3 | AvgLoss -0.005871 | batches 2\n",
      "[A3C] Cluster 108 | Epoch 3/3 | AvgLoss -0.016885 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_108.pt\n",
      "[Cluster 109] loaded tensor ./tensors/cluster_109_tensor.npy -> (B,T,N,F)=(509,64,1,25)\n",
      "[A3C] Cluster 109 | Epoch 1/3 | AvgLoss -0.081682 | batches 2\n",
      "[A3C] Cluster 109 | Epoch 2/3 | AvgLoss -0.042788 | batches 2\n",
      "[A3C] Cluster 109 | Epoch 3/3 | AvgLoss 0.001626 | batches 2\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_109.pt\n",
      "[Cluster 110] loaded tensor ./tensors/cluster_110_tensor.npy -> (B,T,N,F)=(509,64,2,25)\n",
      "[A3C] Cluster 110 | Epoch 1/3 | AvgLoss -0.004087 | batches 4\n",
      "[A3C] Cluster 110 | Epoch 2/3 | AvgLoss -0.024067 | batches 4\n",
      "[A3C] Cluster 110 | Epoch 3/3 | AvgLoss -0.004198 | batches 4\n",
      "  ✅ Saved A3C checkpoint: ./models/a3c_cluster_110.pt\n",
      "✅ Done Block 8: A3C models trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Block 8 — Train A3C multi-stock per-cluster (Final, regime-aware, clipped rewards)\n",
    "\n",
    "import os, gc, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "DATA_DIR  = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Hyperparams (tune as needed) ---\n",
    "EPOCHS_DEFAULT = 3\n",
    "LR_DEFAULT     = 1e-3\n",
    "BATCH_SIZE_DEFAULT = 256\n",
    "REWARD_CLIP = 0.1      # clip rewards to [-REWARD_CLIP, +REWARD_CLIP]\n",
    "GRAD_CLIP = 1.0\n",
    "BETA_ENTROPY = 0.01\n",
    "\n",
    "# Load tensor metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# df_backtest must exist (produced in Block 7.5)\n",
    "# It must contain: ticker, timestamp, realized_return and regime one-hot columns if available\n",
    "if \"df_backtest\" not in globals():\n",
    "    # try to load from CSV if user saved it\n",
    "    fb = os.path.join(\"./backtest_ddpg/\", \"df_backtest.csv\")\n",
    "    if os.path.exists(fb):\n",
    "        df_backtest = pd.read_csv(fb, parse_dates=[\"timestamp\"])\n",
    "    else:\n",
    "        raise RuntimeError(\"df_backtest not found in workspace. Run Block 7.5 first.\")\n",
    "\n",
    "# Ensure timestamp dtype\n",
    "df_backtest[\"timestamp\"] = pd.to_datetime(df_backtest[\"timestamp\"])\n",
    "\n",
    "# --- Model ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]   # use last timestep hidden\n",
    "        return self.actor(h), self.critic(h).squeeze(-1)\n",
    "\n",
    "# --- Loss (A3C-style with entropy) ---\n",
    "def a3c_loss(logits, values, actions, rewards, beta=BETA_ENTROPY):\n",
    "    # logits: (B, n_actions), values: (B,), actions: (B,), rewards: (B,)\n",
    "    adv = rewards - values\n",
    "    critic = adv.pow(2).mean()\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    selected_logp = logp.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    actor_loss = -(selected_logp * adv.detach()).mean()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    entropy = -(probs * logp).sum(-1).mean()\n",
    "    return actor_loss + 0.5 * critic - beta * entropy\n",
    "\n",
    "# --- Utility: build maps for rewards and regime vectors ---\n",
    "def build_lookup_maps(tickers, df_backtest_local):\n",
    "    # rewards_map[(ticker, timestamp)] -> float realized_return\n",
    "    # regime_map[(ticker, timestamp)] -> np.array([rb, rbu, rs]) (order matches one-hot columns if present)\n",
    "    rewards_map = {}\n",
    "    regime_map = {}\n",
    "    has_regime_cols = all(c in df_backtest_local.columns for c in [\"regime_bear\",\"regime_bull\",\"regime_sideway\"])\n",
    "    sub = df_backtest_local[df_backtest_local[\"ticker\"].isin(tickers)].sort_values([\"ticker\",\"timestamp\"])\n",
    "    for _, row in sub.iterrows():\n",
    "        tk = row[\"ticker\"]\n",
    "        ts = pd.to_datetime(row[\"timestamp\"])\n",
    "        r = row.get(\"realized_return\", np.nan)\n",
    "        if pd.notna(r):\n",
    "            rewards_map[(tk, ts)] = float(r)\n",
    "        else:\n",
    "            # leave absent -> later impute zero\n",
    "            pass\n",
    "        if has_regime_cols:\n",
    "            try:\n",
    "                rv = np.array([\n",
    "                    float(row.get(\"regime_bear\", 0.0)),\n",
    "                    float(row.get(\"regime_bull\", 0.0)),\n",
    "                    float(row.get(\"regime_sideway\", 0.0))\n",
    "                ], dtype=np.float32)\n",
    "            except Exception:\n",
    "                rv = np.array([0.0,0.0,0.0], dtype=np.float32)\n",
    "            regime_map[(tk, ts)] = rv\n",
    "    return rewards_map, regime_map\n",
    "\n",
    "# --- Training per cluster ---\n",
    "def process_cluster(meta, epochs=EPOCHS_DEFAULT, lr=LR_DEFAULT, batch_size=BATCH_SIZE_DEFAULT, reward_clip=REWARD_CLIP):\n",
    "    c_id = int(meta[\"cluster\"])\n",
    "    tickers = meta[\"tickers\"]\n",
    "    dates = [pd.to_datetime(d) for d in meta[\"dates\"]]            # dates are strings in metadata\n",
    "    dates_shifted = [pd.to_datetime(d) for d in meta.get(\"dates_shifted\", meta[\"dates\"])]\n",
    "    tensor_file = os.path.join(DATA_DIR, meta[\"tensor_file\"])\n",
    "    mask_file   = os.path.join(DATA_DIR, meta[\"mask_file\"])\n",
    "\n",
    "    if not os.path.exists(tensor_file) or not os.path.exists(mask_file):\n",
    "        print(f\"Cluster {c_id}: tensor/mask files missing, skip.\")\n",
    "        return\n",
    "\n",
    "    X = np.load(tensor_file, mmap_mode=\"r\")\n",
    "    M = np.load(mask_file, mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    # X shape: (B, T, N, F) from earlier design -> ensure that shape\n",
    "    # note: previous code used X shape (B, T, N, F) — but often saved as (B, T, N, F)\n",
    "    # our code expects X as (B, T, N, F)\n",
    "    if X.ndim != 4:\n",
    "        raise ValueError(f\"Unexpected tensor shape for {tensor_file}: {X.shape}\")\n",
    "\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"[Cluster {c_id}] loaded tensor {tensor_file} -> (B,T,N,F)=({B},{T},{N},{F})\")\n",
    "\n",
    "    # clean\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    M = M.astype(np.float32)\n",
    "    # build reward/regime lookups\n",
    "    rewards_map, regime_map = build_lookup_maps(tickers, df_backtest)\n",
    "\n",
    "    # default regime vector if not found\n",
    "    default_regime = np.array([0.0,0.0,0.0], dtype=np.float32)\n",
    "\n",
    "    # We'll augment features by regime one-hot (3 dims), repeated along time axis\n",
    "    extra_dim = 3\n",
    "    F_aug = F + extra_dim\n",
    "\n",
    "    # --- model & optimizer ---\n",
    "    model = A3CNet(n_features=F_aug, hidden=64).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # generator that yields mini-batches (flatten B*N -> total samples)\n",
    "    total = B * N\n",
    "    def iterator():\n",
    "        for start in range(0, total, batch_size):\n",
    "            end = min(total, start + batch_size)\n",
    "            xb_list, mb_list, rb_list, idx_list = [], [], [], []\n",
    "            for s in range(start, end):\n",
    "                b, n = divmod(s, N)\n",
    "                entry_date = dates[b]   # datetime\n",
    "                tk = tickers[n]\n",
    "                reward = rewards_map.get((tk, entry_date), 0.0)    # impute missing as 0.0\n",
    "                # load time-series features for this sample\n",
    "                x_sample = X[b, :, n, :]    # (T, F)\n",
    "                m_sample = M[b, :, n, :]    # (T, F)\n",
    "                # regime vector for (tk, entry_date)\n",
    "                rvec = regime_map.get((tk, entry_date), default_regime)\n",
    "                # expand regime across time axis -> (T, 3)\n",
    "                rmat = np.repeat(rvec.reshape(1, -1), x_sample.shape[0], axis=0).astype(np.float32)\n",
    "                # concat along feature axis\n",
    "                x_aug = np.concatenate([x_sample, rmat], axis=1)   # (T, F_aug)\n",
    "                m_aug = np.concatenate([m_sample, np.ones_like(rmat)], axis=1)  # regime assumed present -> mask 1\n",
    "                xb_list.append(x_aug)\n",
    "                mb_list.append(m_aug)\n",
    "                # clip reward to avoid exploding targets\n",
    "                rb_list.append(float(np.clip(reward, -reward_clip, reward_clip)))\n",
    "                idx_list.append((b, n))\n",
    "            yield np.stack(xb_list), np.stack(mb_list), np.array(rb_list, dtype=np.float32), idx_list\n",
    "\n",
    "    # --- Training loop ---\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        loss_ep = 0.0\n",
    "        n_batches = 0\n",
    "        for xb_batch, mb_batch, rb_batch, _ in iterator():\n",
    "            n_batches += 1\n",
    "            # convert to torch\n",
    "            xb = torch.tensor(xb_batch, dtype=torch.float32, device=device)    # (Bbatch, T, F_aug)\n",
    "            mb = torch.tensor(mb_batch, dtype=torch.float32, device=device)\n",
    "            rb = torch.tensor(rb_batch, dtype=torch.float32, device=device)\n",
    "\n",
    "            # apply mask to inputs (zero out padded / missing feature slots)\n",
    "            xb = xb * mb\n",
    "\n",
    "            logits, vals = model(xb)   # logits (B,3), vals (B,)\n",
    "            # sanitize\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "            vals   = torch.nan_to_num(vals, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "            # sample actions via distribution for on-policy update\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            acts = dist.sample()             # (B,)\n",
    "            # map action to signed reward: long=2 -> +r, flat=1 -> 0, short=0 -> -r\n",
    "            reward_tensor = torch.where(acts == 2, rb,\n",
    "                                       torch.where(acts == 0, -rb, torch.zeros_like(rb)))\n",
    "\n",
    "            loss = a3c_loss(logits, vals, acts, reward_tensor, beta=BETA_ENTROPY)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)\n",
    "            opt.step()\n",
    "\n",
    "            loss_ep += float(loss.item())\n",
    "\n",
    "            # free memory\n",
    "            del xb, mb, rb, logits, vals, dist, acts, reward_tensor, loss\n",
    "            gc.collect()\n",
    "        avg_loss = loss_ep / max(1, n_batches)\n",
    "        print(f\"[A3C] Cluster {c_id} | Epoch {ep+1}/{epochs} | AvgLoss {avg_loss:.6f} | batches {n_batches}\")\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Save model checkpoint ---\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"  ✅ Saved A3C checkpoint: {model_path}\")\n",
    "\n",
    "    # cleanup\n",
    "    del X, M, model, opt\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run training for all clusters ---\n",
    "for meta in tensor_index:\n",
    "    try:\n",
    "        process_cluster(meta, epochs=EPOCHS_DEFAULT, lr=LR_DEFAULT, batch_size=BATCH_SIZE_DEFAULT, reward_clip=REWARD_CLIP)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing cluster {meta.get('cluster')}: {e}\")\n",
    "        gc.collect()\n",
    "\n",
    "print(\"✅ Done Block 8: A3C models trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af831fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f3cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9584102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc2555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b46136b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-0.6032\n",
      "  Epoch 2/3, Loss=-0.4641\n",
      "  Epoch 3/3, Loss=-0.5235\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_0.pt\n",
      "Cluster 1 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.8764\n",
      "  Epoch 2/3, Loss=-0.5717\n",
      "  Epoch 3/3, Loss=-0.6274\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_1.pt\n",
      "Cluster 2 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.6268\n",
      "  Epoch 2/3, Loss=-0.5301\n",
      "  Epoch 3/3, Loss=-0.5815\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_2.pt\n",
      "Cluster 3 | X=(509, 64, 33, 25)\n",
      "  Epoch 1/3, Loss=-0.6216\n",
      "  Epoch 2/3, Loss=-0.7012\n",
      "  Epoch 3/3, Loss=-0.7094\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_3.pt\n",
      "Cluster 4 | X=(509, 64, 31, 25)\n",
      "  Epoch 1/3, Loss=-0.7542\n",
      "  Epoch 2/3, Loss=-0.6479\n",
      "  Epoch 3/3, Loss=-0.6742\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_4.pt\n",
      "Cluster 5 | X=(509, 64, 30, 25)\n",
      "  Epoch 1/3, Loss=-0.3925\n",
      "  Epoch 2/3, Loss=-0.5933\n",
      "  Epoch 3/3, Loss=-0.6354\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_5.pt\n",
      "Cluster 6 | X=(509, 64, 30, 25)\n",
      "  Epoch 1/3, Loss=-0.6913\n",
      "  Epoch 2/3, Loss=-0.6499\n",
      "  Epoch 3/3, Loss=-0.6653\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_6.pt\n",
      "Cluster 7 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.4682\n",
      "  Epoch 2/3, Loss=-0.5392\n",
      "  Epoch 3/3, Loss=-0.5300\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_7.pt\n",
      "Cluster 8 | X=(509, 64, 27, 25)\n",
      "  Epoch 1/3, Loss=-0.6507\n",
      "  Epoch 2/3, Loss=-0.5877\n",
      "  Epoch 3/3, Loss=-0.5897\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_8.pt\n",
      "Cluster 9 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.5318\n",
      "  Epoch 2/3, Loss=-0.6167\n",
      "  Epoch 3/3, Loss=-0.6315\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_9.pt\n",
      "Cluster 10 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.7431\n",
      "  Epoch 2/3, Loss=-0.5732\n",
      "  Epoch 3/3, Loss=-0.6353\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_10.pt\n",
      "Cluster 11 | X=(509, 64, 30, 25)\n",
      "  Epoch 1/3, Loss=-1.0720\n",
      "  Epoch 2/3, Loss=-0.7050\n",
      "  Epoch 3/3, Loss=-0.6515\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_11.pt\n",
      "Cluster 12 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.4986\n",
      "  Epoch 2/3, Loss=-0.5677\n",
      "  Epoch 3/3, Loss=-0.5527\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_12.pt\n",
      "Cluster 13 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=0.5614\n",
      "  Epoch 2/3, Loss=-0.5517\n",
      "  Epoch 3/3, Loss=-0.5827\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_13.pt\n",
      "Cluster 14 | X=(509, 64, 31, 25)\n",
      "  Epoch 1/3, Loss=-0.6112\n",
      "  Epoch 2/3, Loss=-0.6683\n",
      "  Epoch 3/3, Loss=-0.6640\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_14.pt\n",
      "Cluster 15 | X=(509, 64, 30, 25)\n",
      "  Epoch 1/3, Loss=-0.1156\n",
      "  Epoch 2/3, Loss=-0.6013\n",
      "  Epoch 3/3, Loss=-0.5628\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_15.pt\n",
      "Cluster 16 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.2702\n",
      "  Epoch 2/3, Loss=-0.5775\n",
      "  Epoch 3/3, Loss=-0.5910\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_16.pt\n",
      "Cluster 17 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.7195\n",
      "  Epoch 2/3, Loss=-0.5979\n",
      "  Epoch 3/3, Loss=-0.6197\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_17.pt\n",
      "Cluster 18 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.5905\n",
      "  Epoch 2/3, Loss=-0.6189\n",
      "  Epoch 3/3, Loss=-0.6204\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_18.pt\n",
      "Cluster 19 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.3041\n",
      "  Epoch 2/3, Loss=-0.5872\n",
      "  Epoch 3/3, Loss=-0.6152\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_19.pt\n",
      "Cluster 20 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.6788\n",
      "  Epoch 2/3, Loss=-0.5332\n",
      "  Epoch 3/3, Loss=-0.5595\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_20.pt\n",
      "Cluster 21 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.4344\n",
      "  Epoch 2/3, Loss=-0.6072\n",
      "  Epoch 3/3, Loss=-0.6023\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_21.pt\n",
      "Cluster 22 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.7202\n",
      "  Epoch 2/3, Loss=-0.5718\n",
      "  Epoch 3/3, Loss=-0.6249\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_22.pt\n",
      "Cluster 23 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=1.0644\n",
      "  Epoch 2/3, Loss=-0.4737\n",
      "  Epoch 3/3, Loss=-0.5587\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_23.pt\n",
      "Cluster 24 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.4638\n",
      "  Epoch 2/3, Loss=-0.5782\n",
      "  Epoch 3/3, Loss=-0.6117\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_24.pt\n",
      "Cluster 25 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.8003\n",
      "  Epoch 2/3, Loss=-0.5385\n",
      "  Epoch 3/3, Loss=-0.5649\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_25.pt\n",
      "Cluster 26 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.1729\n",
      "  Epoch 2/3, Loss=-0.5547\n",
      "  Epoch 3/3, Loss=-0.5178\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_26.pt\n",
      "Cluster 27 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.5385\n",
      "  Epoch 2/3, Loss=-0.5430\n",
      "  Epoch 3/3, Loss=-0.5612\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_27.pt\n",
      "Cluster 28 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.1608\n",
      "  Epoch 2/3, Loss=-0.5941\n",
      "  Epoch 3/3, Loss=-0.5946\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_28.pt\n",
      "Cluster 29 | X=(509, 64, 25, 25)\n",
      "  Epoch 1/3, Loss=-0.4082\n",
      "  Epoch 2/3, Loss=-0.4516\n",
      "  Epoch 3/3, Loss=-0.5395\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_29.pt\n",
      "Cluster 30 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.5094\n",
      "  Epoch 2/3, Loss=-0.6093\n",
      "  Epoch 3/3, Loss=-0.6061\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_30.pt\n",
      "Cluster 31 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.5171\n",
      "  Epoch 2/3, Loss=-0.5546\n",
      "  Epoch 3/3, Loss=-0.5662\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_31.pt\n",
      "Cluster 32 | X=(509, 64, 29, 25)\n",
      "  Epoch 1/3, Loss=-0.1337\n",
      "  Epoch 2/3, Loss=-0.6043\n",
      "  Epoch 3/3, Loss=-0.5710\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_32.pt\n",
      "Cluster 33 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.0383\n",
      "  Epoch 2/3, Loss=-0.5238\n",
      "  Epoch 3/3, Loss=-0.5230\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_33.pt\n",
      "Cluster 34 | X=(509, 64, 25, 25)\n",
      "  Epoch 1/3, Loss=0.9027\n",
      "  Epoch 2/3, Loss=-0.5969\n",
      "  Epoch 3/3, Loss=-0.0563\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_34.pt\n",
      "Cluster 35 | X=(509, 64, 24, 25)\n",
      "  Epoch 1/3, Loss=-0.7292\n",
      "  Epoch 2/3, Loss=-0.5501\n",
      "  Epoch 3/3, Loss=-0.5047\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_35.pt\n",
      "Cluster 36 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.0482\n",
      "  Epoch 2/3, Loss=-0.5788\n",
      "  Epoch 3/3, Loss=-0.5774\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_36.pt\n",
      "Cluster 37 | X=(509, 64, 25, 25)\n",
      "  Epoch 1/3, Loss=-0.5530\n",
      "  Epoch 2/3, Loss=-0.2456\n",
      "  Epoch 3/3, Loss=-0.6598\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_37.pt\n",
      "Cluster 38 | X=(509, 64, 27, 25)\n",
      "  Epoch 1/3, Loss=-0.5780\n",
      "  Epoch 2/3, Loss=-0.6047\n",
      "  Epoch 3/3, Loss=-0.5585\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_38.pt\n",
      "Cluster 39 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.1745\n",
      "  Epoch 2/3, Loss=-0.5208\n",
      "  Epoch 3/3, Loss=-0.5839\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_39.pt\n",
      "Cluster 40 | X=(509, 64, 24, 25)\n",
      "  Epoch 1/3, Loss=-0.3386\n",
      "  Epoch 2/3, Loss=-0.5637\n",
      "  Epoch 3/3, Loss=-0.4781\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_40.pt\n",
      "Cluster 41 | X=(509, 64, 28, 25)\n",
      "  Epoch 1/3, Loss=-0.1211\n",
      "  Epoch 2/3, Loss=-0.5700\n",
      "  Epoch 3/3, Loss=-0.5657\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_41.pt\n",
      "Cluster 42 | X=(509, 64, 27, 25)\n",
      "  Epoch 1/3, Loss=-0.4010\n",
      "  Epoch 2/3, Loss=-0.6465\n",
      "  Epoch 3/3, Loss=-0.5318\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_42.pt\n",
      "Cluster 43 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-0.3501\n",
      "  Epoch 2/3, Loss=-0.4967\n",
      "  Epoch 3/3, Loss=-0.4741\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_43.pt\n",
      "Cluster 44 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-1.0799\n",
      "  Epoch 2/3, Loss=-0.5477\n",
      "  Epoch 3/3, Loss=-0.5427\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_44.pt\n",
      "Cluster 45 | X=(509, 64, 22, 25)\n",
      "  Epoch 1/3, Loss=-0.2442\n",
      "  Epoch 2/3, Loss=-0.4652\n",
      "  Epoch 3/3, Loss=-0.4463\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_45.pt\n",
      "Cluster 46 | X=(509, 64, 25, 25)\n",
      "  Epoch 1/3, Loss=-0.7529\n",
      "  Epoch 2/3, Loss=-0.2412\n",
      "  Epoch 3/3, Loss=-0.6719\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_46.pt\n",
      "Cluster 47 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-0.5859\n",
      "  Epoch 2/3, Loss=-0.5234\n",
      "  Epoch 3/3, Loss=-0.5033\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_47.pt\n",
      "Cluster 48 | X=(509, 64, 24, 25)\n",
      "  Epoch 1/3, Loss=-0.3931\n",
      "  Epoch 2/3, Loss=-0.5296\n",
      "  Epoch 3/3, Loss=-0.4910\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_48.pt\n",
      "Cluster 49 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-0.4875\n",
      "  Epoch 2/3, Loss=-0.4720\n",
      "  Epoch 3/3, Loss=-0.4923\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_49.pt\n",
      "Cluster 50 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.7314\n",
      "  Epoch 2/3, Loss=-0.5811\n",
      "  Epoch 3/3, Loss=-0.5845\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_50.pt\n",
      "Cluster 51 | X=(509, 64, 26, 25)\n",
      "  Epoch 1/3, Loss=-0.6858\n",
      "  Epoch 2/3, Loss=-0.4996\n",
      "  Epoch 3/3, Loss=-0.5964\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_51.pt\n",
      "Cluster 52 | X=(509, 64, 22, 25)\n",
      "  Epoch 1/3, Loss=-0.4684\n",
      "  Epoch 2/3, Loss=-0.4740\n",
      "  Epoch 3/3, Loss=-0.4799\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_52.pt\n",
      "Cluster 53 | X=(509, 64, 25, 25)\n",
      "  Epoch 1/3, Loss=-0.7195\n",
      "  Epoch 2/3, Loss=-0.2827\n",
      "  Epoch 3/3, Loss=-0.5845\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_53.pt\n",
      "Cluster 54 | X=(509, 64, 18, 25)\n",
      "  Epoch 1/3, Loss=-1.0760\n",
      "  Epoch 2/3, Loss=-0.4010\n",
      "  Epoch 3/3, Loss=-0.3958\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_54.pt\n",
      "Cluster 55 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-0.5017\n",
      "  Epoch 2/3, Loss=-0.4701\n",
      "  Epoch 3/3, Loss=-0.4848\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_55.pt\n",
      "Cluster 56 | X=(509, 64, 24, 25)\n",
      "  Epoch 1/3, Loss=-0.4273\n",
      "  Epoch 2/3, Loss=-0.5503\n",
      "  Epoch 3/3, Loss=-0.4923\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_56.pt\n",
      "Cluster 57 | X=(509, 64, 24, 25)\n",
      "  Epoch 1/3, Loss=-0.0371\n",
      "  Epoch 2/3, Loss=-0.5469\n",
      "  Epoch 3/3, Loss=-0.5061\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_57.pt\n",
      "Cluster 58 | X=(509, 64, 22, 25)\n",
      "  Epoch 1/3, Loss=-0.2671\n",
      "  Epoch 2/3, Loss=-0.4302\n",
      "  Epoch 3/3, Loss=-0.4497\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_58.pt\n",
      "Cluster 59 | X=(509, 64, 21, 25)\n",
      "  Epoch 1/3, Loss=-0.5598\n",
      "  Epoch 2/3, Loss=-0.4420\n",
      "  Epoch 3/3, Loss=-0.4980\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_59.pt\n",
      "Cluster 60 | X=(509, 64, 20, 25)\n",
      "  Epoch 1/3, Loss=0.1981\n",
      "  Epoch 2/3, Loss=-0.3087\n",
      "  Epoch 3/3, Loss=-0.3827\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_60.pt\n",
      "Cluster 61 | X=(509, 64, 25, 25)\n",
      "  Epoch 1/3, Loss=-0.5436\n",
      "  Epoch 2/3, Loss=-0.1764\n",
      "  Epoch 3/3, Loss=-0.6321\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_61.pt\n",
      "Cluster 62 | X=(509, 64, 22, 25)\n",
      "  Epoch 1/3, Loss=-0.9348\n",
      "  Epoch 2/3, Loss=-0.5535\n",
      "  Epoch 3/3, Loss=-0.4996\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_62.pt\n",
      "Cluster 63 | X=(509, 64, 23, 25)\n",
      "  Epoch 1/3, Loss=-0.1945\n",
      "  Epoch 2/3, Loss=-0.4631\n",
      "  Epoch 3/3, Loss=-0.4725\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_63.pt\n",
      "Cluster 64 | X=(509, 64, 24, 25)\n",
      "  Epoch 1/3, Loss=-0.4981\n",
      "  Epoch 2/3, Loss=-0.5199\n",
      "  Epoch 3/3, Loss=-0.5199\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_64.pt\n",
      "Cluster 65 | X=(509, 64, 21, 25)\n",
      "  Epoch 1/3, Loss=-0.4052\n",
      "  Epoch 2/3, Loss=-0.4610\n",
      "  Epoch 3/3, Loss=-0.4401\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_65.pt\n",
      "Cluster 66 | X=(509, 64, 21, 25)\n",
      "  Epoch 1/3, Loss=-0.5189\n",
      "  Epoch 2/3, Loss=-0.4354\n",
      "  Epoch 3/3, Loss=-0.4777\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_66.pt\n",
      "Cluster 67 | X=(509, 64, 17, 25)\n",
      "  Epoch 1/3, Loss=-0.4539\n",
      "  Epoch 2/3, Loss=-0.3226\n",
      "  Epoch 3/3, Loss=-0.3599\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_67.pt\n",
      "Cluster 68 | X=(509, 64, 19, 25)\n",
      "  Epoch 1/3, Loss=-0.3069\n",
      "  Epoch 2/3, Loss=-0.4289\n",
      "  Epoch 3/3, Loss=-0.3772\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_68.pt\n",
      "Cluster 69 | X=(509, 64, 16, 25)\n",
      "  Epoch 1/3, Loss=-0.7912\n",
      "  Epoch 2/3, Loss=-0.2601\n",
      "  Epoch 3/3, Loss=-0.3730\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_69.pt\n",
      "Cluster 70 | X=(509, 64, 16, 25)\n",
      "  Epoch 1/3, Loss=-0.3724\n",
      "  Epoch 2/3, Loss=-0.4346\n",
      "  Epoch 3/3, Loss=-0.3108\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_70.pt\n",
      "Cluster 71 | X=(509, 64, 15, 25)\n",
      "  Epoch 1/3, Loss=-0.3270\n",
      "  Epoch 2/3, Loss=-0.2420\n",
      "  Epoch 3/3, Loss=-0.3145\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_71.pt\n",
      "Cluster 72 | X=(509, 64, 14, 25)\n",
      "  Epoch 1/3, Loss=-0.3954\n",
      "  Epoch 2/3, Loss=-0.3389\n",
      "  Epoch 3/3, Loss=-0.2739\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_72.pt\n",
      "Cluster 73 | X=(509, 64, 13, 25)\n",
      "  Epoch 1/3, Loss=-0.4010\n",
      "  Epoch 2/3, Loss=-0.2261\n",
      "  Epoch 3/3, Loss=-0.2679\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_73.pt\n",
      "Cluster 74 | X=(509, 64, 12, 25)\n",
      "  Epoch 1/3, Loss=-0.2281\n",
      "  Epoch 2/3, Loss=-0.1553\n",
      "  Epoch 3/3, Loss=-0.3146\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_74.pt\n",
      "Cluster 75 | X=(509, 64, 15, 25)\n",
      "  Epoch 1/3, Loss=-0.0003\n",
      "  Epoch 2/3, Loss=-0.3012\n",
      "  Epoch 3/3, Loss=-0.2160\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_75.pt\n",
      "Cluster 76 | X=(509, 64, 11, 25)\n",
      "  Epoch 1/3, Loss=-0.1244\n",
      "  Epoch 2/3, Loss=-0.2676\n",
      "  Epoch 3/3, Loss=-0.2051\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_76.pt\n",
      "Cluster 77 | X=(509, 64, 10, 25)\n",
      "  Epoch 1/3, Loss=0.7776\n",
      "  Epoch 2/3, Loss=-0.3911\n",
      "  Epoch 3/3, Loss=-0.1005\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_77.pt\n",
      "Cluster 78 | X=(509, 64, 10, 25)\n",
      "  Epoch 1/3, Loss=-0.1629\n",
      "  Epoch 2/3, Loss=-0.2471\n",
      "  Epoch 3/3, Loss=-0.1926\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_78.pt\n",
      "Cluster 79 | X=(509, 64, 10, 25)\n",
      "  Epoch 1/3, Loss=-0.1817\n",
      "  Epoch 2/3, Loss=-0.1717\n",
      "  Epoch 3/3, Loss=-0.2848\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_79.pt\n",
      "Cluster 80 | X=(509, 64, 12, 25)\n",
      "  Epoch 1/3, Loss=-0.4530\n",
      "  Epoch 2/3, Loss=-0.3046\n",
      "  Epoch 3/3, Loss=-0.2527\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_80.pt\n",
      "Cluster 81 | X=(509, 64, 11, 25)\n",
      "  Epoch 1/3, Loss=-0.2361\n",
      "  Epoch 2/3, Loss=-0.2427\n",
      "  Epoch 3/3, Loss=-0.1956\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_81.pt\n",
      "Cluster 82 | X=(509, 64, 8, 25)\n",
      "  Epoch 1/3, Loss=0.0045\n",
      "  Epoch 2/3, Loss=-0.2475\n",
      "  Epoch 3/3, Loss=-0.1380\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_82.pt\n",
      "Cluster 83 | X=(509, 64, 5, 25)\n",
      "  Epoch 1/3, Loss=0.0986\n",
      "  Epoch 2/3, Loss=0.0642\n",
      "  Epoch 3/3, Loss=-0.1856\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_83.pt\n",
      "Cluster 84 | X=(509, 64, 6, 25)\n",
      "  Epoch 1/3, Loss=-0.2232\n",
      "  Epoch 2/3, Loss=-0.1705\n",
      "  Epoch 3/3, Loss=-0.0560\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_84.pt\n",
      "Cluster 85 | X=(509, 64, 6, 25)\n",
      "  Epoch 1/3, Loss=0.0194\n",
      "  Epoch 2/3, Loss=-0.1533\n",
      "  Epoch 3/3, Loss=-0.1141\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_85.pt\n",
      "Cluster 86 | X=(509, 64, 5, 25)\n",
      "  Epoch 1/3, Loss=-0.5331\n",
      "  Epoch 2/3, Loss=-0.0033\n",
      "  Epoch 3/3, Loss=-0.0622\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_86.pt\n",
      "Cluster 87 | X=(509, 64, 5, 25)\n",
      "  Epoch 1/3, Loss=0.3022\n",
      "  Epoch 2/3, Loss=-0.1007\n",
      "  Epoch 3/3, Loss=-0.1165\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_87.pt\n",
      "Cluster 88 | X=(509, 64, 5, 25)\n",
      "  Epoch 1/3, Loss=-0.5056\n",
      "  Epoch 2/3, Loss=0.0457\n",
      "  Epoch 3/3, Loss=-0.1968\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_88.pt\n",
      "Cluster 89 | X=(509, 64, 7, 25)\n",
      "  Epoch 1/3, Loss=-0.2214\n",
      "  Epoch 2/3, Loss=-0.0685\n",
      "  Epoch 3/3, Loss=-0.1719\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_89.pt\n",
      "Cluster 90 | X=(509, 64, 6, 25)\n",
      "  Epoch 1/3, Loss=0.2864\n",
      "  Epoch 2/3, Loss=-0.0357\n",
      "  Epoch 3/3, Loss=-0.0136\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_90.pt\n",
      "Cluster 91 | X=(509, 64, 6, 25)\n",
      "  Epoch 1/3, Loss=0.0530\n",
      "  Epoch 2/3, Loss=-0.0265\n",
      "  Epoch 3/3, Loss=-0.1269\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_91.pt\n",
      "Cluster 92 | X=(509, 64, 4, 25)\n",
      "  Epoch 1/3, Loss=1.2730\n",
      "  Epoch 2/3, Loss=0.0931\n",
      "  Epoch 3/3, Loss=-0.2121\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_92.pt\n",
      "Cluster 93 | X=(509, 64, 6, 25)\n",
      "  Epoch 1/3, Loss=-0.2321\n",
      "  Epoch 2/3, Loss=-0.0346\n",
      "  Epoch 3/3, Loss=-0.1073\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_93.pt\n",
      "Cluster 94 | X=(509, 64, 6, 25)\n",
      "  Epoch 1/3, Loss=-0.1688\n",
      "  Epoch 2/3, Loss=-0.1966\n",
      "  Epoch 3/3, Loss=0.0190\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_94.pt\n",
      "Cluster 95 | X=(509, 64, 3, 25)\n",
      "  Epoch 1/3, Loss=0.2938\n",
      "  Epoch 2/3, Loss=-0.1990\n",
      "  Epoch 3/3, Loss=-0.0884\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_95.pt\n",
      "Cluster 96 | X=(509, 64, 3, 25)\n",
      "  Epoch 1/3, Loss=-0.4051\n",
      "  Epoch 2/3, Loss=-0.0270\n",
      "  Epoch 3/3, Loss=0.0151\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_96.pt\n",
      "Cluster 97 | X=(509, 64, 2, 25)\n",
      "  Epoch 1/3, Loss=-0.2040\n",
      "  Epoch 2/3, Loss=-0.0772\n",
      "  Epoch 3/3, Loss=-0.0002\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_97.pt\n",
      "Cluster 98 | X=(509, 64, 2, 25)\n",
      "  Epoch 1/3, Loss=-0.2713\n",
      "  Epoch 2/3, Loss=-0.0175\n",
      "  Epoch 3/3, Loss=-0.0340\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_98.pt\n",
      "Cluster 99 | X=(509, 64, 2, 25)\n",
      "  Epoch 1/3, Loss=-0.0825\n",
      "  Epoch 2/3, Loss=0.0174\n",
      "  Epoch 3/3, Loss=-0.0591\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_99.pt\n",
      "Cluster 100 | X=(509, 64, 3, 25)\n",
      "  Epoch 1/3, Loss=0.2404\n",
      "  Epoch 2/3, Loss=-0.0547\n",
      "  Epoch 3/3, Loss=-0.0112\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_100.pt\n",
      "Cluster 101 | X=(509, 64, 3, 25)\n",
      "  Epoch 1/3, Loss=0.0237\n",
      "  Epoch 2/3, Loss=-0.1389\n",
      "  Epoch 3/3, Loss=-0.0578\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_101.pt\n",
      "Cluster 102 | X=(509, 64, 2, 25)\n",
      "  Epoch 1/3, Loss=0.4684\n",
      "  Epoch 2/3, Loss=-0.0353\n",
      "  Epoch 3/3, Loss=-0.1918\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_102.pt\n",
      "Cluster 103 | X=(509, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=0.0465\n",
      "  Epoch 2/3, Loss=-0.0462\n",
      "  Epoch 3/3, Loss=-0.0698\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_103.pt\n",
      "Cluster 104 | X=(509, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=-0.2855\n",
      "  Epoch 2/3, Loss=-0.1882\n",
      "  Epoch 3/3, Loss=-0.0944\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_104.pt\n",
      "Cluster 105 | X=(509, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=-0.1504\n",
      "  Epoch 2/3, Loss=-0.0197\n",
      "  Epoch 3/3, Loss=0.0621\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_105.pt\n",
      "Cluster 106 | X=(509, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=-0.0062\n",
      "  Epoch 2/3, Loss=0.0039\n",
      "  Epoch 3/3, Loss=-0.0139\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_106.pt\n",
      "Cluster 107 | X=(509, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=0.3380\n",
      "  Epoch 2/3, Loss=0.1572\n",
      "  Epoch 3/3, Loss=0.0140\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_107.pt\n",
      "Cluster 108 | X=(309, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=0.0641\n",
      "  Epoch 2/3, Loss=-0.0257\n",
      "  Epoch 3/3, Loss=-0.0177\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_108.pt\n",
      "Cluster 109 | X=(509, 64, 1, 25)\n",
      "  Epoch 1/3, Loss=-0.2431\n",
      "  Epoch 2/3, Loss=-0.0957\n",
      "  Epoch 3/3, Loss=0.0122\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_109.pt\n",
      "Cluster 110 | X=(509, 64, 2, 25)\n",
      "  Epoch 1/3, Loss=0.0289\n",
      "  Epoch 2/3, Loss=-0.0159\n",
      "  Epoch 3/3, Loss=-0.0769\n",
      "  ✅ Saved model checkpoint: ./models/a3c_cluster_110.pt\n",
      "✅ Done Block 8: models saved in ./models/\n"
     ]
    }
   ],
   "source": [
    "# Block 8 — Train A3C multi-stock per-cluster (Final Stable)\n",
    "\n",
    "import os, gc, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "DATA_DIR  = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tensor metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model định nghĩa ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Loss ---\n",
    "def a3c_loss(logits, values, actions, rewards, beta=0.01):\n",
    "    adv = rewards - values.squeeze(-1)\n",
    "    critic = adv.pow(2).mean()\n",
    "    logp = torch.log_softmax(logits, dim=-1)\n",
    "    actor = -(logp.gather(1, actions.unsqueeze(1)).squeeze(1) * adv.detach()).mean()\n",
    "    entropy = -(torch.softmax(logits, dim=-1) * logp).sum(-1).mean()\n",
    "    return actor + 0.5*critic - beta*entropy\n",
    "\n",
    "# --- Training per cluster ---\n",
    "def process_cluster(meta, epochs=3, lr=1e-3, batch_size=256):\n",
    "    c_id, tickers, dates, dates_shifted = (\n",
    "        meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"], meta[\"dates_shifted\"]\n",
    "    )\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # --- Clean tensors ---\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    M = M.astype(np.float32)\n",
    "    assert not np.isnan(X).any(), \"NaN still in X\"\n",
    "    assert not np.isinf(X).any(), \"Inf still in X\"\n",
    "\n",
    "    # --- Reward lookup từ df_backtest ---\n",
    "    rewards_map = {}\n",
    "    for tk in tickers:\n",
    "        sub = df_backtest[df_backtest[\"ticker\"] == tk].sort_values(\"timestamp\")\n",
    "        for _, row in sub.iterrows():\n",
    "            entry = row[\"timestamp\"]\n",
    "            r = row[\"realized_return\"]\n",
    "            if pd.notna(r):\n",
    "                rewards_map[(tk, entry)] = float(r)\n",
    "\n",
    "    # --- Model + optimizer ---\n",
    "    model = A3CNet(F).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Mini-batch generator\n",
    "    total = B * N\n",
    "    def iterator():\n",
    "        for start in range(0, total, batch_size):\n",
    "            end = min(total, start+batch_size)\n",
    "            xb, mb, rb, idx = [], [], [], []\n",
    "            for s in range(start, end):\n",
    "                b, n = divmod(s, N)\n",
    "                entry_date = pd.to_datetime(dates[b])\n",
    "                tk = tickers[n]\n",
    "                reward = rewards_map.get((tk, entry_date), 0.0)  # impute NaN->0\n",
    "                xb.append(X[b, :, n, :])\n",
    "                mb.append(M[b, :, n, :])\n",
    "                rb.append(reward)\n",
    "                idx.append((b, n))\n",
    "            yield np.stack(xb), np.stack(mb), np.array(rb, dtype=np.float32), idx\n",
    "\n",
    "    # --- Train ---\n",
    "    for ep in range(epochs):\n",
    "        loss_ep = 0\n",
    "        for xb, mb, rb, _ in iterator():\n",
    "            xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "            mb = torch.tensor(mb, dtype=torch.float32).to(device)\n",
    "            rb = torch.tensor(rb, dtype=torch.float32).to(device)\n",
    "\n",
    "            xb = xb * mb  # apply mask\n",
    "\n",
    "            logits, vals = model(xb)\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            act = dist.sample()\n",
    "            # reward theo action: long=2, short=0, flat=1\n",
    "            reward = torch.where(act==2, rb,\n",
    "                        torch.where(act==0, -rb, torch.zeros_like(rb)))\n",
    "            loss = a3c_loss(logits, vals, act, reward)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "            opt.step()\n",
    "            loss_ep += loss.item()\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Loss={loss_ep:.4f}\")\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Save model ---\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"  ✅ Saved model checkpoint: {model_path}\")\n",
    "\n",
    "    del X, M, model, opt\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run all clusters ---\n",
    "for meta in tensor_index:\n",
    "    process_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 8: models saved in {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad567b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7418f9a1",
   "metadata": {},
   "source": [
    "**Block 9: Suy luận từ mô hình A3C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5e2629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inference] Cluster 0 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 1 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 2 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 3 | X=(509, 64, 33, 25)\n",
      "[Inference] Cluster 4 | X=(509, 64, 31, 25)\n",
      "[Inference] Cluster 5 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 6 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 7 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 8 | X=(509, 64, 27, 25)\n",
      "[Inference] Cluster 9 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 10 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 11 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 12 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 13 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 14 | X=(509, 64, 31, 25)\n",
      "[Inference] Cluster 15 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 16 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 17 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 18 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 19 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 20 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 21 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 22 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 23 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 24 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 25 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 26 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 27 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 28 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 29 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 30 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 31 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 32 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 33 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 34 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 35 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 36 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 37 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 38 | X=(509, 64, 27, 25)\n",
      "[Inference] Cluster 39 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 40 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 41 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 42 | X=(509, 64, 27, 25)\n",
      "[Inference] Cluster 43 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 44 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 45 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 46 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 47 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 48 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 49 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 50 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 51 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 52 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 53 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 54 | X=(509, 64, 18, 25)\n",
      "[Inference] Cluster 55 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 56 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 57 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 58 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 59 | X=(509, 64, 21, 25)\n",
      "[Inference] Cluster 60 | X=(509, 64, 20, 25)\n",
      "[Inference] Cluster 61 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 62 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 63 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 64 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 65 | X=(509, 64, 21, 25)\n",
      "[Inference] Cluster 66 | X=(509, 64, 21, 25)\n",
      "[Inference] Cluster 67 | X=(509, 64, 17, 25)\n",
      "[Inference] Cluster 68 | X=(509, 64, 19, 25)\n",
      "[Inference] Cluster 69 | X=(509, 64, 16, 25)\n",
      "[Inference] Cluster 70 | X=(509, 64, 16, 25)\n",
      "[Inference] Cluster 71 | X=(509, 64, 15, 25)\n",
      "[Inference] Cluster 72 | X=(509, 64, 14, 25)\n",
      "[Inference] Cluster 73 | X=(509, 64, 13, 25)\n",
      "[Inference] Cluster 74 | X=(509, 64, 12, 25)\n",
      "[Inference] Cluster 75 | X=(509, 64, 15, 25)\n",
      "[Inference] Cluster 76 | X=(509, 64, 11, 25)\n",
      "[Inference] Cluster 77 | X=(509, 64, 10, 25)\n",
      "[Inference] Cluster 78 | X=(509, 64, 10, 25)\n",
      "[Inference] Cluster 79 | X=(509, 64, 10, 25)\n",
      "[Inference] Cluster 80 | X=(509, 64, 12, 25)\n",
      "[Inference] Cluster 81 | X=(509, 64, 11, 25)\n",
      "[Inference] Cluster 82 | X=(509, 64, 8, 25)\n",
      "[Inference] Cluster 83 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 84 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 85 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 86 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 87 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 88 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 89 | X=(509, 64, 7, 25)\n",
      "[Inference] Cluster 90 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 91 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 92 | X=(509, 64, 4, 25)\n",
      "[Inference] Cluster 93 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 94 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 95 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 96 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 97 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 98 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 99 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 100 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 101 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 102 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 103 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 104 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 105 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 106 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 107 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 108 | X=(309, 64, 1, 25)\n",
      "[Inference] Cluster 109 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 110 | X=(509, 64, 2, 25)\n",
      "✅ Done Block 9: inference signals saved to ./signals/a3c_signals_infer.csv (có regime thực).\n"
     ]
    }
   ],
   "source": [
    "# Block 9 — Inference từ checkpoint A3C (có regime thực từ df_backtest)\n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "SIG_DIR   = \"./signals/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reset signals file\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model định nghĩa lại (giống Block 😎 ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Tạo regime map từ df_backtest ---\n",
    "# df_backtest phải có cột: [\"ticker\",\"timestamp\",\"regime_bear\",\"regime_bull\",\"regime_sideway\"]\n",
    "regime_map = {\n",
    "    (row.ticker, pd.to_datetime(row.timestamp)): \n",
    "        np.array([row.regime_bear, row.regime_bull, row.regime_sideway], dtype=np.float32)\n",
    "    for _, row in df_backtest.iterrows()\n",
    "}\n",
    "\n",
    "# --- Inference function ---\n",
    "def infer_cluster(meta, batch_size=256):\n",
    "    c_id, tickers, dates, dates_shifted = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"], meta[\"dates_shifted\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"[Inference] Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # Load model checkpoint\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️ Model checkpoint not found: {model_path}, skip\")\n",
    "        return\n",
    "    model = A3CNet(F+3).to(device)  # +3 vì có regime one-hot\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Inference & save signals\n",
    "    total = B * N\n",
    "    with open(SIG_FILE, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, total, batch_size):\n",
    "                end = min(total, start+batch_size)\n",
    "                xb, mb, idx = [], [], []\n",
    "                for s in range(start, end):\n",
    "                    b, n = divmod(s, N)\n",
    "                    tk = tickers[n]\n",
    "                    dt = pd.to_datetime(dates[b])\n",
    "\n",
    "                    # regime thực tại ngày dt\n",
    "                    regime_vec = regime_map.get((tk, dt), np.array([0,0,0], dtype=np.float32))\n",
    "\n",
    "                    # append regime vào mỗi timestep của sequence\n",
    "                    seq_x = X[b, :, n, :]            # shape (T, F)\n",
    "                    seq_x = np.concatenate([seq_x, np.tile(regime_vec, (T,1))], axis=1)  # (T, F+3)\n",
    "\n",
    "                    xb.append(seq_x)\n",
    "                    mb.append(M[b, :, n, :])  # mask gốc không đổi (vẫn shape (T,F))\n",
    "                    idx.append((b, n))\n",
    "\n",
    "                xb = torch.tensor(np.stack(xb), dtype=torch.float32).to(device)\n",
    "                mb = torch.tensor(np.stack(mb), dtype=torch.float32).to(device)\n",
    "\n",
    "                # (không mask lên regime vì đó là thông tin \"luôn có\")\n",
    "                xb = xb * torch.cat([mb, torch.ones(mb.shape[0], mb.shape[1], 3, device=device)], dim=2)\n",
    "\n",
    "                logits, _ = model(xb)\n",
    "                acts = torch.argmax(logits, dim=-1).cpu().numpy() - 1  # (-1,0,1)\n",
    "\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    w.writerow([dates[b], tickers[n], int(acts[k])])\n",
    "\n",
    "                del xb, mb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, M, model\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run inference all clusters ---\n",
    "for meta in tensor_index:\n",
    "    infer_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 9: inference signals saved to {SIG_FILE} (có regime thực).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63784ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b95fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378de040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f487374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inference] Cluster 0 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 1 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 2 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 3 | X=(509, 64, 33, 25)\n",
      "[Inference] Cluster 4 | X=(509, 64, 31, 25)\n",
      "[Inference] Cluster 5 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 6 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 7 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 8 | X=(509, 64, 27, 25)\n",
      "[Inference] Cluster 9 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 10 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 11 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 12 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 13 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 14 | X=(509, 64, 31, 25)\n",
      "[Inference] Cluster 15 | X=(509, 64, 30, 25)\n",
      "[Inference] Cluster 16 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 17 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 18 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 19 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 20 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 21 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 22 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 23 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 24 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 25 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 26 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 27 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 28 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 29 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 30 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 31 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 32 | X=(509, 64, 29, 25)\n",
      "[Inference] Cluster 33 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 34 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 35 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 36 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 37 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 38 | X=(509, 64, 27, 25)\n",
      "[Inference] Cluster 39 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 40 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 41 | X=(509, 64, 28, 25)\n",
      "[Inference] Cluster 42 | X=(509, 64, 27, 25)\n",
      "[Inference] Cluster 43 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 44 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 45 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 46 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 47 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 48 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 49 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 50 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 51 | X=(509, 64, 26, 25)\n",
      "[Inference] Cluster 52 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 53 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 54 | X=(509, 64, 18, 25)\n",
      "[Inference] Cluster 55 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 56 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 57 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 58 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 59 | X=(509, 64, 21, 25)\n",
      "[Inference] Cluster 60 | X=(509, 64, 20, 25)\n",
      "[Inference] Cluster 61 | X=(509, 64, 25, 25)\n",
      "[Inference] Cluster 62 | X=(509, 64, 22, 25)\n",
      "[Inference] Cluster 63 | X=(509, 64, 23, 25)\n",
      "[Inference] Cluster 64 | X=(509, 64, 24, 25)\n",
      "[Inference] Cluster 65 | X=(509, 64, 21, 25)\n",
      "[Inference] Cluster 66 | X=(509, 64, 21, 25)\n",
      "[Inference] Cluster 67 | X=(509, 64, 17, 25)\n",
      "[Inference] Cluster 68 | X=(509, 64, 19, 25)\n",
      "[Inference] Cluster 69 | X=(509, 64, 16, 25)\n",
      "[Inference] Cluster 70 | X=(509, 64, 16, 25)\n",
      "[Inference] Cluster 71 | X=(509, 64, 15, 25)\n",
      "[Inference] Cluster 72 | X=(509, 64, 14, 25)\n",
      "[Inference] Cluster 73 | X=(509, 64, 13, 25)\n",
      "[Inference] Cluster 74 | X=(509, 64, 12, 25)\n",
      "[Inference] Cluster 75 | X=(509, 64, 15, 25)\n",
      "[Inference] Cluster 76 | X=(509, 64, 11, 25)\n",
      "[Inference] Cluster 77 | X=(509, 64, 10, 25)\n",
      "[Inference] Cluster 78 | X=(509, 64, 10, 25)\n",
      "[Inference] Cluster 79 | X=(509, 64, 10, 25)\n",
      "[Inference] Cluster 80 | X=(509, 64, 12, 25)\n",
      "[Inference] Cluster 81 | X=(509, 64, 11, 25)\n",
      "[Inference] Cluster 82 | X=(509, 64, 8, 25)\n",
      "[Inference] Cluster 83 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 84 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 85 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 86 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 87 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 88 | X=(509, 64, 5, 25)\n",
      "[Inference] Cluster 89 | X=(509, 64, 7, 25)\n",
      "[Inference] Cluster 90 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 91 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 92 | X=(509, 64, 4, 25)\n",
      "[Inference] Cluster 93 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 94 | X=(509, 64, 6, 25)\n",
      "[Inference] Cluster 95 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 96 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 97 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 98 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 99 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 100 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 101 | X=(509, 64, 3, 25)\n",
      "[Inference] Cluster 102 | X=(509, 64, 2, 25)\n",
      "[Inference] Cluster 103 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 104 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 105 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 106 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 107 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 108 | X=(309, 64, 1, 25)\n",
      "[Inference] Cluster 109 | X=(509, 64, 1, 25)\n",
      "[Inference] Cluster 110 | X=(509, 64, 2, 25)\n",
      "✅ Done Block 9: inference signals saved to ./signals/a3c_signals_infer.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 9 — Inference từ checkpoint A3C\n",
    "\n",
    "import os, gc, json, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DATA_DIR = \"./tensors/\"\n",
    "MODEL_DIR = \"./models/\"\n",
    "SIG_DIR   = \"./signals/\"\n",
    "os.makedirs(SIG_DIR, exist_ok=True)\n",
    "\n",
    "SIG_FILE = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reset signals file\n",
    "if os.path.exists(SIG_FILE):\n",
    "    os.remove(SIG_FILE)\n",
    "with open(SIG_FILE, \"w\", newline=\"\") as f:\n",
    "    csv.writer(f).writerow([\"date\",\"ticker\",\"signal\"])\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "# --- Model định nghĩa lại (giống Block 8) ---\n",
    "class A3CNet(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)\n",
    "        self.actor = nn.Linear(hidden, 3)   # short, flat, long\n",
    "        self.critic = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.actor(h), self.critic(h)\n",
    "\n",
    "# --- Inference function ---\n",
    "def infer_cluster(meta, batch_size=256):\n",
    "    c_id, tickers, dates, dates_shifted = meta[\"cluster\"], meta[\"tickers\"], meta[\"dates\"], meta[\"dates_shifted\"]\n",
    "    X = np.load(os.path.join(DATA_DIR, meta[\"tensor_file\"]), mmap_mode=\"r\")\n",
    "    M = np.load(os.path.join(DATA_DIR, meta[\"mask_file\"]), mmap_mode=\"r\")\n",
    "    if X.size == 0:\n",
    "        return\n",
    "    B, T, N, F = X.shape\n",
    "    print(f\"[Inference] Cluster {c_id} | X={X.shape}\")\n",
    "\n",
    "    # Load model checkpoint\n",
    "    model_path = os.path.join(MODEL_DIR, f\"a3c_cluster_{c_id}.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"⚠️ Model checkpoint not found: {model_path}, skip\")\n",
    "        return\n",
    "    model = A3CNet(F).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Inference & save signals\n",
    "    total = B * N\n",
    "    with open(SIG_FILE, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, total, batch_size):\n",
    "                end = min(total, start+batch_size)\n",
    "                xb, mb, idx = [], [], []\n",
    "                for s in range(start, end):\n",
    "                    b, n = divmod(s, N)\n",
    "                    xb.append(X[b, :, n, :])\n",
    "                    mb.append(M[b, :, n, :])\n",
    "                    idx.append((b, n))\n",
    "                xb = torch.tensor(np.stack(xb), dtype=torch.float32).to(device)\n",
    "                mb = torch.tensor(np.stack(mb), dtype=torch.float32).to(device)\n",
    "\n",
    "                # Áp dụng mask\n",
    "                xb = xb * mb\n",
    "\n",
    "                acts = torch.argmax(model(xb)[0], dim=-1).cpu().numpy() - 1  # (-1,0,1)\n",
    "                for k,(b,n) in enumerate(idx):\n",
    "                    # dùng dates[b] (ngày cuối window), nhưng reward tính T+1 (dates_shifted)\n",
    "                    w.writerow([dates[b], tickers[n], int(acts[k])])\n",
    "                del xb, mb, acts\n",
    "                gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    del X, M, model\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# --- Run inference all clusters ---\n",
    "for meta in tensor_index:\n",
    "    infer_cluster(meta)\n",
    "\n",
    "print(f\"✅ Done Block 9: inference signals saved to {SIG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f8af",
   "metadata": {},
   "source": [
    "**Block 10 : Huấn luyện Cluster DDPG (chỉ với trường hợp vị thế long)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89195ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 10 — Cluster DDPG + Execution Lag + Turnover Cost (Final, realistic backtest with extra logs)\n",
    "import os, gc, json, csv, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from FiinQuantX import FiinSession\n",
    "\n",
    "# ====== I/O paths ======\n",
    "DATA_DIR   = \"./tensors/\"\n",
    "SIG_DIR    = \"./signals/\"\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "MODEL_DIR  = \"./models/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ====== Config ======\n",
    "INIT_CAPITAL    = 10_000.0\n",
    "BENCHMARK_TKR   = \"VNINDEX\"\n",
    "EXECUTION_LAG   = 2           # T+2 execution/settlement (we align signals accordingly)\n",
    "COST_BPS        = 20          # 0.20% per round-trip on turnover\n",
    "STATE_LKBK      = 20\n",
    "MIN_NAMES_PER_CLUSTER = 1\n",
    "MAX_HOLD_TICKERS = 10\n",
    "MAX_HOLD_DAYS = 15\n",
    "SEED = 42\n",
    "\n",
    "# DDPG hyper\n",
    "EPOCHS       = 60\n",
    "BATCH_SIZE   = 128\n",
    "LR_ACTOR     = 1e-4\n",
    "LR_CRITIC    = 1e-4\n",
    "GAMMA        = 0.94\n",
    "TAU          = 0.01\n",
    "NOISE_STD    = 0.15\n",
    "NOISE_DECAY  = 0.97\n",
    "HIDDEN       = 128\n",
    "BUFFER_MAX   = 100_000\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ====== Load artifacts ======\n",
    "# A3C signals (inference)\n",
    "sig_file = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "if not os.path.exists(sig_file):\n",
    "    raise FileNotFoundError(f\"{sig_file} not found. Please run A3C inference earlier.\")\n",
    "signals = pd.read_csv(sig_file)\n",
    "signals[\"date\"] = pd.to_datetime(signals[\"date\"])\n",
    "\n",
    "# df_backtest must exist (from Block 7.5)\n",
    "# Expect df_backtest in notebook namespace or saved CSV in OUTPUT_DIR\n",
    "if 'df_backtest' not in globals():\n",
    "    fb_path = os.path.join(OUTPUT_DIR, \"df_backtest.csv\")\n",
    "    if os.path.exists(fb_path):\n",
    "        df_backtest = pd.read_csv(fb_path, parse_dates=[\"timestamp\"])\n",
    "    else:\n",
    "        raise FileNotFoundError(\"df_backtest not found in workspace and ./backtest_ddpg/df_backtest.csv missing. Produce df_backtest first.\")\n",
    "\n",
    "# Work copy\n",
    "df_px = df_backtest.rename(columns={\"timestamp\":\"date\"}).copy()\n",
    "df_px[\"date\"] = pd.to_datetime(df_px[\"date\"])\n",
    "\n",
    "# realized mapping used for shaping / planned exits (index by (ticker,date))\n",
    "realized = df_px[[\"ticker\",\"date\",\"exit_date\",\"exit_price\",\"realized_return\",\n",
    "                  \"entry_regime\",\"tp_level\",\"sl_level\",\"exit_type\",\"horizon_days\",\"fee\"]].copy()\n",
    "realized = realized.set_index([\"ticker\",\"date\"]).sort_index()\n",
    "\n",
    "# cluster metadata from tensors (tensor_index.json)\n",
    "tidx_path = os.path.join(DATA_DIR, \"tensor_index.json\")\n",
    "if not os.path.exists(tidx_path):\n",
    "    raise FileNotFoundError(f\"{tidx_path} not found. Provide tensor_index.json.\")\n",
    "with open(tidx_path, \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    for tk in meta.get(\"tickers\", []):\n",
    "        # keep first mapping encountered\n",
    "        ticker2cluster.setdefault(tk, meta.get(\"cluster\"))\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "# price wide and daily returns for realistic test\n",
    "px_wide = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "daily_ret = px_wide.pct_change().fillna(0.0)\n",
    "\n",
    "# ====== train/test split (time-based) ======\n",
    "TRAIN_START = pd.Timestamp(\"2023-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")\n",
    "TEST_END    = daily_ret.index.max()\n",
    "\n",
    "# ====== Prepare signals with execution lag (T+2) ======\n",
    "sig_wide_raw = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\").sort_index()\n",
    "idx_all = daily_ret.index.union(sig_wide_raw.index)\n",
    "daily_ret = daily_ret.reindex(idx_all).fillna(0.0)\n",
    "sig_wide = sig_wide_raw.reindex(idx_all).fillna(0.0)\n",
    "sig_wide_lag = sig_wide.shift(EXECUTION_LAG)   # we will use this for building actives\n",
    "\n",
    "# Keep tickers that exist in cluster mapping and price universe\n",
    "tickers = sorted([t for t in daily_ret.columns if t in ticker2cluster.index])\n",
    "daily_ret = daily_ret[tickers].astype(\"float32\")\n",
    "sig_wide_lag = sig_wide_lag[tickers].astype(\"float32\")\n",
    "cluster_of = ticker2cluster.loc[tickers]\n",
    "\n",
    "clusters = sorted(cluster_of.unique().tolist())\n",
    "cluster_members = {c: cluster_of[cluster_of==c].index.tolist() for c in clusters}\n",
    "C = len(clusters)\n",
    "\n",
    "# ====== Helpers to build cluster-level states using realized returns (for shaping during train) ======\n",
    "def build_state_arrays_realized(ret_w, sig_lag, realized_idx, start, end, K=STATE_LKBK):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "      - S_mat: (T', 2*C*K + 3) where we append (global) regime one-hot\n",
    "      - R_mat: (T', C) cluster realized returns (mean realized_return among active tickers)\n",
    "      - dates: dates2\n",
    "      - ACTIVE_masks: dict[c] -> DataFrame active mask aligned to dates2\n",
    "    \"\"\"\n",
    "    dates_all = ret_w.loc[start:end].index\n",
    "    act_cols, ret_cols = [], []\n",
    "    ACTIVE_masks = {}\n",
    "\n",
    "    for c in clusters:\n",
    "        tks = cluster_members[c]\n",
    "        if not tks:\n",
    "            act_c = pd.Series(0.0, index=dates_all, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates_all, name=c)\n",
    "            ACTIVE_masks[c] = pd.DataFrame(0.0, index=dates_all, columns=tks)\n",
    "        else:\n",
    "            S_c = sig_lag[tks].reindex(dates_all).fillna(0.0)\n",
    "            active_mask = (S_c > 0).astype(\"float32\")   # long-only\n",
    "            ACTIVE_masks[c] = active_mask\n",
    "\n",
    "            # lookup realized_return for each (tk, date)\n",
    "            rr = pd.DataFrame(index=dates_all, columns=tks, dtype=\"float32\")\n",
    "            for tk in tks:\n",
    "                vals = []\n",
    "                for d in dates_all:\n",
    "                    try:\n",
    "                        vals.append(realized_idx.loc[(tk, d), \"realized_return\"])\n",
    "                    except Exception:\n",
    "                        vals.append(np.nan)\n",
    "                rr[tk] = pd.Series(vals, index=dates_all, dtype=\"float32\")\n",
    "\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0).fillna(0.0)   # equal weight among active tickers\n",
    "            ret_c = (rr.fillna(0.0) * w).sum(axis=1).astype(\"float32\")\n",
    "            act_c = active_mask.mean(axis=1).astype(\"float32\")\n",
    "\n",
    "        act_cols.append(act_c.rename(c))\n",
    "        ret_cols.append(ret_c.rename(c))\n",
    "\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "\n",
    "    Tfull = len(dates_all)\n",
    "    # build lookback stacks: (T, C, K)\n",
    "    A_stack = np.zeros((Tfull, C, K), dtype=\"float32\")\n",
    "    R_stack = np.zeros((Tfull, C, K), dtype=\"float32\")\n",
    "    for k in range(K):\n",
    "        A_stack[:, :, k] = act_df.shift(k).fillna(0.0).values\n",
    "        R_stack[:, :, k] = cret_df.shift(k).fillna(0.0).values\n",
    "\n",
    "    valid_idx = np.arange(Tfull) >= (K - 1)\n",
    "    dates2 = dates_all[valid_idx]\n",
    "    A3 = A_stack[valid_idx]   # (T', C, K)\n",
    "    R3 = R_stack[valid_idx]   # (T', C, K)\n",
    "\n",
    "    S_flat = np.concatenate([A3.reshape(len(dates2), -1), R3.reshape(len(dates2), -1)], axis=1).astype(\"float32\")\n",
    "\n",
    "    # global regime one-hot from realized_idx (majority across tickers if possible)\n",
    "    regimes = []\n",
    "    for d in dates2:\n",
    "        try:\n",
    "            sample = realized_idx.xs(d, level=\"date\")\n",
    "            rvec = np.array([\n",
    "                sample.get(\"entry_regime\", (sample.index.to_series()*0).astype(object)).apply(lambda x: 1 if x==\"bear\" else 0).mean() if \"entry_regime\" in sample.columns else 0.0,\n",
    "                sample.get(\"entry_regime\", (sample.index.to_series()*0).astype(object)).apply(lambda x: 1 if x==\"bull\" else 0).mean() if \"entry_regime\" in sample.columns else 0.0,\n",
    "                sample.get(\"entry_regime\", (sample.index.to_series()*0).astype(object)).apply(lambda x: 1 if x==\"sideway\" else 0).mean() if \"entry_regime\" in sample.columns else 0.0\n",
    "            ], dtype=\"float32\")\n",
    "        except Exception:\n",
    "            rvec = np.array([0.0,0.0,0.0], dtype=\"float32\")\n",
    "        regimes.append(rvec)\n",
    "    regimes = np.stack(regimes, axis=0)\n",
    "\n",
    "    S_mat = np.concatenate([S_flat, regimes], axis=1)\n",
    "    R_mat = cret_df.loc[dates2].values.astype(\"float32\")\n",
    "    ACTIVE_masks = {c: ACTIVE_masks[c].loc[dates2] for c in clusters}\n",
    "    return S_mat, R_mat, dates2, ACTIVE_masks\n",
    "\n",
    "# Build train/test arrays (for DDPG training)\n",
    "S_train, R_train, d_train, ACTIVE_train = build_state_arrays_realized(daily_ret, sig_wide_lag, realized, TRAIN_START, TRAIN_END, K=STATE_LKBK)\n",
    "S_test,  R_test,  d_test,  ACTIVE_test  = build_state_arrays_realized(daily_ret, sig_wide_lag, realized, TEST_START,  TEST_END,  K=STATE_LKBK)\n",
    "\n",
    "# ====== DDPG actor/critic definitions ======\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, a_dim)\n",
    "        )\n",
    "    def forward(self, s):\n",
    "        logits = self.net(s)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, s, a):\n",
    "        return self.net(torch.cat([s,a], dim=-1))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, maxlen=BUFFER_MAX):\n",
    "        self.maxlen = maxlen; self.buf=[]\n",
    "    def push(self, s,a,r,s2):\n",
    "        if len(self.buf) >= self.maxlen:\n",
    "            self.buf.pop(0)\n",
    "        self.buf.append((s,a,r,s2))\n",
    "    def sample(self, bs):\n",
    "        n = min(bs, len(self.buf))\n",
    "        idx = np.random.choice(len(self.buf), n, replace=False)\n",
    "        s,a,r,s2 = zip(*[self.buf[i] for i in idx])\n",
    "        return (np.array(s, np.float32), np.array(a, np.float32),\n",
    "                np.array(r, np.float32).reshape(-1,1), np.array(s2, np.float32))\n",
    "\n",
    "def soft_update(src, tgt, tau):\n",
    "    with torch.no_grad():\n",
    "        for p, tp in zip(src.parameters(), tgt.parameters()):\n",
    "            tp.data.mul_(1-tau); tp.data.add_(tau*p.data)\n",
    "\n",
    "# Reward helper (cluster-level) used in training shaping\n",
    "def port_reward_cluster(w_cluster, r_cluster, prev_w_cluster=None, cost_bps=COST_BPS, train_penalty=True):\n",
    "    gross = float(np.dot(w_cluster, r_cluster))\n",
    "    turnover = 0.0\n",
    "    if prev_w_cluster is not None:\n",
    "        turnover = float(np.sum(np.abs(w_cluster - prev_w_cluster)))\n",
    "    fee = (cost_bps/1e4) * turnover\n",
    "    penalty = fee if train_penalty else 0.0\n",
    "    return gross - penalty\n",
    "\n",
    "# ====== Instantiate models & training setup ======\n",
    "s_dim = S_train.shape[1]; a_dim = C\n",
    "actor = Actor(s_dim, a_dim).to(DEVICE)\n",
    "critic = Critic(s_dim, a_dim).to(DEVICE)\n",
    "t_actor = Actor(s_dim, a_dim).to(DEVICE); t_actor.load_state_dict(actor.state_dict())\n",
    "t_critic = Critic(s_dim, a_dim).to(DEVICE); t_critic.load_state_dict(critic.state_dict())\n",
    "optA = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "optC = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "mse = nn.MSELoss()\n",
    "buf = ReplayBuffer(BUFFER_MAX)\n",
    "\n",
    "# ====== Training loop (DDPG) ======\n",
    "prev_w = None\n",
    "noise_std = NOISE_STD\n",
    "min_buffer = max(500, BATCH_SIZE*5)\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    c_loss = a_loss = 0.0\n",
    "    prev_w = None\n",
    "    # iterate time steps\n",
    "    for t in range(len(S_train)-1):\n",
    "        s = torch.from_numpy(S_train[t]).to(DEVICE).unsqueeze(0)\n",
    "        s2 = torch.from_numpy(S_train[t+1]).to(DEVICE).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            w = actor(s).cpu().numpy()[0]\n",
    "        # exploration on simplex via logits noise\n",
    "        logits = np.log(np.clip(w, 1e-9, 1.0))\n",
    "        logits = logits + np.random.normal(0, noise_std, size=a_dim)\n",
    "        w_e = np.exp(logits); w_e = w_e / (w_e.sum() + 1e-12)\n",
    "        r = port_reward_cluster(w_e, R_train[t], prev_w_cluster=prev_w, train_penalty=True)\n",
    "        prev_w = w_e.copy()\n",
    "        buf.push(S_train[t], w_e, r, S_train[t+1])\n",
    "\n",
    "        # update networks\n",
    "        if len(buf.buf) >= min_buffer:\n",
    "            sb, ab, rb, s2b = buf.sample(BATCH_SIZE)\n",
    "            sb = torch.from_numpy(sb).to(DEVICE)\n",
    "            ab = torch.from_numpy(ab).to(DEVICE)\n",
    "            rb = torch.from_numpy(rb).to(DEVICE)\n",
    "            s2b = torch.from_numpy(s2b).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                a2 = t_actor(s2b)\n",
    "                q2 = t_critic(s2b, a2)\n",
    "                y = rb + GAMMA * q2\n",
    "\n",
    "            q = critic(sb, ab)\n",
    "            lc = mse(q, y)\n",
    "            optC.zero_grad(); lc.backward(); optC.step()\n",
    "\n",
    "            ap = actor(sb)\n",
    "            la = -critic(sb, ap).mean()\n",
    "            optA.zero_grad(); la.backward(); optA.step()\n",
    "\n",
    "            soft_update(actor, t_actor, TAU)\n",
    "            soft_update(critic, t_critic, TAU)\n",
    "\n",
    "            c_loss += float(lc.item()); a_loss += float(la.item())\n",
    "\n",
    "    noise_std *= NOISE_DECAY\n",
    "    print(f\"[DDPG] Epoch {ep+1}/{EPOCHS} | Critic {c_loss:.4f} | Actor {a_loss:.4f} | noise {noise_std:.5f}\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# Save actor\n",
    "torch.save(actor.state_dict(), os.path.join(MODEL_DIR, \"ddpg_actor.pt\"))\n",
    "\n",
    "# ====== Backtest (realistic daily returns, dynamic top-10, SL/TP, cash buffer) ======\n",
    "dates = pd.DatetimeIndex(d_test)   # aligned dates from state builder\n",
    "dates = dates.sort_values()\n",
    "capital = INIT_CAPITAL\n",
    "nav = capital\n",
    "\n",
    "# Outputs & logs\n",
    "portfolio_value = pd.Series(index=dates, dtype=\"float64\")\n",
    "portfolio_value.iloc[0] = capital\n",
    "\n",
    "# Trade log: detailed fields incl entry_alloc/exit_alloc\n",
    "trade_log_cols = [\n",
    "    \"entry_date\",\"exit_date\",\"ticker\",\"entry_price\",\"exit_price\",\n",
    "    \"entry_alloc\",\"exit_alloc\",\"entry_regime\",\"exit_type\",\n",
    "    \"tp_level\",\"sl_level\",\"realized_return\",\"holding_days\",\"fee\"\n",
    "]\n",
    "trade_log = []\n",
    "\n",
    "# cluster weights CSV writer\n",
    "cw_path = os.path.join(OUTPUT_DIR, \"cluster_weights_test.csv\")\n",
    "with open(cw_path, \"w\", newline=\"\") as f:\n",
    "    cw = csv.writer(f); cw.writerow([\"date\"] + [f\"cluster_{c}\" for c in clusters] + [\"cash_buffer\"])\n",
    "\n",
    "# dynamic daily ticker weights (post-rebalance)\n",
    "weights_by_ticker_path = os.path.join(OUTPUT_DIR, \"weights_by_ticker.csv\")\n",
    "weights_df = pd.DataFrame(0.0, index=dates, columns=tickers, dtype=\"float32\")\n",
    "\n",
    "# holdings: dict[ticker] -> dict(entry_date, entry_price, planned_exit_date, planned_exit_price, entry_regime, tp, sl, entry_alloc)\n",
    "holdings = {}\n",
    "\n",
    "# previous day weights for turnover calc\n",
    "prev_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "\n",
    "# Extra logs\n",
    "regime_daily_list = []\n",
    "holdings_lastday_rows = []\n",
    "\n",
    "# ====== Backtest main loop (FULL with logging) ======\n",
    "for i, dt in enumerate(dates):\n",
    "    # prepare state for actor: use previous state to avoid leakage (action lag 1 step)\n",
    "    s_idx = i-1 if i>0 else i\n",
    "    s = torch.from_numpy(S_test[s_idx].astype(\"float32\")).to(DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        w_c = actor(s).cpu().numpy()[0].astype(\"float32\")\n",
    "    # clip & normalize\n",
    "    w_c = np.clip(w_c, 0.0, 1.0)\n",
    "    if w_c.sum() > 0:\n",
    "        w_c = w_c / w_c.sum()\n",
    "    else:\n",
    "        w_c = np.ones_like(w_c) / len(w_c)\n",
    "\n",
    "    # determine cash buffer from global regime at date dt (from df_px if present)\n",
    "    cash_buffer = 0.15  # default sideway\n",
    "    majority_regime = None\n",
    "    try:\n",
    "        todays = df_px[df_px[\"date\"] == dt]\n",
    "        if \"entry_regime\" in todays.columns:\n",
    "            mode = todays[\"entry_regime\"].mode()\n",
    "            if len(mode) > 0:\n",
    "                modev = mode.iloc[0]\n",
    "                if modev == \"bull\": cash_buffer = 0.02\n",
    "                elif modev == \"bear\": cash_buffer = 0.35\n",
    "                else: cash_buffer = 0.15\n",
    "                majority_regime = modev\n",
    "    except Exception:\n",
    "        majority_regime = None\n",
    "\n",
    "    # save cluster weights (include cash_buffer)\n",
    "    with open(cw_path, \"a\", newline=\"\") as f:\n",
    "        cw = csv.writer(f)\n",
    "        cw.writerow([dt.strftime(\"%Y-%m-%d\")] + [float(x) for x in w_c] + [float(cash_buffer)])\n",
    "\n",
    "    # Map cluster weights down to candidate tickers and compute per-ticker score\n",
    "    score = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "    if dt in sig_wide_lag.index:\n",
    "        active_matrix_row = sig_wide_lag.loc[dt]\n",
    "    else:\n",
    "        active_matrix_row = pd.Series(0.0, index=tickers)\n",
    "\n",
    "    for j, c in enumerate(clusters):\n",
    "        members = cluster_members[c]\n",
    "        if not members: continue\n",
    "        try:\n",
    "            act_row = active_matrix_row[members]\n",
    "        except Exception:\n",
    "            act_row = pd.Series(0.0, index=members)\n",
    "        valid = [tk for tk in members if (tk in act_row.index and act_row[tk] > 0)]\n",
    "        if len(valid) >= MIN_NAMES_PER_CLUSTER:\n",
    "            per_tk = float(w_c[j] / len(valid))\n",
    "            score.loc[valid] += per_tk\n",
    "\n",
    "    # Now choose top-k but include held tickers\n",
    "    candidates = score[score > 0].sort_values(ascending=False)\n",
    "    held_now = list(holdings.keys())\n",
    "    combined_candidates = pd.concat([candidates, pd.Series(0.0, index=held_now)]).groupby(level=0).first()\n",
    "    topk = combined_candidates.sort_values(ascending=False).head(MAX_HOLD_TICKERS).index.tolist()\n",
    "\n",
    "    target_scores = score.loc[topk].copy()\n",
    "    target_scores = target_scores.fillna(0.0)\n",
    "    total_score = target_scores.sum()\n",
    "    investable = 1.0 - cash_buffer\n",
    "    if total_score <= 1e-12:\n",
    "        target_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "    else:\n",
    "        target_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "        alloc = (target_scores / total_score) * investable\n",
    "        target_w_ticker.loc[alloc.index] = alloc.values\n",
    "\n",
    "    # Compute turnover and apply fee immediately (approx)\n",
    "    turnover = float(np.sum(np.abs(target_w_ticker.values - prev_w_ticker.values)))\n",
    "    fee = (COST_BPS / 1e4) * turnover\n",
    "    nav = nav * (1.0 - fee)\n",
    "\n",
    "    # Detect opens/closes\n",
    "    new_open = [tk for tk in target_w_ticker.index if (prev_w_ticker.loc[tk] == 0.0 and target_w_ticker.loc[tk] > 0.0)]\n",
    "    closed = [tk for tk in prev_w_ticker.index if (prev_w_ticker.loc[tk] > 0.0 and target_w_ticker.loc[tk] == 0.0)]\n",
    "\n",
    "    # Forced exits (planned exit or max_hold_days)\n",
    "    to_force_exit = []\n",
    "    for tk, info in list(holdings.items()):\n",
    "        planned_exit = info.get(\"planned_exit_date\", pd.NaT)\n",
    "        entry_dt = info[\"entry_date\"]\n",
    "        if pd.notna(planned_exit) and planned_exit <= dt:\n",
    "            to_force_exit.append(tk); continue\n",
    "        hold_days = (dt - entry_dt).days\n",
    "        if isinstance(hold_days, (int, np.integer)) and hold_days >= MAX_HOLD_DAYS:\n",
    "            to_force_exit.append(tk)\n",
    "\n",
    "    # Execute forced exits first\n",
    "    for tk in to_force_exit:\n",
    "        info = holdings.pop(tk)\n",
    "        entry_dt = info[\"entry_date\"]\n",
    "        entry_price = info.get(\"entry_price\", np.nan)\n",
    "        entry_alloc = info.get(\"entry_alloc\", 0.0)\n",
    "        try:\n",
    "            rec = realized.loc[(tk, entry_dt)]\n",
    "            exit_price = rec[\"exit_price\"]; exit_date = rec[\"exit_date\"]\n",
    "            rret = rec[\"realized_return\"]; etype = rec.get(\"exit_type\", \"forced\")\n",
    "            fee_tr = rec.get(\"fee\", (COST_BPS/1e4)*abs(prev_w_ticker.loc[tk]))\n",
    "        except Exception:\n",
    "            exit_price = px_wide.loc[dt, tk] if (dt in px_wide.index and tk in px_wide.columns) else np.nan\n",
    "            exit_date = dt\n",
    "            rret = np.log(exit_price / entry_price) if pd.notna(exit_price) and entry_price>0 else np.nan\n",
    "            etype = \"forced\"\n",
    "            fee_tr = (COST_BPS/1e4) * abs(prev_w_ticker.loc[tk])\n",
    "        holding_days = (exit_date - entry_dt).days if pd.notna(exit_date) else np.nan\n",
    "\n",
    "        trade_log.append({\n",
    "            \"entry_date\": entry_dt, \"exit_date\": exit_date, \"ticker\": tk,\n",
    "            \"entry_price\": entry_price, \"exit_price\": exit_price,\n",
    "            \"entry_alloc\": float(entry_alloc), \"exit_alloc\": float(prev_w_ticker.loc[tk]) if tk in prev_w_ticker.index else 0.0,\n",
    "            \"entry_regime\": info.get(\"entry_regime\", None),\n",
    "            \"exit_type\": etype,\n",
    "            \"tp_level\": info.get(\"tp_level\", np.nan),\n",
    "            \"sl_level\": info.get(\"sl_level\", np.nan),\n",
    "            \"realized_return\": rret, \"holding_days\": holding_days, \"fee\": fee_tr\n",
    "        })\n",
    "        prev_w = prev_w_ticker.loc[tk]\n",
    "        if not pd.isna(rret):\n",
    "            nav = nav * (1.0 + prev_w * rret)\n",
    "        prev_w_ticker.loc[tk] = 0.0\n",
    "\n",
    "    # Open new positions (record entry_alloc)\n",
    "    for tk in new_open:\n",
    "        if len(holdings) >= MAX_HOLD_TICKERS:\n",
    "            continue\n",
    "        entry_price = px_wide.loc[dt, tk] if (dt in px_wide.index and tk in px_wide.columns) else np.nan\n",
    "        planned_exit_date = np.nan; planned_exit_price = np.nan; tp_level = np.nan; sl_level = np.nan; entry_regime = np.nan\n",
    "        try:\n",
    "            rec = realized.loc[(tk, dt)]\n",
    "            planned_exit_date = rec[\"exit_date\"]; planned_exit_price = rec[\"exit_price\"]\n",
    "            tp_level = rec.get(\"tp_level\", np.nan); sl_level = rec.get(\"sl_level\", np.nan)\n",
    "            entry_regime = rec.get(\"entry_regime\", np.nan)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        holdings[tk] = {\n",
    "            \"entry_date\": dt,\n",
    "            \"entry_price\": entry_price,\n",
    "            \"planned_exit_date\": planned_exit_date,\n",
    "            \"planned_exit_price\": planned_exit_price,\n",
    "            \"entry_regime\": entry_regime,\n",
    "            \"tp_level\": tp_level,\n",
    "            \"sl_level\": sl_level,\n",
    "            \"entry_alloc\": float(target_w_ticker.loc[tk]) if tk in target_w_ticker.index else 0.0\n",
    "        }\n",
    "\n",
    "    # Rebalance: use target weights as current_weights\n",
    "    current_weights = target_w_ticker.copy()\n",
    "    if current_weights.sum() > 1.0:\n",
    "        current_weights = current_weights / current_weights.sum()\n",
    "\n",
    "    # compute day's portfolio return using daily_ret (mark-to-market)\n",
    "    if dt in daily_ret.index:\n",
    "        r_vec = daily_ret.loc[dt].reindex(current_weights.index).fillna(0.0).values\n",
    "    else:\n",
    "        r_vec = np.zeros(len(current_weights), dtype=float)\n",
    "    port_daily_ret = float(np.dot(current_weights.values, r_vec))\n",
    "    nav = nav * (1.0 + port_daily_ret)\n",
    "\n",
    "    # store weights and update prev_w_ticker\n",
    "    weights_df.loc[dt] = current_weights\n",
    "    prev_w_ticker = current_weights.copy()\n",
    "\n",
    "    # Planned exits today: finalize and remove holdings (record exit_alloc)\n",
    "    for tk in list(holdings.keys()):\n",
    "        info = holdings[tk]\n",
    "        planned_exit = info.get(\"planned_exit_date\", pd.NaT)\n",
    "        if pd.notna(planned_exit) and planned_exit == dt:\n",
    "            entry_dt = info[\"entry_date\"]; entry_price = info[\"entry_price\"]; entry_alloc = info.get(\"entry_alloc\", 0.0)\n",
    "            try:\n",
    "                rec = realized.loc[(tk, entry_dt)]\n",
    "                exit_price = rec[\"exit_price\"]; exit_date = rec[\"exit_date\"]; rret = rec[\"realized_return\"]\n",
    "                etype = rec.get(\"exit_type\", \"exit\"); fee_tr = rec.get(\"fee\", (COST_BPS/1e4)*abs(prev_w_ticker.loc[tk]))\n",
    "            except Exception:\n",
    "                exit_price = px_wide.loc[dt, tk] if (dt in px_wide.index and tk in px_wide.columns) else np.nan\n",
    "                exit_date = dt\n",
    "                rret = np.log(exit_price / entry_price) if pd.notna(exit_price) and entry_price>0 else np.nan\n",
    "                etype = \"exit\"; fee_tr = (COST_BPS/1e4)*abs(prev_w_ticker.loc[tk])\n",
    "\n",
    "            trade_log.append({\n",
    "                \"entry_date\": entry_dt, \"exit_date\": exit_date, \"ticker\": tk,\n",
    "                \"entry_price\": entry_price, \"exit_price\": exit_price,\n",
    "                \"entry_alloc\": float(entry_alloc), \"exit_alloc\": float(prev_w_ticker.loc[tk]) if tk in prev_w_ticker.index else 0.0,\n",
    "                \"entry_regime\": info.get(\"entry_regime\", None), \"exit_type\": etype,\n",
    "                \"tp_level\": info.get(\"tp_level\", np.nan), \"sl_level\": info.get(\"sl_level\", np.nan),\n",
    "                \"realized_return\": rret, \"holding_days\": (exit_date - entry_dt).days if pd.notna(exit_date) else np.nan,\n",
    "                \"fee\": fee_tr\n",
    "            })\n",
    "            prev_w_ticker.loc[tk] = 0.0\n",
    "            holdings.pop(tk, None)\n",
    "\n",
    "    # Save NAV + Logs daily\n",
    "    portfolio_value.iloc[i] = nav\n",
    "    regime_daily_list.append({\n",
    "        \"date\": dt, \"regime\": majority_regime, \"cash_buffer\": cash_buffer, \"nav\": nav\n",
    "    })\n",
    "    # snapshot holdings for this day (after rebal/mark-to-market)\n",
    "    for tk, info in holdings.items():\n",
    "        holdings_lastday_rows.append({\n",
    "            \"snapshot_date\": dt,\n",
    "            \"ticker\": tk,\n",
    "            \"entry_date\": info[\"entry_date\"],\n",
    "            \"entry_price\": info[\"entry_price\"],\n",
    "            \"tp_level\": info.get(\"tp_level\", np.nan),\n",
    "            \"sl_level\": info.get(\"sl_level\", np.nan),\n",
    "            \"entry_regime\": info.get(\"entry_regime\", None),\n",
    "            \"weight\": float(current_weights.loc[tk]) if tk in current_weights.index else 0.0,\n",
    "            \"nav\": nav,\n",
    "            \"entry_alloc\": info.get(\"entry_alloc\", 0.0)\n",
    "        })\n",
    "\n",
    "    if (i % 50) == 0:\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ====== Save outputs ======\n",
    "portfolio_value.to_frame(\"portfolio_value\").to_csv(os.path.join(OUTPUT_DIR, \"portfolio_value_test.csv\"))\n",
    "weights_df.to_csv(weights_by_ticker_path)\n",
    "pd.DataFrame(trade_log).sort_values([\"entry_date\",\"ticker\"]).to_csv(os.path.join(OUTPUT_DIR, \"trade_log.csv\"), index=False)\n",
    "pd.DataFrame(regime_daily_list).to_csv(os.path.join(OUTPUT_DIR, \"regime_daily.csv\"), index=False)\n",
    "pd.DataFrame(holdings_lastday_rows).to_csv(os.path.join(OUTPUT_DIR, \"holdings_lastday.csv\"), index=False)\n",
    "\n",
    "# ==== Benchmark (FiinQuantX call unchanged) ====\n",
    "print(\"Fetching VNINDEX for benchmark (buy & hold)...\")\n",
    "client = FiinSession(username=\"DSTC_18@fiinquant.vn\", password=\"Fiinquant0606\").login()\n",
    "bench = client.Fetch_Trading_Data(\n",
    "    realtime=False, tickers=BENCHMARK_TKR, fields=['close'],\n",
    "    adjusted=True, by=\"1d\", from_date=str(dates.min().date())\n",
    ").get_data()\n",
    "bench[\"date\"] = pd.to_datetime(bench[\"timestamp\"])\n",
    "bench = bench.set_index(\"date\")[\"close\"].sort_index().reindex(dates).ffill().bfill()\n",
    "bench_ret = bench.pct_change().fillna(0.0)\n",
    "benchmark_value = (1 + bench_ret).cumprod() * INIT_CAPITAL\n",
    "benchmark_value.to_frame(\"benchmark_value\").to_csv(os.path.join(OUTPUT_DIR, \"benchmark_value_test.csv\"))\n",
    "\n",
    "# Save actor for inference later\n",
    "torch.save(actor.state_dict(), os.path.join(MODEL_DIR, \"ddpg_actor.pt\"))\n",
    "\n",
    "print(\"✅ Done Block 10: DDPG train + realistic backtest saved to\", OUTPUT_DIR)\n",
    "print(\" - portfolio:\", os.path.join(OUTPUT_DIR, \"portfolio_value_test.csv\"))\n",
    "print(\" - weights:\", weights_by_ticker_path)\n",
    "print(\" - trades:\", os.path.join(OUTPUT_DIR, \"trade_log.csv\"))\n",
    "print(\" - regime daily:\", os.path.join(OUTPUT_DIR, \"regime_daily.csv\"))\n",
    "print(\" - holdings snapshot:\", os.path.join(OUTPUT_DIR, \"holdings_lastday.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ea5e2",
   "metadata": {},
   "source": [
    "**code mới nhất nằm ngày trên**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798fda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f1314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20116a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d30a9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDPG] Epoch 1/120 | Critic 0.0000 | Actor 0.0000 | noise 0.14550\n",
      "[DDPG] Epoch 2/120 | Critic 0.0010 | Actor -8.7141 | noise 0.14113\n",
      "[DDPG] Epoch 3/120 | Critic 0.0056 | Actor -9.7634 | noise 0.13690\n",
      "[DDPG] Epoch 4/120 | Critic 0.0040 | Actor -6.7523 | noise 0.13279\n",
      "[DDPG] Epoch 5/120 | Critic 0.0009 | Actor -6.5903 | noise 0.12881\n",
      "[DDPG] Epoch 6/120 | Critic 0.0003 | Actor -6.5519 | noise 0.12495\n",
      "[DDPG] Epoch 7/120 | Critic 0.0005 | Actor -6.5082 | noise 0.12120\n",
      "[DDPG] Epoch 8/120 | Critic 0.0007 | Actor -7.3679 | noise 0.11756\n",
      "[DDPG] Epoch 9/120 | Critic 0.0005 | Actor -7.5439 | noise 0.11403\n",
      "[DDPG] Epoch 10/120 | Critic 0.0004 | Actor -7.2145 | noise 0.11061\n",
      "[DDPG] Epoch 11/120 | Critic 0.0003 | Actor -6.9752 | noise 0.10730\n",
      "[DDPG] Epoch 12/120 | Critic 0.0004 | Actor -7.0616 | noise 0.10408\n",
      "[DDPG] Epoch 13/120 | Critic 0.0004 | Actor -6.8767 | noise 0.10095\n",
      "[DDPG] Epoch 14/120 | Critic 0.0003 | Actor -7.0262 | noise 0.09793\n",
      "[DDPG] Epoch 15/120 | Critic 0.0003 | Actor -7.1496 | noise 0.09499\n",
      "[DDPG] Epoch 16/120 | Critic 0.0003 | Actor -7.4441 | noise 0.09214\n",
      "[DDPG] Epoch 17/120 | Critic 0.0003 | Actor -7.4881 | noise 0.08937\n",
      "[DDPG] Epoch 18/120 | Critic 0.0002 | Actor -7.4787 | noise 0.08669\n",
      "[DDPG] Epoch 19/120 | Critic 0.0006 | Actor -7.9147 | noise 0.08409\n",
      "[DDPG] Epoch 20/120 | Critic 0.0010 | Actor -8.3561 | noise 0.08157\n",
      "[DDPG] Epoch 21/120 | Critic 0.0013 | Actor -9.1349 | noise 0.07912\n",
      "[DDPG] Epoch 22/120 | Critic 0.0007 | Actor -10.1618 | noise 0.07675\n",
      "[DDPG] Epoch 23/120 | Critic 0.0005 | Actor -11.1768 | noise 0.07445\n",
      "[DDPG] Epoch 24/120 | Critic 0.0004 | Actor -11.8998 | noise 0.07221\n",
      "[DDPG] Epoch 25/120 | Critic 0.0005 | Actor -12.5455 | noise 0.07005\n",
      "[DDPG] Epoch 26/120 | Critic 0.0005 | Actor -12.9871 | noise 0.06794\n",
      "[DDPG] Epoch 27/120 | Critic 0.0005 | Actor -13.5437 | noise 0.06591\n",
      "[DDPG] Epoch 28/120 | Critic 0.0003 | Actor -14.3084 | noise 0.06393\n",
      "[DDPG] Epoch 29/120 | Critic 0.0004 | Actor -14.3446 | noise 0.06201\n",
      "[DDPG] Epoch 30/120 | Critic 0.0003 | Actor -14.4043 | noise 0.06015\n",
      "[DDPG] Epoch 31/120 | Critic 0.0003 | Actor -14.5072 | noise 0.05835\n",
      "[DDPG] Epoch 32/120 | Critic 0.0003 | Actor -14.4656 | noise 0.05660\n",
      "[DDPG] Epoch 33/120 | Critic 0.0003 | Actor -14.4841 | noise 0.05490\n",
      "[DDPG] Epoch 34/120 | Critic 0.0003 | Actor -14.6968 | noise 0.05325\n",
      "[DDPG] Epoch 35/120 | Critic 0.0004 | Actor -14.7480 | noise 0.05165\n",
      "[DDPG] Epoch 36/120 | Critic 0.0004 | Actor -15.2192 | noise 0.05010\n",
      "[DDPG] Epoch 37/120 | Critic 0.0004 | Actor -15.6006 | noise 0.04860\n",
      "[DDPG] Epoch 38/120 | Critic 0.0004 | Actor -15.8630 | noise 0.04714\n",
      "[DDPG] Epoch 39/120 | Critic 0.0004 | Actor -15.7636 | noise 0.04573\n",
      "[DDPG] Epoch 40/120 | Critic 0.0005 | Actor -16.3344 | noise 0.04436\n",
      "[DDPG] Epoch 41/120 | Critic 0.0006 | Actor -16.9827 | noise 0.04303\n",
      "[DDPG] Epoch 42/120 | Critic 0.0006 | Actor -17.6564 | noise 0.04174\n",
      "[DDPG] Epoch 43/120 | Critic 0.0005 | Actor -18.5049 | noise 0.04048\n",
      "[DDPG] Epoch 44/120 | Critic 0.0005 | Actor -19.2488 | noise 0.03927\n",
      "[DDPG] Epoch 45/120 | Critic 0.0006 | Actor -19.7508 | noise 0.03809\n",
      "[DDPG] Epoch 46/120 | Critic 0.0006 | Actor -20.0015 | noise 0.03695\n",
      "[DDPG] Epoch 47/120 | Critic 0.0006 | Actor -20.3293 | noise 0.03584\n",
      "[DDPG] Epoch 48/120 | Critic 0.0011 | Actor -20.6638 | noise 0.03476\n",
      "[DDPG] Epoch 49/120 | Critic 0.0024 | Actor -21.0474 | noise 0.03372\n",
      "[DDPG] Epoch 50/120 | Critic 0.0036 | Actor -21.4115 | noise 0.03271\n",
      "[DDPG] Epoch 51/120 | Critic 0.0052 | Actor -21.8491 | noise 0.03173\n",
      "[DDPG] Epoch 52/120 | Critic 0.0044 | Actor -22.3489 | noise 0.03078\n",
      "[DDPG] Epoch 53/120 | Critic 0.0050 | Actor -22.5442 | noise 0.02985\n",
      "[DDPG] Epoch 54/120 | Critic 0.0058 | Actor -22.6948 | noise 0.02896\n",
      "[DDPG] Epoch 55/120 | Critic 0.0053 | Actor -23.0281 | noise 0.02809\n",
      "[DDPG] Epoch 56/120 | Critic 0.0056 | Actor -23.2137 | noise 0.02725\n",
      "[DDPG] Epoch 57/120 | Critic 0.0062 | Actor -23.3679 | noise 0.02643\n",
      "[DDPG] Epoch 58/120 | Critic 0.0058 | Actor -23.4979 | noise 0.02564\n",
      "[DDPG] Epoch 59/120 | Critic 0.0054 | Actor -23.6790 | noise 0.02487\n",
      "[DDPG] Epoch 60/120 | Critic 0.0058 | Actor -23.5610 | noise 0.02412\n",
      "[DDPG] Epoch 61/120 | Critic 0.0051 | Actor -23.7822 | noise 0.02340\n",
      "[DDPG] Epoch 62/120 | Critic 0.0058 | Actor -23.6549 | noise 0.02270\n",
      "[DDPG] Epoch 63/120 | Critic 0.0056 | Actor -23.9344 | noise 0.02201\n",
      "[DDPG] Epoch 64/120 | Critic 0.0051 | Actor -23.9198 | noise 0.02135\n",
      "[DDPG] Epoch 65/120 | Critic 0.0049 | Actor -24.0146 | noise 0.02071\n",
      "[DDPG] Epoch 66/120 | Critic 0.0048 | Actor -23.8998 | noise 0.02009\n",
      "[DDPG] Epoch 67/120 | Critic 0.0050 | Actor -23.9650 | noise 0.01949\n",
      "[DDPG] Epoch 68/120 | Critic 0.0044 | Actor -23.8663 | noise 0.01890\n",
      "[DDPG] Epoch 69/120 | Critic 0.0053 | Actor -23.8468 | noise 0.01834\n",
      "[DDPG] Epoch 70/120 | Critic 0.0048 | Actor -23.8432 | noise 0.01779\n",
      "[DDPG] Epoch 71/120 | Critic 0.0055 | Actor -24.0866 | noise 0.01725\n",
      "[DDPG] Epoch 72/120 | Critic 0.0050 | Actor -24.1069 | noise 0.01674\n",
      "[DDPG] Epoch 73/120 | Critic 0.0051 | Actor -24.1920 | noise 0.01623\n",
      "[DDPG] Epoch 74/120 | Critic 0.0047 | Actor -23.9883 | noise 0.01575\n",
      "[DDPG] Epoch 75/120 | Critic 0.0047 | Actor -24.0171 | noise 0.01527\n",
      "[DDPG] Epoch 76/120 | Critic 0.0044 | Actor -24.0429 | noise 0.01482\n",
      "[DDPG] Epoch 77/120 | Critic 0.0041 | Actor -24.0499 | noise 0.01437\n",
      "[DDPG] Epoch 78/120 | Critic 0.0042 | Actor -23.9682 | noise 0.01394\n",
      "[DDPG] Epoch 79/120 | Critic 0.0042 | Actor -23.8561 | noise 0.01352\n",
      "[DDPG] Epoch 80/120 | Critic 0.0040 | Actor -23.8544 | noise 0.01312\n",
      "[DDPG] Epoch 81/120 | Critic 0.0040 | Actor -24.1119 | noise 0.01272\n",
      "[DDPG] Epoch 82/120 | Critic 0.0039 | Actor -24.2766 | noise 0.01234\n",
      "[DDPG] Epoch 83/120 | Critic 0.0040 | Actor -24.5599 | noise 0.01197\n",
      "[DDPG] Epoch 84/120 | Critic 0.0037 | Actor -24.5768 | noise 0.01161\n",
      "[DDPG] Epoch 85/120 | Critic 0.0038 | Actor -24.4797 | noise 0.01126\n",
      "[DDPG] Epoch 86/120 | Critic 0.0038 | Actor -24.2726 | noise 0.01093\n",
      "[DDPG] Epoch 87/120 | Critic 0.0040 | Actor -24.5528 | noise 0.01060\n",
      "[DDPG] Epoch 88/120 | Critic 0.0037 | Actor -24.9336 | noise 0.01028\n",
      "[DDPG] Epoch 89/120 | Critic 0.0036 | Actor -24.9486 | noise 0.00997\n",
      "[DDPG] Epoch 90/120 | Critic 0.0035 | Actor -25.1072 | noise 0.00967\n",
      "[DDPG] Epoch 91/120 | Critic 0.0034 | Actor -25.3452 | noise 0.00938\n",
      "[DDPG] Epoch 92/120 | Critic 0.0036 | Actor -25.2298 | noise 0.00910\n",
      "[DDPG] Epoch 93/120 | Critic 0.0034 | Actor -25.3486 | noise 0.00883\n",
      "[DDPG] Epoch 94/120 | Critic 0.0034 | Actor -25.4103 | noise 0.00856\n",
      "[DDPG] Epoch 95/120 | Critic 0.0032 | Actor -25.3060 | noise 0.00831\n",
      "[DDPG] Epoch 96/120 | Critic 0.0033 | Actor -25.4009 | noise 0.00806\n",
      "[DDPG] Epoch 97/120 | Critic 0.0037 | Actor -25.5309 | noise 0.00782\n",
      "[DDPG] Epoch 98/120 | Critic 0.0032 | Actor -25.7187 | noise 0.00758\n",
      "[DDPG] Epoch 99/120 | Critic 0.0032 | Actor -25.8784 | noise 0.00735\n",
      "[DDPG] Epoch 100/120 | Critic 0.0034 | Actor -25.7224 | noise 0.00713\n",
      "[DDPG] Epoch 101/120 | Critic 0.0034 | Actor -25.9466 | noise 0.00692\n",
      "[DDPG] Epoch 102/120 | Critic 0.0033 | Actor -26.0281 | noise 0.00671\n",
      "[DDPG] Epoch 103/120 | Critic 0.0033 | Actor -26.0046 | noise 0.00651\n",
      "[DDPG] Epoch 104/120 | Critic 0.0030 | Actor -25.7695 | noise 0.00631\n",
      "[DDPG] Epoch 105/120 | Critic 0.0031 | Actor -25.9778 | noise 0.00613\n",
      "[DDPG] Epoch 106/120 | Critic 0.0031 | Actor -26.0316 | noise 0.00594\n",
      "[DDPG] Epoch 107/120 | Critic 0.0034 | Actor -25.9468 | noise 0.00576\n",
      "[DDPG] Epoch 108/120 | Critic 0.0032 | Actor -26.0880 | noise 0.00559\n",
      "[DDPG] Epoch 109/120 | Critic 0.0032 | Actor -25.7908 | noise 0.00542\n",
      "[DDPG] Epoch 110/120 | Critic 0.0033 | Actor -25.9962 | noise 0.00526\n",
      "[DDPG] Epoch 111/120 | Critic 0.0031 | Actor -25.8999 | noise 0.00510\n",
      "[DDPG] Epoch 112/120 | Critic 0.0032 | Actor -25.9435 | noise 0.00495\n",
      "[DDPG] Epoch 113/120 | Critic 0.0029 | Actor -26.0917 | noise 0.00480\n",
      "[DDPG] Epoch 114/120 | Critic 0.0030 | Actor -25.7994 | noise 0.00466\n",
      "[DDPG] Epoch 115/120 | Critic 0.0031 | Actor -26.1713 | noise 0.00452\n",
      "[DDPG] Epoch 116/120 | Critic 0.0030 | Actor -25.9979 | noise 0.00438\n",
      "[DDPG] Epoch 117/120 | Critic 0.0030 | Actor -26.0851 | noise 0.00425\n",
      "[DDPG] Epoch 118/120 | Critic 0.0028 | Actor -26.0914 | noise 0.00412\n",
      "[DDPG] Epoch 119/120 | Critic 0.0031 | Actor -25.9457 | noise 0.00400\n",
      "[DDPG] Epoch 120/120 | Critic 0.0030 | Actor -26.1706 | noise 0.00388\n",
      "Fetching VNINDEX for benchmark...\n",
      "Fetching data, it may take a while. Please wait...\n",
      "✅ Done Block 10 with extra logs\n"
     ]
    }
   ],
   "source": [
    "# Block 10 — Cluster DDPG + Execution Lag + Turnover Cost (Final, realistic backtest with extra logs)\n",
    "import os, gc, json, csv, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from FiinQuantX import FiinSession\n",
    "\n",
    "# ====== I/O paths ======\n",
    "DATA_DIR   = \"./tensors/\"\n",
    "SIG_DIR    = \"./signals/\"\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "MODEL_DIR  = \"./models/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ====== Config ======\n",
    "INIT_CAPITAL    = 10_000.0\n",
    "BENCHMARK_TKR   = \"VNINDEX\"\n",
    "EXECUTION_LAG   = 2\n",
    "COST_BPS        = 20\n",
    "STATE_LKBK      = 20\n",
    "MIN_NAMES_PER_CLUSTER = 1\n",
    "MAX_HOLD_TICKERS = 10\n",
    "MAX_HOLD_DAYS = 15\n",
    "SEED = 42\n",
    "\n",
    "# DDPG hyper\n",
    "EPOCHS       = 120\n",
    "BATCH_SIZE   = 128\n",
    "LR_ACTOR     = 1e-4\n",
    "LR_CRITIC    = 1e-4\n",
    "GAMMA        = 0.94\n",
    "TAU          = 0.01\n",
    "NOISE_STD    = 0.15\n",
    "NOISE_DECAY  = 0.97\n",
    "HIDDEN       = 128\n",
    "BUFFER_MAX   = 100_000\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ====== Load artifacts ======\n",
    "sig_file = os.path.join(SIG_DIR, \"a3c_signals_infer.csv\")\n",
    "signals = pd.read_csv(sig_file)\n",
    "signals[\"date\"] = pd.to_datetime(signals[\"date\"])\n",
    "\n",
    "# df_backtest must exist (from Block 7.5)\n",
    "df_px = df_backtest.rename(columns={\"timestamp\":\"date\"}).copy()\n",
    "df_px[\"date\"] = pd.to_datetime(df_px[\"date\"])\n",
    "\n",
    "realized = df_px[[\"ticker\",\"date\",\"exit_date\",\"exit_price\",\"realized_return\",\n",
    "                  \"entry_regime\",\"tp_level\",\"sl_level\",\"exit_type\",\"horizon_days\",\"fee\"]].copy()\n",
    "realized = realized.set_index([\"ticker\",\"date\"]).sort_index()\n",
    "\n",
    "with open(os.path.join(DATA_DIR, \"tensor_index.json\"), \"r\") as f:\n",
    "    tensor_index = json.load(f)\n",
    "\n",
    "ticker2cluster = {}\n",
    "for meta in tensor_index:\n",
    "    for tk in meta[\"tickers\"]:\n",
    "        ticker2cluster.setdefault(tk, meta[\"cluster\"])\n",
    "ticker2cluster = pd.Series(ticker2cluster)\n",
    "\n",
    "px_wide = df_px.pivot(index=\"date\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "daily_ret = px_wide.pct_change().fillna(0.0)\n",
    "\n",
    "TRAIN_START = pd.Timestamp(\"2023-01-01\")\n",
    "TRAIN_END   = pd.Timestamp(\"2024-12-31\")\n",
    "TEST_START  = pd.Timestamp(\"2025-01-01\")\n",
    "TEST_END    = daily_ret.index.max()\n",
    "\n",
    "sig_wide_raw = signals.pivot_table(index=\"date\", columns=\"ticker\", values=\"signal\", aggfunc=\"last\").sort_index()\n",
    "idx_all = daily_ret.index.union(sig_wide_raw.index)\n",
    "daily_ret = daily_ret.reindex(idx_all).fillna(0.0)\n",
    "sig_wide = sig_wide_raw.reindex(idx_all).fillna(0.0)\n",
    "sig_wide_lag = sig_wide.shift(EXECUTION_LAG)\n",
    "\n",
    "tickers = sorted([t for t in daily_ret.columns if t in ticker2cluster.index])\n",
    "daily_ret = daily_ret[tickers].astype(\"float32\")\n",
    "sig_wide_lag = sig_wide_lag[tickers].astype(\"float32\")\n",
    "cluster_of = ticker2cluster.loc[tickers]\n",
    "clusters = sorted(cluster_of.unique().tolist())\n",
    "cluster_members = {c: cluster_of[cluster_of==c].index.tolist() for c in clusters}\n",
    "C = len(clusters)\n",
    "\n",
    "# ====== Build states ======\n",
    "def build_state_arrays_realized(ret_w, sig_lag, realized_idx, start, end, K=STATE_LKBK):\n",
    "    dates_all = ret_w.loc[start:end].index\n",
    "    act_cols, ret_cols, ACTIVE_masks = [], [], {}\n",
    "    for c in clusters:\n",
    "        tks = cluster_members[c]\n",
    "        if not tks:\n",
    "            act_c = pd.Series(0.0, index=dates_all, name=c)\n",
    "            ret_c = pd.Series(0.0, index=dates_all, name=c)\n",
    "            ACTIVE_masks[c] = pd.DataFrame(0.0, index=dates_all, columns=tks)\n",
    "        else:\n",
    "            S_c = sig_lag[tks].reindex(dates_all).fillna(0.0)\n",
    "            active_mask = (S_c > 0).astype(\"float32\")\n",
    "            ACTIVE_masks[c] = active_mask\n",
    "            rr = pd.DataFrame(index=dates_all, columns=tks, dtype=\"float32\")\n",
    "            for tk in tks:\n",
    "                vals = []\n",
    "                for d in dates_all:\n",
    "                    try:\n",
    "                        vals.append(realized_idx.loc[(tk, d), \"realized_return\"])\n",
    "                    except Exception:\n",
    "                        vals.append(np.nan)\n",
    "                rr[tk] = pd.Series(vals, index=dates_all, dtype=\"float32\")\n",
    "            denom = active_mask.sum(axis=1).replace(0, np.nan)\n",
    "            w = active_mask.div(denom, axis=0).fillna(0.0)\n",
    "            ret_c = (rr.fillna(0.0) * w).sum(axis=1).astype(\"float32\")\n",
    "            act_c = active_mask.mean(axis=1).astype(\"float32\")\n",
    "        act_cols.append(act_c.rename(c)); ret_cols.append(ret_c.rename(c))\n",
    "\n",
    "    act_df = pd.concat(act_cols, axis=1).astype(\"float32\")\n",
    "    cret_df = pd.concat(ret_cols, axis=1).astype(\"float32\")\n",
    "    Tfull = len(dates_all)\n",
    "    A_stack = np.zeros((Tfull, C, K), dtype=\"float32\")\n",
    "    R_stack = np.zeros((Tfull, C, K), dtype=\"float32\")\n",
    "    for k in range(K):\n",
    "        A_stack[:, :, k] = act_df.shift(k).fillna(0.0).values\n",
    "        R_stack[:, :, k] = cret_df.shift(k).fillna(0.0).values\n",
    "    valid_idx = np.arange(Tfull) >= (K - 1)\n",
    "    dates2 = dates_all[valid_idx]\n",
    "    A3, R3 = A_stack[valid_idx], R_stack[valid_idx]\n",
    "    S_flat = np.concatenate([A3.reshape(len(dates2), -1), R3.reshape(len(dates2), -1)], axis=1).astype(\"float32\")\n",
    "\n",
    "    regimes = []\n",
    "    for d in dates2:\n",
    "        try:\n",
    "            sample = realized_idx.xs(d, level=\"date\")\n",
    "            rvec = np.array([\n",
    "                sample[\"entry_regime\"].eq(\"bear\").mean() if \"entry_regime\" in sample.columns else 0.0,\n",
    "                sample[\"entry_regime\"].eq(\"bull\").mean() if \"entry_regime\" in sample.columns else 0.0,\n",
    "                sample[\"entry_regime\"].eq(\"sideway\").mean() if \"entry_regime\" in sample.columns else 0.0\n",
    "            ], dtype=\"float32\")\n",
    "        except Exception:\n",
    "            rvec = np.array([0.0,0.0,0.0], dtype=\"float32\")\n",
    "        regimes.append(rvec)\n",
    "    regimes = np.stack(regimes, axis=0)\n",
    "\n",
    "    S_mat = np.concatenate([S_flat, regimes], axis=1)\n",
    "    R_mat = cret_df.loc[dates2].values.astype(\"float32\")\n",
    "    ACTIVE_masks = {c: ACTIVE_masks[c].loc[dates2] for c in clusters}\n",
    "    return S_mat, R_mat, dates2, ACTIVE_masks\n",
    "\n",
    "S_train, R_train, d_train, ACTIVE_train = build_state_arrays_realized(daily_ret, sig_wide_lag, realized, TRAIN_START, TRAIN_END)\n",
    "S_test,  R_test,  d_test,  ACTIVE_test  = build_state_arrays_realized(daily_ret, sig_wide_lag, realized, TEST_START, TEST_END)\n",
    "\n",
    "# ====== DDPG Actor/Critic ======\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, a_dim)\n",
    "        )\n",
    "    def forward(self, s): return torch.softmax(self.net(s), dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden=HIDDEN):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(s_dim + a_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, s,a): return self.net(torch.cat([s,a], dim=-1))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, maxlen=BUFFER_MAX): self.maxlen=maxlen; self.buf=[]\n",
    "    def push(self,s,a,r,s2):\n",
    "        if len(self.buf)>=self.maxlen: self.buf.pop(0)\n",
    "        self.buf.append((s,a,r,s2))\n",
    "    def sample(self,bs):\n",
    "        n=min(bs,len(self.buf)); idx=np.random.choice(len(self.buf),n,replace=False)\n",
    "        s,a,r,s2=zip(*[self.buf[i] for i in idx])\n",
    "        return np.array(s,np.float32),np.array(a,np.float32),np.array(r,np.float32).reshape(-1,1),np.array(s2,np.float32)\n",
    "\n",
    "def soft_update(src,tgt,tau):\n",
    "    with torch.no_grad():\n",
    "        for p,tp in zip(src.parameters(),tgt.parameters()):\n",
    "            tp.data.mul_(1-tau); tp.data.add_(tau*p.data)\n",
    "\n",
    "def port_reward_cluster(w_cluster,r_cluster,prev_w_cluster=None,cost_bps=COST_BPS,train_penalty=True):\n",
    "    gross=float(np.dot(w_cluster,r_cluster))\n",
    "    turnover=0.0\n",
    "    if prev_w_cluster is not None:\n",
    "        turnover=float(np.sum(np.abs(w_cluster-prev_w_cluster)))\n",
    "    fee=(cost_bps/1e-4)*0.0 if False else (cost_bps/1e4)*turnover  # explicit\n",
    "    return gross - (fee if train_penalty else 0.0)\n",
    "\n",
    "s_dim=S_train.shape[1]; a_dim=C\n",
    "actor=Actor(s_dim,a_dim).to(DEVICE); critic=Critic(s_dim,a_dim).to(DEVICE)\n",
    "t_actor=Actor(s_dim,a_dim).to(DEVICE); t_actor.load_state_dict(actor.state_dict())\n",
    "t_critic=Critic(s_dim,a_dim).to(DEVICE); t_critic.load_state_dict(critic.state_dict())\n",
    "optA=optim.Adam(actor.parameters(),lr=LR_ACTOR); optC=optim.Adam(critic.parameters(),lr=LR_CRITIC)\n",
    "mse=nn.MSELoss(); buf=ReplayBuffer(BUFFER_MAX)\n",
    "\n",
    "# ====== Training ======\n",
    "noise_std=NOISE_STD; min_buffer=max(500,BATCH_SIZE*5)\n",
    "for ep in range(EPOCHS):\n",
    "    prev_w=None; c_loss=a_loss=0.0\n",
    "    for t in range(len(S_train)-1):\n",
    "        s=torch.from_numpy(S_train[t]).to(DEVICE).unsqueeze(0)\n",
    "        s2=torch.from_numpy(S_train[t+1]).to(DEVICE).unsqueeze(0)\n",
    "        with torch.no_grad(): w=actor(s).cpu().numpy()[0]\n",
    "        logits=np.log(np.clip(w,1e-9,1.0))+np.random.normal(0,noise_std,size=a_dim)\n",
    "        w_e=np.exp(logits); w_e=w_e/(w_e.sum()+1e-12)\n",
    "        r=port_reward_cluster(w_e,R_train[t],prev_w_cluster=prev_w,train_penalty=True); prev_w=w_e.copy()\n",
    "        buf.push(S_train[t],w_e,r,S_train[t+1])\n",
    "        if len(buf.buf)>=min_buffer:\n",
    "            sb,ab,rb,s2b=buf.sample(BATCH_SIZE)\n",
    "            sb=torch.from_numpy(sb).to(DEVICE); ab=torch.from_numpy(ab).to(DEVICE)\n",
    "            rb=torch.from_numpy(rb).to(DEVICE); s2b=torch.from_numpy(s2b).to(DEVICE)\n",
    "            with torch.no_grad(): a2=t_actor(s2b); q2=t_critic(s2b,a2); y=rb+GAMMA*q2\n",
    "            q=critic(sb,ab); lc=mse(q,y); optC.zero_grad(); lc.backward(); optC.step()\n",
    "            ap=actor(sb); la=-critic(sb,ap).mean(); optA.zero_grad(); la.backward(); optA.step()\n",
    "            soft_update(actor,t_actor,TAU); soft_update(critic,t_critic,TAU)\n",
    "            c_loss+=float(lc.item()); a_loss+=float(la.item())\n",
    "    noise_std*=NOISE_DECAY\n",
    "    print(f\"[DDPG] Epoch {ep+1}/{EPOCHS} | Critic {c_loss:.4f} | Actor {a_loss:.4f} | noise {noise_std:.5f}\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(actor.state_dict(), os.path.join(MODEL_DIR,\"ddpg_actor.pt\"))\n",
    "\n",
    "# ====== Backtest (with extra logs) ======\n",
    "dates=pd.DatetimeIndex(d_test).sort_values()\n",
    "capital=INIT_CAPITAL; nav=capital\n",
    "portfolio_value=pd.Series(index=dates,dtype=\"float64\"); portfolio_value.iloc[0]=capital\n",
    "trade_log=[] \n",
    "regime_daily_list=[] \n",
    "holdings_lastday_rows=[]\n",
    "cw_path=os.path.join(OUTPUT_DIR,\"cluster_weights_test.csv\")\n",
    "with open(cw_path,\"w\",newline=\"\") as f:\n",
    "    cw=csv.writer(f)\n",
    "    cw.writerow([\"date\"] + [f\"cluster_{c}\" for c in clusters] + [\"cash_buffer\"])\n",
    "\n",
    "weights_df=pd.DataFrame(0.0,index=dates,columns=tickers,dtype=\"float32\")\n",
    "holdings={}; prev_w_ticker=pd.Series(0.0,index=tickers,dtype=\"float32\")\n",
    "\n",
    "# ====== Backtest main loop (FULL with logging) ======\n",
    "for i, dt in enumerate(dates):\n",
    "    # prepare state for actor: use previous state to avoid leakage\n",
    "    s_idx = i-1 if i>0 else i\n",
    "    s = torch.from_numpy(S_test[s_idx].astype(\"float32\")).to(DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        w_c = actor(s).cpu().numpy()[0].astype(\"float32\")\n",
    "    w_c = np.clip(w_c, 0.0, 1.0)\n",
    "    if w_c.sum() > 0:\n",
    "        w_c = w_c / w_c.sum()\n",
    "    else:\n",
    "        w_c = np.ones_like(w_c) / len(w_c)\n",
    "\n",
    "    # --- Determine regime majority & cash buffer (compute BEFORE writing cluster weights row) ---\n",
    "    cash_buffer = 0.15  # default\n",
    "    majority_regime = None\n",
    "    try:\n",
    "        todays = df_px[df_px[\"date\"] == dt]\n",
    "        if \"entry_regime\" in todays.columns:\n",
    "            mode = todays[\"entry_regime\"].mode()\n",
    "            if len(mode) > 0:\n",
    "                majority_regime = mode.iloc[0]\n",
    "                if majority_regime == \"bull\":   cash_buffer = 0.02\n",
    "                elif majority_regime == \"bear\": cash_buffer = 0.35\n",
    "                else:                           cash_buffer = 0.15\n",
    "    except Exception:\n",
    "        majority_regime = None\n",
    "\n",
    "    # save cluster weights (include cash_buffer)\n",
    "    with open(cw_path, \"a\", newline=\"\") as f:\n",
    "        cw = csv.writer(f)\n",
    "        cw.writerow([dt.strftime(\"%Y-%m-%d\")] + [float(x) for x in w_c] + [float(cash_buffer)])\n",
    "\n",
    "    # --- Map cluster weights down to tickers ---\n",
    "    score = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "    # safe access to active row\n",
    "    if dt in sig_wide_lag.index:\n",
    "        active_matrix_row = sig_wide_lag.loc[dt]\n",
    "    else:\n",
    "        # empty series aligned to tickers\n",
    "        active_matrix_row = pd.Series(0.0, index=tickers)\n",
    "\n",
    "    for j, c in enumerate(clusters):\n",
    "        members = cluster_members[c]\n",
    "        if not members: continue\n",
    "        try:\n",
    "            act_row = active_matrix_row[members]\n",
    "        except Exception:\n",
    "            act_row = pd.Series(0.0, index=members)\n",
    "        valid = [tk for tk in members if (tk in act_row.index and act_row[tk] > 0)]\n",
    "        if len(valid) >= MIN_NAMES_PER_CLUSTER:\n",
    "            per_tk = float(w_c[j] / len(valid))\n",
    "            score.loc[valid] += per_tk\n",
    "\n",
    "    # --- Select top-k tickers dynamic ---\n",
    "    candidates = score[score > 0].sort_values(ascending=False)\n",
    "    held_now = list(holdings.keys())\n",
    "    combined_candidates = pd.concat([candidates, pd.Series(0.0, index=held_now)]).groupby(level=0).first()\n",
    "    topk = combined_candidates.sort_values(ascending=False).head(MAX_HOLD_TICKERS).index.tolist()\n",
    "\n",
    "    target_scores = score.loc[topk].fillna(0.0)\n",
    "    total_score = target_scores.sum()\n",
    "    investable = 1.0 - cash_buffer\n",
    "    if total_score <= 1e-12:\n",
    "        target_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "    else:\n",
    "        alloc = (target_scores / total_score) * investable\n",
    "        target_w_ticker = pd.Series(0.0, index=tickers, dtype=\"float32\")\n",
    "        target_w_ticker.loc[alloc.index] = alloc.values\n",
    "\n",
    "    # --- Turnover cost (apply immediately as reduction of NAV) ---\n",
    "    turnover = float(np.sum(np.abs(target_w_ticker.values - prev_w_ticker.values)))\n",
    "    fee = (COST_BPS / 1e4) * turnover\n",
    "    nav = nav * (1.0 - fee)\n",
    "\n",
    "    # --- Detect new open / close ---\n",
    "    new_open = [tk for tk in target_w_ticker.index if (prev_w_ticker.loc[tk] == 0.0 and target_w_ticker.loc[tk] > 0.0)]\n",
    "    closed  = [tk for tk in prev_w_ticker.index if (prev_w_ticker.loc[tk] > 0.0 and target_w_ticker.loc[tk] == 0.0)]\n",
    "\n",
    "    # --- Forced exits (planned exit or max_hold_days) ---\n",
    "    to_force_exit = []\n",
    "    for tk, info in list(holdings.items()):\n",
    "        planned_exit = info.get(\"planned_exit_date\", pd.NaT)\n",
    "        entry_dt = info[\"entry_date\"]\n",
    "        if pd.notna(planned_exit) and planned_exit <= dt:\n",
    "            to_force_exit.append(tk); continue\n",
    "        hold_days = (dt - entry_dt).days\n",
    "        if isinstance(hold_days, (int, np.integer)) and hold_days >= MAX_HOLD_DAYS:\n",
    "            to_force_exit.append(tk)\n",
    "\n",
    "    for tk in to_force_exit:\n",
    "        info = holdings.pop(tk)\n",
    "        entry_dt = info[\"entry_date\"]\n",
    "        try:\n",
    "            rec = realized.loc[(tk, entry_dt)]\n",
    "            exit_price = rec[\"exit_price\"]; exit_date = rec[\"exit_date\"]\n",
    "            rret = rec[\"realized_return\"]; etype = rec[\"exit_type\"]; fee_tr = rec.get(\"fee\", (COST_BPS/1e4)*abs(prev_w_ticker.loc[tk]))\n",
    "        except Exception:\n",
    "            exit_price = px_wide.loc[dt, tk] if (dt in px_wide.index and tk in px_wide.columns) else np.nan\n",
    "            exit_date = dt\n",
    "            rret = np.log(exit_price / info[\"entry_price\"]) if pd.notna(exit_price) and info[\"entry_price\"]>0 else np.nan\n",
    "            etype = \"forced\"; fee_tr = (COST_BPS/1e4) * abs(prev_w_ticker.loc[tk])\n",
    "        trade_log.append({\n",
    "            \"entry_date\": entry_dt, \"exit_date\": exit_date, \"ticker\": tk,\n",
    "            \"entry_price\": info[\"entry_price\"], \"exit_price\": exit_price,\n",
    "            \"entry_regime\": info.get(\"entry_regime\", None),\n",
    "            \"exit_type\": etype, \"tp_level\": info.get(\"tp_level\", np.nan),\n",
    "            \"sl_level\": info.get(\"sl_level\", np.nan), \"realized_return\": rret,\n",
    "            \"holding_days\": (exit_date - entry_dt).days if pd.notna(exit_date) else np.nan,\n",
    "            \"fee\": fee_tr\n",
    "        })\n",
    "        prev_w = prev_w_ticker.loc[tk]\n",
    "        if not pd.isna(rret):\n",
    "            nav = nav * (1.0 + prev_w * rret)\n",
    "        prev_w_ticker.loc[tk] = 0.0\n",
    "\n",
    "    # --- Open new positions ---\n",
    "    for tk in new_open:\n",
    "        if len(holdings) >= MAX_HOLD_TICKERS: continue\n",
    "        entry_price = px_wide.loc[dt, tk] if (dt in px_wide.index and tk in px_wide.columns) else np.nan\n",
    "        planned_exit_date = np.nan; planned_exit_price = np.nan; tp_level = np.nan; sl_level = np.nan; entry_regime = np.nan\n",
    "        try:\n",
    "            rec = realized.loc[(tk, dt)]\n",
    "            planned_exit_date = rec[\"exit_date\"]; planned_exit_price = rec[\"exit_price\"]\n",
    "            tp_level = rec.get(\"tp_level\", np.nan); sl_level = rec.get(\"sl_level\", np.nan)\n",
    "            entry_regime = rec.get(\"entry_regime\", np.nan)\n",
    "        except Exception: pass\n",
    "        holdings[tk] = {\n",
    "            \"entry_date\": dt, \"entry_price\": entry_price,\n",
    "            \"planned_exit_date\": planned_exit_date, \"planned_exit_price\": planned_exit_price,\n",
    "            \"entry_regime\": entry_regime, \"tp_level\": tp_level, \"sl_level\": sl_level\n",
    "        }\n",
    "\n",
    "    # --- Rebalance and daily PnL (mark-to-market using daily_ret) ---\n",
    "    current_weights = target_w_ticker.copy()\n",
    "    if current_weights.sum() > 1.0:\n",
    "        current_weights = current_weights / current_weights.sum()\n",
    "    # safe get r_vec\n",
    "    if dt in daily_ret.index:\n",
    "        r_vec = daily_ret.loc[dt].reindex(current_weights.index).fillna(0.0).values\n",
    "    else:\n",
    "        r_vec = np.zeros(len(current_weights), dtype=float)\n",
    "    port_daily_ret = float(np.dot(current_weights.values, r_vec))\n",
    "    nav = nav * (1.0 + port_daily_ret)\n",
    "\n",
    "    weights_df.loc[dt] = current_weights\n",
    "    prev_w_ticker = current_weights.copy()\n",
    "\n",
    "    # --- Planned exits today (finalize trade log and remove holdings) ---\n",
    "    for tk in list(holdings.keys()):\n",
    "        info = holdings[tk]\n",
    "        planned_exit = info.get(\"planned_exit_date\", pd.NaT)\n",
    "        if pd.notna(planned_exit) and planned_exit == dt:\n",
    "            try:\n",
    "                rec = realized.loc[(tk, info[\"entry_date\"])]\n",
    "                exit_price = rec[\"exit_price\"]; exit_date = rec[\"exit_date\"]; rret = rec[\"realized_return\"]\n",
    "                etype = rec.get(\"exit_type\", \"exit\"); fee_tr = rec.get(\"fee\", (COST_BPS/1e4)*abs(prev_w_ticker.loc[tk]))\n",
    "            except Exception:\n",
    "                exit_price = px_wide.loc[dt, tk] if (dt in px_wide.index and tk in px_wide.columns) else np.nan\n",
    "                exit_date = dt\n",
    "                rret = np.log(exit_price / info[\"entry_price\"]) if pd.notna(exit_price) and info[\"entry_price\"]>0 else np.nan\n",
    "                etype = \"exit\"; fee_tr = (COST_BPS/1e4)*abs(prev_w_ticker.loc[tk])\n",
    "            trade_log.append({\n",
    "                \"entry_date\": info[\"entry_date\"], \"exit_date\": exit_date, \"ticker\": tk,\n",
    "                \"entry_price\": info[\"entry_price\"], \"exit_price\": exit_price,\n",
    "                \"entry_regime\": info.get(\"entry_regime\", None), \"exit_type\": etype,\n",
    "                \"tp_level\": info.get(\"tp_level\", np.nan), \"sl_level\": info.get(\"sl_level\", np.nan),\n",
    "                \"realized_return\": rret, \"holding_days\": (exit_date - info[\"entry_date\"]).days if pd.notna(exit_date) else np.nan,\n",
    "                \"fee\": fee_tr\n",
    "            })\n",
    "            prev_w_ticker.loc[tk] = 0.0\n",
    "            holdings.pop(tk, None)\n",
    "\n",
    "    # --- Save NAV + Logs daily ---\n",
    "    portfolio_value.iloc[i] = nav\n",
    "    regime_daily_list.append({\n",
    "        \"date\": dt, \"regime\": majority_regime, \"cash_buffer\": cash_buffer, \"nav\": nav\n",
    "    })\n",
    "    for tk, info in holdings.items():\n",
    "        holdings_lastday_rows.append({\n",
    "            \"snapshot_date\": dt, \"ticker\": tk,\n",
    "            \"entry_date\": info[\"entry_date\"], \"entry_price\": info[\"entry_price\"],\n",
    "            \"tp_level\": info.get(\"tp_level\", np.nan), \"sl_level\": info.get(\"sl_level\", np.nan),\n",
    "            \"entry_regime\": info.get(\"entry_regime\", None),\n",
    "            \"weight\": float(current_weights.loc[tk]) if tk in current_weights.index else 0.0,\n",
    "            \"nav\": nav\n",
    "        })\n",
    "\n",
    "    if (i % 50) == 0:\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ====== Save outputs ======\n",
    "portfolio_value.to_frame(\"portfolio_value\").to_csv(os.path.join(OUTPUT_DIR,\"portfolio_value_test.csv\"))\n",
    "weights_df.to_csv(os.path.join(OUTPUT_DIR,\"weights_by_ticker.csv\"))\n",
    "pd.DataFrame(trade_log).to_csv(os.path.join(OUTPUT_DIR,\"trade_log.csv\"),index=False)\n",
    "pd.DataFrame(regime_daily_list).to_csv(os.path.join(OUTPUT_DIR,\"regime_daily.csv\"),index=False)\n",
    "pd.DataFrame(holdings_lastday_rows).to_csv(os.path.join(OUTPUT_DIR,\"holdings_lastday.csv\"),index=False)\n",
    "\n",
    "# ==== Benchmark ====\n",
    "print(\"Fetching VNINDEX for benchmark...\")\n",
    "client=FiinSession(username=\"DSTC_18@fiinquant.vn\",password=\"Fiinquant0606\").login()\n",
    "bench=client.Fetch_Trading_Data(realtime=False,tickers=BENCHMARK_TKR,fields=['close'],\n",
    "    adjusted=True,by=\"1d\",from_date=str(dates.min().date())).get_data()\n",
    "bench[\"date\"]=pd.to_datetime(bench[\"timestamp\"])\n",
    "bench=bench.set_index(\"date\")[\"close\"].sort_index().reindex(dates).ffill().bfill()\n",
    "bench_ret=bench.pct_change().fillna(0.0)\n",
    "benchmark_value=(1+bench_ret).cumprod()*INIT_CAPITAL\n",
    "benchmark_value.to_frame(\"benchmark_value\").to_csv(os.path.join(OUTPUT_DIR,\"benchmark_value_test.csv\"))\n",
    "\n",
    "torch.save(actor.state_dict(), os.path.join(MODEL_DIR,\"ddpg_actor.pt\"))\n",
    "print(\"✅ Done Block 10 with extra logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7dc72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f49e0e1c",
   "metadata": {},
   "source": [
    "**BLOCK 10.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27593e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] OUTPUT_DIR = ./backtest_ddpg/\n",
      "[Info] EXECUTION_LAG = 2 days (settlement)\n",
      "[Info] Will generate snapshots for 237 days: 2025-02-05 -> 2025-09-29\n",
      "[Progress] Generated 50/237 days - last date: 2025-03-26\n",
      "[Progress] Generated 100/237 days - last date: 2025-05-15\n",
      "[Progress] Generated 150/237 days - last date: 2025-07-04\n",
      "[Progress] Generated 200/237 days - last date: 2025-08-23\n",
      "[Progress] Generated 237/237 days - last date: 2025-09-29\n",
      "[Saved] trade_log_detailed -> ./backtest_ddpg/trade_log_detailed.csv\n",
      "=== Done Block 10.5 ===\n",
      "Snapshots created: 237 (files positions_snapshot_YYYY-MM-DD.csv)\n",
      "Signals created:   237 (files signals_today_YYYY-MM-DD.csv)\n",
      "Payloads created:  237 (files telegram_payload_YYYY-MM-DD.json)\n",
      "Trade log detailed: ./backtest_ddpg/trade_log_detailed.csv\n",
      "Example snapshot file (first): ./backtest_ddpg/positions_snapshot_2025-02-05.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 10.5 — Save enriched artifacts for Telegram reporting (full, fixed snapshot carry-over)\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------- CONFIG & PATHS --------\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CONFIG_PATH = \"config.json\"\n",
    "# default execution lag (days) if not specified in config\n",
    "DEFAULT_EXECUTION_LAG = 2\n",
    "\n",
    "# --------- Load config.json (optional) ----------\n",
    "cfg = {}\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    try:\n",
    "        with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot parse config.json:\", e)\n",
    "EXECUTION_LAG = int(cfg.get(\"pipeline\", {}).get(\"execution_lag\", DEFAULT_EXECUTION_LAG))\n",
    "\n",
    "print(f\"[Info] OUTPUT_DIR = {OUTPUT_DIR}\")\n",
    "print(f\"[Info] EXECUTION_LAG = {EXECUTION_LAG} days (settlement)\")\n",
    "\n",
    "# --------- Helper functions ----------\n",
    "def safe_parse_dates(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def choose_column(df, candidates, default=None):\n",
    "    \"\"\"Return first existing column name from candidates or default.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return default\n",
    "\n",
    "def ensure_col(df, col, default_val=np.nan):\n",
    "    if col not in df.columns:\n",
    "        df[col] = default_val\n",
    "    return df\n",
    "\n",
    "def get_price_on_or_before(px_wide, price_map, ticker, dt):\n",
    "    \"\"\"\n",
    "    Try price_map[(ticker, dt)] exact; else fallback to px_wide[ticker].loc[:dt].iloc[-1] if available.\n",
    "    Returns np.nan if cannot find.\n",
    "    \"\"\"\n",
    "    if pd.isna(dt):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # exact lookup first (if df_backtest-based price_map exists)\n",
    "        return float(price_map.loc[(ticker, dt)])\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if ticker in px_wide.columns:\n",
    "            ser = px_wide[ticker].loc[:dt].dropna()\n",
    "            if len(ser) > 0:\n",
    "                return float(ser.iloc[-1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "# --------- Load trade_log (expected from Block 10) ----------\n",
    "trade_log_path = os.path.join(OUTPUT_DIR, \"trade_log.csv\")\n",
    "if not os.path.exists(trade_log_path):\n",
    "    raise FileNotFoundError(f\"trade_log.csv not found in {OUTPUT_DIR}. Please run Block 10 first.\")\n",
    "\n",
    "trade_log = pd.read_csv(trade_log_path)\n",
    "# try to parse common date columns\n",
    "trade_log = safe_parse_dates(trade_log, [\"entry_date\", \"exit_date\", \"entry_dt\", \"exit_dt\", \"timestamp\"])\n",
    "\n",
    "# normalize column names: prefer 'entry_price', 'exit_price' etc.\n",
    "entry_price_col = choose_column(trade_log, [\"entry_price\", \"entry_trade_price\", \"entry_trade_px\", \"entry_px\", \"entry_trade_price\"])\n",
    "exit_price_col = choose_column(trade_log, [\"exit_price\", \"exit_trade_price\", \"exit_trade_px\", \"exit_px\", \"exit_trade_price\"])\n",
    "\n",
    "# ensure we have columns named 'entry_price' and 'exit_price' for downstream code\n",
    "if entry_price_col and entry_price_col != \"entry_price\":\n",
    "    trade_log = trade_log.rename(columns={entry_price_col: \"entry_price\"})\n",
    "elif \"entry_price\" not in trade_log.columns:\n",
    "    trade_log[\"entry_price\"] = np.nan\n",
    "\n",
    "if exit_price_col and exit_price_col != \"exit_price\":\n",
    "    trade_log = trade_log.rename(columns={exit_price_col: \"exit_price\"})\n",
    "elif \"exit_price\" not in trade_log.columns:\n",
    "    trade_log[\"exit_price\"] = np.nan\n",
    "\n",
    "# ensure entry_date/exit_date columns exist under these canonical names\n",
    "if \"entry_date\" not in trade_log.columns:\n",
    "    cand = choose_column(trade_log, [\"entry_dt\", \"timestamp\", \"date\"])\n",
    "    if cand:\n",
    "        trade_log = trade_log.rename(columns={cand: \"entry_date\"})\n",
    "trade_log = safe_parse_dates(trade_log, [\"entry_date\"])\n",
    "\n",
    "if \"exit_date\" not in trade_log.columns:\n",
    "    trade_log[\"exit_date\"] = pd.NaT\n",
    "else:\n",
    "    trade_log = safe_parse_dates(trade_log, [\"exit_date\"])\n",
    "\n",
    "# fill missing ticker column check\n",
    "if \"ticker\" not in trade_log.columns:\n",
    "    raise KeyError(\"trade_log.csv must contain a 'ticker' column\")\n",
    "\n",
    "# canonical: entry_price, exit_price, entry_date (datetime), exit_date (datetime), ticker\n",
    "trade_log = ensure_col(trade_log, \"entry_price\", np.nan)\n",
    "trade_log = ensure_col(trade_log, \"exit_price\", np.nan)\n",
    "\n",
    "# --------- Load df_backtest (prices) ----------\n",
    "df_backtest_path = os.path.join(OUTPUT_DIR, \"df_backtest.csv\")\n",
    "df_backtest = None\n",
    "if os.path.exists(df_backtest_path):\n",
    "    try:\n",
    "        df_backtest = pd.read_csv(df_backtest_path)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot read df_backtest.csv:\", e)\n",
    "\n",
    "# If df_backtest not found on disk, try to use variable in global namespace (user may have it in memory)\n",
    "if df_backtest is None:\n",
    "    try:\n",
    "        # 'df_backtest' variable might exist in user's global scope\n",
    "        df_backtest = globals().get(\"df_backtest\", None)\n",
    "        if df_backtest is not None:\n",
    "            # save a copy to OUTPUT_DIR for reproducibility\n",
    "            try:\n",
    "                df_backtest.to_csv(df_backtest_path, index=False)\n",
    "                print(f\"[Info] Saved df_backtest from memory to {df_backtest_path}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        df_backtest = None\n",
    "\n",
    "if df_backtest is None:\n",
    "    # fallback: try to build minimal px from trade_log (not ideal)\n",
    "    raise FileNotFoundError(f\"df_backtest.csv not found in {OUTPUT_DIR} and variable df_backtest not present in memory. Block 10 must produce price history (df_backtest).\")\n",
    "\n",
    "# fix timestamp/date column name\n",
    "if \"timestamp\" not in df_backtest.columns:\n",
    "    if \"date\" in df_backtest.columns:\n",
    "        df_backtest = df_backtest.rename(columns={\"date\": \"timestamp\"})\n",
    "    elif \"datetime\" in df_backtest.columns:\n",
    "        df_backtest = df_backtest.rename(columns={\"datetime\": \"timestamp\"})\n",
    "    else:\n",
    "        raise KeyError(\"df_backtest must contain a 'timestamp' or 'date' column\")\n",
    "\n",
    "df_backtest[\"timestamp\"] = pd.to_datetime(df_backtest[\"timestamp\"], errors=\"coerce\")\n",
    "if df_backtest[\"timestamp\"].isna().all():\n",
    "    raise ValueError(\"df_backtest timestamp column could not be parsed as datetimes\")\n",
    "\n",
    "# ensure we have close price col\n",
    "if \"close\" not in df_backtest.columns:\n",
    "    raise KeyError(\"df_backtest must contain 'close' column\")\n",
    "\n",
    "# create price structures\n",
    "price_map = df_backtest.set_index([\"ticker\", \"timestamp\"])[\"close\"]\n",
    "px_wide = df_backtest.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "\n",
    "# --------- Load weights_by_ticker, portfolio value, regime_daily if present ----------\n",
    "weights_path = os.path.join(OUTPUT_DIR, \"weights_by_ticker.csv\")\n",
    "weights_df = pd.read_csv(weights_path, index_col=0, parse_dates=True) if os.path.exists(weights_path) else None\n",
    "\n",
    "pv_path = os.path.join(OUTPUT_DIR, \"portfolio_value_test.csv\")\n",
    "pv = None\n",
    "if os.path.exists(pv_path):\n",
    "    try:\n",
    "        pv = pd.read_csv(pv_path, index_col=0, parse_dates=True).iloc[:, 0]\n",
    "    except Exception:\n",
    "        pv = None\n",
    "\n",
    "regime_path = os.path.join(OUTPUT_DIR, \"regime_daily.csv\")\n",
    "regime_df = pd.read_csv(regime_path, parse_dates=[\"date\"]).set_index(\"date\") if os.path.exists(regime_path) else None\n",
    "\n",
    "# --------- Prepare trade_log canonical columns and settlement dates ----------\n",
    "# Add status column (open/closed) based on exit_date presence\n",
    "trade_log[\"status\"] = np.where(pd.notna(trade_log[\"exit_date\"]), \"closed\", \"open\")\n",
    "\n",
    "# compute settlement dates using EXECUTION_LAG (if trade already contains 'settlement' columns, prefer them)\n",
    "# prefer user-provided settle columns if exist:\n",
    "entry_settle_col = choose_column(trade_log, [\"entry_settlement_date\", \"entry_settle_date\", \"entry_settled\"])\n",
    "exit_settle_col  = choose_column(trade_log, [\"exit_settlement_date\", \"exit_settle_date\", \"exit_settled\"])\n",
    "\n",
    "if entry_settle_col:\n",
    "    trade_log[\"entry_settle_date\"] = pd.to_datetime(trade_log[entry_settle_col], errors=\"coerce\")\n",
    "else:\n",
    "    trade_log[\"entry_settle_date\"] = trade_log[\"entry_date\"] + pd.to_timedelta(EXECUTION_LAG, unit=\"D\")\n",
    "\n",
    "if exit_settle_col:\n",
    "    trade_log[\"exit_settle_date\"] = pd.to_datetime(trade_log[exit_settle_col], errors=\"coerce\")\n",
    "else:\n",
    "    # if exit_date is NaT, keep exit_settle_date as NaT\n",
    "    trade_log[\"exit_settle_date\"] = pd.to_datetime(trade_log[\"exit_date\"], errors=\"coerce\") + pd.to_timedelta(EXECUTION_LAG, unit=\"D\")\n",
    "    trade_log.loc[trade_log[\"exit_date\"].isna(), \"exit_settle_date\"] = pd.NaT\n",
    "\n",
    "# --------- Choose date range to iterate (based on portfolio nav if possible) ----------\n",
    "if pv is not None:\n",
    "    start_date = pd.to_datetime(pv.index.min())\n",
    "    end_date = pd.to_datetime(pv.index.max())\n",
    "else:\n",
    "    # fallback to union of df_backtest dates\n",
    "    start_date = df_backtest[\"timestamp\"].min()\n",
    "    end_date = df_backtest[\"timestamp\"].max()\n",
    "\n",
    "dates = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "print(f\"[Info] Will generate snapshots for {len(dates)} days: {start_date.date()} -> {end_date.date()}\")\n",
    "\n",
    "# --------- Create output subfolders (optional) ----------\n",
    "SNAP_DIR = OUTPUT_DIR  # same folder; user requested simple filenames\n",
    "# --------- Main loop: for each day, create snapshots, signals, payload ----------\n",
    "created_snapshots = 0\n",
    "created_signals = 0\n",
    "created_payloads = 0\n",
    "\n",
    "# Precompute: last available price per (ticker, date) to speed up repeated lookups\n",
    "# (we will still use price_map+px_wide per get_price_on_or_before)\n",
    "for i, d in enumerate(dates):\n",
    "    # use pd.Timestamp (normalize to midnight)\n",
    "    d = pd.Timestamp(d).normalize()\n",
    "    # --- determine open positions at EOD d using settlement dates ---\n",
    "    # A position is considered \"open at EOD d\" if entry_settle_date <= d and (exit_settle_date is NA or exit_settle_date > d)\n",
    "    mask_open = (trade_log[\"entry_settle_date\"].notna()) & (trade_log[\"entry_settle_date\"] <= d) & (\n",
    "        (trade_log[\"exit_settle_date\"].isna()) | (trade_log[\"exit_settle_date\"] > d)\n",
    "    )\n",
    "    open_pos_df = trade_log.loc[mask_open].copy().reset_index(drop=True)\n",
    "\n",
    "    # enrich open positions with last_price as of d (use get_price_on_or_before)\n",
    "    enriched_rows = []\n",
    "    for idx, row in open_pos_df.iterrows():\n",
    "        tk = row[\"ticker\"]\n",
    "        entry_px = row.get(\"entry_price\", np.nan)\n",
    "        last_px = get_price_on_or_before(px_wide, price_map, tk, d)\n",
    "        if not pd.isna(entry_px) and not pd.isna(last_px):\n",
    "            unreal_pct = (last_px / float(entry_px) - 1.0) * 100.0\n",
    "        else:\n",
    "            unreal_pct = np.nan\n",
    "        # position size pct from weights_df if exists\n",
    "        pos_size = 0.0\n",
    "        if weights_df is not None:\n",
    "            try:\n",
    "                if d in weights_df.index and tk in weights_df.columns:\n",
    "                    pos_size = float(weights_df.loc[d, tk])\n",
    "                else:\n",
    "                    pos_size = 0.0\n",
    "            except Exception:\n",
    "                pos_size = 0.0\n",
    "        enriched_rows.append({\n",
    "            \"snapshot_date\": d,\n",
    "            \"ticker\": tk,\n",
    "            \"entry_date\": row.get(\"entry_date\", pd.NaT),\n",
    "            \"entry_settle_date\": row.get(\"entry_settle_date\", pd.NaT),\n",
    "            \"entry_price\": entry_px,\n",
    "            \"last_price\": last_px,\n",
    "            \"current_unrealized_pct\": unreal_pct,\n",
    "            \"tp_level\": row.get(\"tp_level\", np.nan),\n",
    "            \"sl_level\": row.get(\"sl_level\", np.nan),\n",
    "            \"entry_regime\": row.get(\"entry_regime\", None),\n",
    "            \"position_size_pct\": pos_size\n",
    "        })\n",
    "\n",
    "    snapshot_df = pd.DataFrame(enriched_rows, columns=[\n",
    "        \"snapshot_date\",\"ticker\",\"entry_date\",\"entry_settle_date\",\"entry_price\",\"last_price\",\n",
    "        \"current_unrealized_pct\",\"tp_level\",\"sl_level\",\"entry_regime\",\"position_size_pct\"\n",
    "    ])\n",
    "    # Save snapshot even if empty\n",
    "    snap_fname = os.path.join(SNAP_DIR, f\"positions_snapshot_{d.date()}.csv\")\n",
    "    snapshot_df.to_csv(snap_fname, index=False)\n",
    "    created_snapshots += 1\n",
    "\n",
    "    # --- signals_today: rows where entry_date == d or exit_date == d (these are trade timestamps, not settle) ---\n",
    "    signals_mask = ( (trade_log[\"entry_date\"].notna()) & (trade_log[\"entry_date\"].dt.normalize() == d) ) | \\\n",
    "                   ( (trade_log[\"exit_date\"].notna()) & (trade_log[\"exit_date\"].dt.normalize() == d) )\n",
    "    signals_df = trade_log.loc[signals_mask].copy().reset_index(drop=True)\n",
    "    # add action column\n",
    "    def _action(r):\n",
    "        if pd.notna(r.get(\"entry_date\")) and pd.to_datetime(r[\"entry_date\"]).normalize() == d and (pd.isna(r.get(\"exit_date\")) or pd.to_datetime(r.get(\"exit_date\")).normalize() != d):\n",
    "            return \"BUY\"\n",
    "        if pd.notna(r.get(\"exit_date\")) and pd.to_datetime(r[\"exit_date\"]).normalize() == d:\n",
    "            # If both entry and exit same day, treat exit as SELL\n",
    "            return \"SELL\"\n",
    "        return \"BUY\" if pd.notna(r.get(\"entry_date\")) and pd.to_datetime(r[\"entry_date\"]).normalize() == d else \"SELL\"\n",
    "    if not signals_df.empty:\n",
    "        signals_df[\"action\"] = signals_df.apply(_action, axis=1)\n",
    "    # Save signals file\n",
    "    sig_fname = os.path.join(SNAP_DIR, f\"signals_today_{d.date()}.csv\")\n",
    "    # select sensible columns to save\n",
    "    out_sig_cols = []\n",
    "    for c in [\"ticker\",\"action\",\"entry_date\",\"exit_date\",\"entry_price\",\"exit_price\",\"tp_level\",\"sl_level\",\"exit_type\",\"status\"]:\n",
    "        if c in signals_df.columns:\n",
    "            out_sig_cols.append(c)\n",
    "    if len(out_sig_cols)==0:\n",
    "        # fallback to save everything\n",
    "        signals_df.to_csv(sig_fname, index=False)\n",
    "    else:\n",
    "        signals_df[out_sig_cols].to_csv(sig_fname, index=False)\n",
    "    created_signals += 1\n",
    "\n",
    "    # --- telegram payload JSON for this date ---\n",
    "    # regime and cash_buffer\n",
    "    if regime_df is not None and d in regime_df.index:\n",
    "        regime_row = regime_df.loc[d]\n",
    "        # try to standardize regime/cash_buffer names\n",
    "        regime_val = regime_row.get(\"regime\", None) if hasattr(regime_row, \"get\") else regime_row.get(\"entry_regime\", None) if \"entry_regime\" in regime_row.index else None\n",
    "        cash_buffer = regime_row.get(\"cash_buffer\", None) if hasattr(regime_row, \"get\") else None\n",
    "        # fallback if regime row is a Series without .get\n",
    "        if isinstance(regime_row, (pd.Series, )):\n",
    "            if \"regime\" in regime_row.index:\n",
    "                regime_val = regime_row[\"regime\"]\n",
    "            elif \"entry_regime\" in regime_row.index:\n",
    "                regime_val = regime_row[\"entry_regime\"]\n",
    "            if \"cash_buffer\" in regime_row.index:\n",
    "                cash_buffer = float(regime_row[\"cash_buffer\"])\n",
    "    else:\n",
    "        regime_val = None\n",
    "        cash_buffer = None\n",
    "    # nav at date\n",
    "    nav_val = None\n",
    "    if pv is not None:\n",
    "        try:\n",
    "            # take last NAV at or before d\n",
    "            nav_val = float(pv.loc[:d].iloc[-1])\n",
    "        except Exception:\n",
    "            nav_val = None\n",
    "\n",
    "    # Build payload dict\n",
    "    payload = {\n",
    "        \"date\": str(d.date()),\n",
    "        \"regime\": regime_val if regime_val is not None else None,\n",
    "        \"cash_buffer\": float(cash_buffer) if (cash_buffer is not None and not pd.isna(cash_buffer)) else None,\n",
    "        \"nav\": float(nav_val) if (nav_val is not None and not pd.isna(nav_val)) else None,\n",
    "        \"positions\": [],\n",
    "        \"signals\": []\n",
    "    }\n",
    "\n",
    "    # positions: from snapshot_df\n",
    "    for _, r in snapshot_df.iterrows():\n",
    "        payload[\"positions\"].append({\n",
    "            \"ticker\": r[\"ticker\"],\n",
    "            \"entry_date\": str(r[\"entry_date\"].date()) if pd.notna(r[\"entry_date\"]) else None,\n",
    "            \"entry_settle_date\": str(r[\"entry_settle_date\"].date()) if pd.notna(r[\"entry_settle_date\"]) else None,\n",
    "            \"entry_price\": float(r[\"entry_price\"]) if not pd.isna(r[\"entry_price\"]) else None,\n",
    "            \"last_price\": float(r[\"last_price\"]) if not pd.isna(r[\"last_price\"]) else None,\n",
    "            \"unrealized_pct\": float(r[\"current_unrealized_pct\"]) if not pd.isna(r[\"current_unrealized_pct\"]) else None,\n",
    "            \"tp_level\": float(r[\"tp_level\"]) if not pd.isna(r[\"tp_level\"]) else None,\n",
    "            \"sl_level\": float(r[\"sl_level\"]) if not pd.isna(r[\"sl_level\"]) else None,\n",
    "            \"position_size_pct\": float(r[\"position_size_pct\"]) if not pd.isna(r[\"position_size_pct\"]) else None,\n",
    "            \"entry_regime\": r.get(\"entry_regime\", None)\n",
    "        })\n",
    "\n",
    "    # signals: from signals_df\n",
    "    for _, r in signals_df.iterrows():\n",
    "        action = r.get(\"action\", None) if \"action\" in r.index else None\n",
    "        payload[\"signals\"].append({\n",
    "            \"ticker\": r.get(\"ticker\", None),\n",
    "            \"action\": action,\n",
    "            \"entry_date\": str(r[\"entry_date\"].date()) if pd.notna(r.get(\"entry_date\")) else None,\n",
    "            \"exit_date\": str(r[\"exit_date\"].date()) if pd.notna(r.get(\"exit_date\")) else None,\n",
    "            \"entry_price\": float(r[\"entry_price\"]) if not pd.isna(r.get(\"entry_price\", np.nan)) else None,\n",
    "            \"exit_price\": float(r[\"exit_price\"]) if not pd.isna(r.get(\"exit_price\", np.nan)) else None,\n",
    "            \"tp_level\": float(r.get(\"tp_level\")) if (r.get(\"tp_level\") is not None and not pd.isna(r.get(\"tp_level\"))) else None,\n",
    "            \"sl_level\": float(r.get(\"sl_level\")) if (r.get(\"sl_level\") is not None and not pd.isna(r.get(\"sl_level\"))) else None,\n",
    "            \"exit_type\": r.get(\"exit_type\", None),\n",
    "            \"status\": r.get(\"status\", None)\n",
    "        })\n",
    "\n",
    "    payload_fname = os.path.join(SNAP_DIR, f\"telegram_payload_{d.date()}.json\")\n",
    "    with open(payload_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2, default=str)\n",
    "    created_payloads += 1\n",
    "\n",
    "    # progress print\n",
    "    if (i + 1) % 50 == 0 or (i + 1) == len(dates):\n",
    "        print(f\"[Progress] Generated {i+1}/{len(dates)} days - last date: {d.date()}\")\n",
    "\n",
    "# --------- Enrich trade_log with last price as-of last_date and unrealized PnL where open ----------\n",
    "# We'll compute 'last_price' (using end_date) and 'current_unrealized_pct' for open trades\n",
    "last_date = pd.Timestamp(dates.max()).normalize()\n",
    "trade_log_detailed = trade_log.copy()\n",
    "\n",
    "# compute last price per trade row (use last available price on or before last_date)\n",
    "_last_prices = []\n",
    "_curr_unreal = []\n",
    "for idx, r in trade_log_detailed.iterrows():\n",
    "    tk = r[\"ticker\"]\n",
    "    entry_px = r.get(\"entry_price\", np.nan)\n",
    "    last_px = get_price_on_or_before(px_wide, price_map, tk, last_date)\n",
    "    _last_prices.append(last_px)\n",
    "    if pd.notna(entry_px) and pd.notna(last_px):\n",
    "        _curr_unreal.append((last_px / float(entry_px) - 1.0) * 100.0)\n",
    "    else:\n",
    "        _curr_unreal.append(np.nan)\n",
    "\n",
    "trade_log_detailed[\"last_price_asof\"] = _last_prices\n",
    "trade_log_detailed[\"current_unrealized_pct_asof_last_date\"] = _curr_unreal\n",
    "\n",
    "# Save trade_log_detailed\n",
    "detailed_path = os.path.join(OUTPUT_DIR, \"trade_log_detailed.csv\")\n",
    "trade_log_detailed.to_csv(detailed_path, index=False)\n",
    "print(f\"[Saved] trade_log_detailed -> {detailed_path}\")\n",
    "\n",
    "# Summary prints\n",
    "print(\"=== Done Block 10.5 ===\")\n",
    "print(f\"Snapshots created: {created_snapshots} (files positions_snapshot_YYYY-MM-DD.csv)\")\n",
    "print(f\"Signals created:   {created_signals} (files signals_today_YYYY-MM-DD.csv)\")\n",
    "print(f\"Payloads created:  {created_payloads} (files telegram_payload_YYYY-MM-DD.json)\")\n",
    "print(f\"Trade log detailed: {detailed_path}\")\n",
    "print(f\"Example snapshot file (first): {os.path.join(SNAP_DIR, f'positions_snapshot_{dates[0].date()}.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc5b470",
   "metadata": {},
   "source": [
    "**Block 11: Thống kê kết quả và vẽ biểu đồ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3329dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Kết quả Test:\n",
      "Ngày bắt đầu                        2025-02-05\n",
      "Ngày kết thúc                       2025-09-29\n",
      "Giá trị cuối                      11690.494071\n",
      "ROI (%)                              17.104017\n",
      "Biến động (năm, %)                   28.069246\n",
      "Sharpe                                1.008656\n",
      "Sortino                               1.591649\n",
      "MaxDrawdown (%)                     -19.096715\n",
      "Tỷ lệ phiên thắng (%)                41.717791\n",
      "Số ngày                                    163\n",
      "Skewness                              0.511906\n",
      "Kurtosis                              1.987763\n",
      "Calmar                                 1.44777\n",
      "Giá trị cuối (VNINDEX)            13125.920558\n",
      "ROI VNINDEX (%)                      31.259206\n",
      "Chênh lệch so với VNINDEX (pp)      -14.155188\n",
      "InformationRatio                     -0.541155\n",
      "Alpha (daily)                         0.000426\n",
      "Beta                                  0.395968\n",
      "dtype: object\n",
      "\n",
      "📊 Stress Test (Trump áp thuế 46%):\n",
      "Ngày bắt đầu                        2025-03-26\n",
      "Ngày kết thúc                       2025-04-15\n",
      "Giá trị cuối                      12208.217189\n",
      "ROI (%)                              -3.607095\n",
      "Biến động (năm, %)                   53.411159\n",
      "Sharpe                               -0.989252\n",
      "Sortino                              -2.161474\n",
      "MaxDrawdown (%)                     -17.601508\n",
      "Tỷ lệ phiên thắng (%)                42.857143\n",
      "Số ngày                                     14\n",
      "Skewness                              0.275881\n",
      "Kurtosis                             -0.978011\n",
      "Calmar                               -2.748674\n",
      "Giá trị cuối (VNINDEX)             9670.607509\n",
      "ROI VNINDEX (%)                       -7.41277\n",
      "Chênh lệch so với VNINDEX (pp)        3.805675\n",
      "InformationRatio                      2.050711\n",
      "Alpha (daily)                         0.001597\n",
      "Beta                                  0.756845\n",
      "dtype: object\n",
      "\n",
      "📊 Trade stats:\n",
      "Turnover mean (daily)       0.349115\n",
      "Turnover median (daily)     0.010241\n",
      "Turnover max (daily)        1.959999\n",
      "Turnover annualized        87.976929\n",
      "dtype: float64\n",
      "\n",
      "📊 Regime performance:\n",
      "             mean       std  count    Sharpe\n",
      "regime                                      \n",
      "bear    -0.000931  0.024578     38 -0.601479\n",
      "bull     0.003127  0.016817     71  2.952161\n",
      "sideway -0.000065  0.012270     54 -0.084665\n",
      "\n",
      "✅ Block 11 hoàn tất. Stats saved: ./backtest_ddpg/stats_test.csv, Charts & files under: ./backtest_ddpg/\n"
     ]
    }
   ],
   "source": [
    "# Block 11 — Hiệu suất & Stress Test (Final, full stats + regime analysis)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# ---- Config / paths ----\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "STATS_FILE = os.path.join(OUTPUT_DIR, \"stats_test.csv\")\n",
    "\n",
    "# Stress events cấu hình ở đây\n",
    "STRESS_EVENTS = [\n",
    "    {\"name\": \"Trump áp thuế 46%\", \"start\": \"2025-03-26\", \"end\": \"2025-04-15\"},\n",
    "]\n",
    "\n",
    "# ---- Helpers ----\n",
    "def max_drawdown(series: pd.Series) -> float:\n",
    "    peak = series.cummax()\n",
    "    dd = (series / peak) - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def calmar_ratio(port_val: pd.Series) -> float:\n",
    "    dd = max_drawdown(port_val)\n",
    "    if dd == 0 or np.isclose(dd, 0):\n",
    "        return np.nan\n",
    "    total_ret = port_val.iloc[-1] / port_val.iloc[0] - 1.0\n",
    "    ann_ret = (1 + total_ret) ** (252.0 / len(port_val)) - 1.0\n",
    "    return float(ann_ret / abs(dd))\n",
    "\n",
    "def information_ratio(port_ret: pd.Series, bench_ret: pd.Series) -> float:\n",
    "    ex = port_ret - bench_ret\n",
    "    if ex.std() == 0:\n",
    "        return np.nan\n",
    "    return float(ex.mean() / ex.std() * np.sqrt(252))\n",
    "\n",
    "def compute_alpha_beta(port_ret: pd.Series, bench_ret: pd.Series):\n",
    "    idx = port_ret.index.intersection(bench_ret.index)\n",
    "    if len(idx) < 2:\n",
    "        return np.nan, np.nan\n",
    "    y = port_ret.loc[idx].values\n",
    "    x = bench_ret.loc[idx].values\n",
    "    if np.isclose(x.std(), 0):\n",
    "        return np.nan, np.nan\n",
    "    X = sm.add_constant(x)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    alpha, beta = float(model.params[0]), float(model.params[1])\n",
    "    return alpha, beta\n",
    "\n",
    "def stats_from_series(port_val: pd.Series, bench_val: pd.Series | None = None):\n",
    "    port_ret = port_val.pct_change().fillna(0.0)\n",
    "    statsd = {}\n",
    "    statsd[\"Ngày bắt đầu\"] = port_val.index.min().strftime(\"%Y-%m-%d\")\n",
    "    statsd[\"Ngày kết thúc\"] = port_val.index.max().strftime(\"%Y-%m-%d\")\n",
    "    statsd[\"Giá trị cuối\"] = float(port_val.iloc[-1])\n",
    "    statsd[\"ROI (%)\"] = float((port_val.iloc[-1] / port_val.iloc[0] - 1.0) * 100)\n",
    "    ann_vol = float(port_ret.std() * np.sqrt(252) * 100)\n",
    "    statsd[\"Biến động (năm, %)\"] = ann_vol\n",
    "    statsd[\"Sharpe\"] = float((port_ret.mean() / port_ret.std() * np.sqrt(252)) if port_ret.std() > 0 else 0.0)\n",
    "    neg_std = port_ret[port_ret < 0].std()\n",
    "    statsd[\"Sortino\"] = float((port_ret.mean() / neg_std * np.sqrt(252)) if (neg_std > 0) else 0.0)\n",
    "    statsd[\"MaxDrawdown (%)\"] = float(max_drawdown(port_val) * 100)\n",
    "    statsd[\"Tỷ lệ phiên thắng (%)\"] = float((port_ret > 0).mean() * 100)\n",
    "    statsd[\"Số ngày\"] = int(len(port_ret))\n",
    "    statsd[\"Skewness\"] = float(stats.skew(port_ret.dropna()))\n",
    "    statsd[\"Kurtosis\"] = float(stats.kurtosis(port_ret.dropna()))\n",
    "    statsd[\"Calmar\"] = float(calmar_ratio(port_val))\n",
    "\n",
    "    if bench_val is not None and len(bench_val) > 1:\n",
    "        bench_ret = bench_val.pct_change().fillna(0.0)\n",
    "        statsd[\"Giá trị cuối (VNINDEX)\"] = float(bench_val.iloc[-1])\n",
    "        statsd[\"ROI VNINDEX (%)\"] = float((bench_val.iloc[-1] / bench_val.iloc[0] - 1.0) * 100)\n",
    "        statsd[\"Chênh lệch so với VNINDEX (pp)\"] = statsd[\"ROI (%)\"] - statsd[\"ROI VNINDEX (%)\"]\n",
    "        statsd[\"InformationRatio\"] = float(information_ratio(port_ret, bench_ret))\n",
    "        a,b = compute_alpha_beta(port_ret, bench_ret)\n",
    "        statsd[\"Alpha (daily)\"] = float(a) if not pd.isna(a) else np.nan\n",
    "        statsd[\"Beta\"] = float(b) if not pd.isna(b) else np.nan\n",
    "\n",
    "    return statsd\n",
    "\n",
    "def plot_equity(port_val, bench_val, title, path):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(port_val, label=\"Chiến lược\", linewidth=1.6)\n",
    "    if bench_val is not None:\n",
    "        plt.plot(bench_val, label=\"VNINDEX (Buy&Hold)\", linewidth=1.2)\n",
    "    plt.title(title); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "def plot_hist(port_ret, title, path, bench_ret=None):\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.hist(port_ret.dropna()*100, bins=50, alpha=0.6, label=\"Chiến lược\")\n",
    "    if bench_ret is not None:\n",
    "        plt.hist(bench_ret.dropna()*100, bins=50, alpha=0.6, label=\"VNINDEX\")\n",
    "    plt.title(title); plt.xlabel(\"Daily Return (%)\"); plt.ylabel(\"Tần suất\")\n",
    "    plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "# ---- Load results ----\n",
    "pv_path = os.path.join(OUTPUT_DIR, \"portfolio_value_test.csv\")\n",
    "bv_path = os.path.join(OUTPUT_DIR, \"benchmark_value_test.csv\")\n",
    "weights_path = os.path.join(OUTPUT_DIR, \"weights_by_ticker.csv\")\n",
    "regime_path = os.path.join(OUTPUT_DIR, \"regime_daily.csv\")\n",
    "df_backtest_path = os.path.join(OUTPUT_DIR, \"df_backtest.csv\")\n",
    "\n",
    "if not os.path.exists(pv_path):\n",
    "    raise FileNotFoundError(f\"{pv_path} not found. Run Block 10 first.\")\n",
    "\n",
    "port_val = pd.read_csv(pv_path, index_col=0, parse_dates=True).iloc[:,0].sort_index()\n",
    "bench_val = None\n",
    "if os.path.exists(bv_path):\n",
    "    bench_val = pd.read_csv(bv_path, index_col=0, parse_dates=True).iloc[:,0].sort_index()\n",
    "    bench_val = bench_val.reindex(port_val.index).ffill().bfill()\n",
    "\n",
    "returns = port_val.pct_change().fillna(0.0)\n",
    "bench_ret = bench_val.pct_change().fillna(0.0) if bench_val is not None else None\n",
    "\n",
    "# ---- Compute & save stats ----\n",
    "stats_test = stats_from_series(port_val, bench_val)\n",
    "pd.DataFrame({\"metric\": list(stats_test.keys()), \"value\": list(stats_test.values())}).to_csv(STATS_FILE, index=False)\n",
    "\n",
    "# ---- Plots toàn kỳ ----\n",
    "plot_equity(port_val, bench_val, \"Equity Curve (Test)\", os.path.join(OUTPUT_DIR, \"equity_test.png\"))\n",
    "plot_hist(returns, \"Histogram lợi nhuận ngày (Test)\", os.path.join(OUTPUT_DIR, \"hist_test.png\"), bench_ret)\n",
    "\n",
    "# Drawdown\n",
    "cum_ret = (1+returns).cumprod()\n",
    "dd = cum_ret / cum_ret.cummax() - 1\n",
    "plt.figure(figsize=(10,4)); plt.plot(dd, color=\"red\")\n",
    "plt.title(\"Drawdown toàn kỳ\"); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,\"drawdown_test.png\")); plt.close()\n",
    "\n",
    "# Rolling Sharpe\n",
    "window = 60\n",
    "roll_sharpe = returns.rolling(window).mean() / returns.rolling(window).std()\n",
    "plt.figure(figsize=(10,4)); plt.plot(roll_sharpe, label=\"Rolling Sharpe (60d)\")\n",
    "plt.axhline(0, color=\"grey\", ls=\"--\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.title(\"Rolling Sharpe (toàn kỳ)\")\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,\"rolling_sharpe.png\")); plt.close()\n",
    "\n",
    "# Rolling Beta\n",
    "if bench_val is not None:\n",
    "    betas = []\n",
    "    for i in range(len(returns) - window):\n",
    "        y = returns.iloc[i:i+window]; x = bench_ret.iloc[i:i+window]\n",
    "        if x.std() == 0: betas.append(np.nan); continue\n",
    "        X = sm.add_constant(x); model = sm.OLS(y, X).fit(); betas.append(model.params[1])\n",
    "    roll_beta = pd.Series(betas, index=returns.index[window:])\n",
    "    plt.figure(figsize=(10,4)); plt.plot(roll_beta, label=\"Rolling Beta (60d)\")\n",
    "    plt.axhline(1, color=\"grey\", ls=\"--\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.title(\"Rolling Beta (toàn kỳ)\")\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,\"rolling_beta.png\")); plt.close()\n",
    "\n",
    "# ---- Monthly heatmap ----\n",
    "monthly = returns.resample(\"M\").agg([\"sum\",\"count\", lambda x: (x>0).mean()])\n",
    "monthly.columns = [\"Return\",\"Trades\",\"WinRate\"]\n",
    "monthly.index.name = \"Date\"\n",
    "\n",
    "pivot2 = monthly.reset_index()\n",
    "pivot2[\"year\"] = pivot2[\"Date\"].dt.year\n",
    "pivot2[\"month\"] = pivot2[\"Date\"].dt.month\n",
    "heat = pivot2.pivot_table(values=\"Return\", index=\"year\", columns=\"month\")\n",
    "plt.figure(figsize=(12,6)); sns.heatmap(heat, annot=True, fmt=\".2%\", cmap=\"RdYlGn\", center=0)\n",
    "plt.title(\"Heatmap lợi nhuận hàng tháng\"); plt.savefig(os.path.join(OUTPUT_DIR,\"heatmap_monthly.png\")); plt.close()\n",
    "monthly.to_csv(os.path.join(OUTPUT_DIR,\"monthly_stats.csv\"))\n",
    "\n",
    "# ---- Stress Test ----\n",
    "stress_stats = {}\n",
    "for ev in STRESS_EVENTS:\n",
    "    start, end, name = pd.Timestamp(ev[\"start\"]), pd.Timestamp(ev[\"end\"]), ev[\"name\"]\n",
    "    sub_port = port_val.loc[start:end]\n",
    "    sub_bench = bench_val.loc[start:end] if bench_val is not None else None\n",
    "    if len(sub_port) > 1:\n",
    "        stats_stress = stats_from_series(sub_port, sub_bench)\n",
    "        stress_stats[name] = stats_stress\n",
    "        plot_equity(sub_port, sub_bench, f\"Equity ({name})\", os.path.join(OUTPUT_DIR,f\"equity_stress_{name}.png\"))\n",
    "        sub_ret = sub_port.pct_change().fillna(0.0)\n",
    "        plot_hist(sub_ret, f\"Histogram ({name})\", os.path.join(OUTPUT_DIR,f\"hist_stress_{name}.png\"))\n",
    "        cum_p = (1+sub_ret).cumprod(); dd_p = cum_p / cum_p.cummax() - 1\n",
    "        plt.figure(figsize=(10,4)); plt.plot(dd_p, label=\"Chiến lược\", color=\"red\")\n",
    "        if sub_bench is not None:\n",
    "            cum_b = (1 + sub_bench.pct_change().fillna(0.0)).cumprod(); dd_b = cum_b / cum_b.cummax() - 1\n",
    "            plt.plot(dd_b, label=\"VNINDEX\", color=\"blue\")\n",
    "        plt.title(f\"Drawdown ({name})\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR,f\"drawdown_stress_{name}.png\")); plt.close()\n",
    "\n",
    "# ---- Top holdings & performance ----\n",
    "if os.path.exists(weights_path):\n",
    "    try:\n",
    "        weights = pd.read_csv(weights_path, index_col=0, parse_dates=True)\n",
    "        mean_w = weights.mean().sort_values(ascending=False).head(10)\n",
    "        plt.figure(figsize=(10,6)); mean_w.plot(kind=\"bar\")\n",
    "        plt.title(\"Top 10 cổ phiếu phân bổ vốn cao nhất\"); plt.ylabel(\"Tỷ trọng\")\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR,\"top10_weights.png\")); plt.close()\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot plot top weights:\", e)\n",
    "\n",
    "if os.path.exists(df_backtest_path):\n",
    "    try:\n",
    "        df_backtest = pd.read_csv(df_backtest_path, parse_dates=[\"timestamp\"])\n",
    "        df_backtest = df_backtest.groupby([\"timestamp\",\"ticker\"], as_index=False).agg({\"close\":\"last\"})\n",
    "        px = df_backtest.pivot(index=\"timestamp\", columns=\"ticker\", values=\"close\").sort_index()\n",
    "        ret_wide = px.pct_change().fillna(0.0)\n",
    "        cum_ret_tk = (1+ret_wide).cumprod().iloc[-1] - 1\n",
    "        top_gain = cum_ret_tk.sort_values(ascending=False).head(10)\n",
    "        top_loss = cum_ret_tk.sort_values().head(10)\n",
    "        plt.figure(figsize=(10,6)); top_gain.plot(kind=\"bar\",color=\"green\"); plt.title(\"Top 10 cổ phiếu lãi nhiều nhất\")\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR,\"top10_gain.png\")); plt.close()\n",
    "        plt.figure(figsize=(10,6)); top_loss.plot(kind=\"bar\",color=\"red\"); plt.title(\"Top 10 cổ phiếu lỗ nhiều nhất\")\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR,\"top10_loss.png\")); plt.close()\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot compute per-stock performance:\", e)\n",
    "\n",
    "# ---- Turnover stats ----\n",
    "trade_stats = {}\n",
    "if os.path.exists(weights_path):\n",
    "    try:\n",
    "        wdf = pd.read_csv(weights_path, index_col=0, parse_dates=True)\n",
    "        turnover_series = wdf.diff().abs().sum(axis=1)\n",
    "        trade_stats[\"Turnover mean (daily)\"] = float(turnover_series.mean())\n",
    "        trade_stats[\"Turnover median (daily)\"] = float(turnover_series.median())\n",
    "        trade_stats[\"Turnover max (daily)\"] = float(turnover_series.max())\n",
    "        trade_stats[\"Turnover annualized\"] = float(turnover_series.mean() * 252)\n",
    "        pd.DataFrame({\"metric\": list(trade_stats.keys()), \"value\": list(trade_stats.values())}).to_csv(os.path.join(OUTPUT_DIR,\"turnover_stats.csv\"), index=False)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: cannot compute turnover stats:\", e)\n",
    "\n",
    "# ---- Regime performance ----\n",
    "if os.path.exists(regime_path):\n",
    "    regime_df = pd.read_csv(regime_path, parse_dates=[\"date\"])\n",
    "    regime_df = regime_df.set_index(\"date\").reindex(port_val.index).fillna(method=\"ffill\")\n",
    "    regime_df[\"daily_ret\"] = returns\n",
    "    perf_by_regime = regime_df.groupby(\"regime\")[\"daily_ret\"].agg([\"mean\",\"std\",\"count\"])\n",
    "    perf_by_regime[\"Sharpe\"] = perf_by_regime[\"mean\"] / perf_by_regime[\"std\"] * np.sqrt(252)\n",
    "    perf_by_regime.to_csv(os.path.join(OUTPUT_DIR,\"regime_performance.csv\"))\n",
    "\n",
    "# ---- Save combined stats ----\n",
    "all_stats = {\"Test\": stats_test}\n",
    "all_stats.update({f\"Stress {k}\":v for k,v in stress_stats.items()})\n",
    "pd.DataFrame(all_stats).T.to_csv(STATS_FILE)\n",
    "\n",
    "# ---- Print summary ----\n",
    "print(\"📊 Kết quả Test:\"); print(pd.Series(stats_test))\n",
    "for k,v in stress_stats.items():\n",
    "    print(f\"\\n📊 Stress Test ({k}):\"); print(pd.Series(v))\n",
    "if trade_stats: print(\"\\n📊 Trade stats:\"); print(pd.Series(trade_stats))\n",
    "if os.path.exists(regime_path):\n",
    "    print(\"\\n📊 Regime performance:\"); print(perf_by_regime)\n",
    "print(f\"\\n✅ Block 11 hoàn tất. Stats saved: {STATS_FILE}, Charts & files under: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd5497",
   "metadata": {},
   "source": [
    "**Block 12A: gửi tín hiệu lên telegram với dữ liệu quá khứ**\n",
    "\n",
    "`Lưu ý` tuy quá khứ nhưng nhóm không để bị nhìn trước tương lai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ef6b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 12a — Hàm gửi tín hiệu Telegram (Offline replay, full version)\n",
    "\n",
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = \"./backtest_ddpg/\"\n",
    "\n",
    "# ==== Load config ====\n",
    "with open(\"config.json\",\"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "TG_TOKEN = cfg[\"telegram\"][\"bot_token\"]\n",
    "TG_CHAT_ID = cfg[\"telegram\"][\"chat_id\"]\n",
    "TG_THREAD_ID = cfg[\"telegram\"][\"message_thread_id\"]\n",
    "\n",
    "def send_daily_report(report_date: str | pd.Timestamp, sleep_sec:int=2):\n",
    "    \"\"\"\n",
    "    Gửi báo cáo Telegram cho 1 ngày bất kỳ trong backtest (offline replay).\n",
    "    - Hiển thị danh mục hiện tại (lọc tỷ trọng > 0)\n",
    "    - Hiển thị lệnh đóng hôm nay (PnL %)\n",
    "    - Hiển thị lệnh mở hôm nay (TP, SL, tỷ trọng)\n",
    "    - Gửi kèm chart equity\n",
    "    \"\"\"\n",
    "    report_date = pd.Timestamp(report_date)\n",
    "\n",
    "    # --- NAV ---\n",
    "    pv = pd.read_csv(os.path.join(OUTPUT_DIR,\"portfolio_value_test.csv\"), index_col=0, parse_dates=True).iloc[:,0]\n",
    "    if report_date not in pv.index:\n",
    "        print(f\"⚠️ {report_date.date()} không có trong NAV index\")\n",
    "        return\n",
    "    nav_today = pv.loc[:report_date].iloc[-1]\n",
    "\n",
    "    # --- Regime ---\n",
    "    regime_df = pd.read_csv(os.path.join(OUTPUT_DIR,\"regime_daily.csv\"), parse_dates=[\"date\"]).set_index(\"date\")\n",
    "    if report_date not in regime_df.index:\n",
    "        print(f\"⚠️ {report_date.date()} không có trong regime_daily.csv\")\n",
    "        return\n",
    "    row = regime_df.loc[report_date]\n",
    "    regime_today = row[\"regime\"]\n",
    "    cash_today = row[\"cash_buffer\"]\n",
    "\n",
    "    # --- Snapshot danh mục ---\n",
    "    snap_path = os.path.join(OUTPUT_DIR, f\"positions_snapshot_{report_date.date()}.csv\")\n",
    "    positions_txt = \"\"\n",
    "    if os.path.exists(snap_path):\n",
    "        pos = pd.read_csv(snap_path, parse_dates=[\"snapshot_date\",\"entry_date\"])\n",
    "        pos = pos[pos[\"position_size_pct\"] > 0]  # chỉ giữ vị thế còn vốn\n",
    "        if len(pos)>0:\n",
    "            for _,r in pos.iterrows():\n",
    "                positions_txt += (\n",
    "                    f\"— {r['ticker']}: Mua {r['entry_price']:.2f} ngày {r['entry_date'].date()}, \"\n",
    "                    f\"SL {r['sl_level']:.2f}, TP {r['tp_level']:.2f}, \"\n",
    "                    f\"Giá hiện {r['last_price']:.2f}, \"\n",
    "                    f\"Lãi/lỗ {r['current_unrealized_pct']:.2f}%, \"\n",
    "                    f\"Tỷ trọng {r['position_size_pct']*100:.1f}%\\n\"\n",
    "                )\n",
    "        else:\n",
    "            positions_txt = \"— Không có vị thế nào đang mở\\n\"\n",
    "    else:\n",
    "        positions_txt = \"— (Không tìm thấy snapshot)\\n\"\n",
    "\n",
    "    # --- Tín hiệu hôm đó ---\n",
    "    sig_path = os.path.join(OUTPUT_DIR, f\"signals_today_{report_date.date()}.csv\")\n",
    "    signals_txt, closed_txt, opened_txt = \"\", \"\", \"\"\n",
    "    if os.path.exists(sig_path):\n",
    "        sigs = pd.read_csv(sig_path, parse_dates=[\"entry_date\",\"exit_date\"])\n",
    "        if len(sigs)>0:\n",
    "            # Lệnh mở\n",
    "            opened = sigs[sigs[\"action\"]==\"BUY\"].copy()\n",
    "            if len(opened)>0:\n",
    "                for _,r in opened.iterrows():\n",
    "                    pos_size = r.get(\"position_size_pct\", 0.0) * 100\n",
    "                    opened_txt += (\n",
    "                        f\"— {r['ticker']}: Mua {r['entry_price']:.2f}, \"\n",
    "                        f\"TP {r['tp_level']:.2f}, SL {r['sl_level']:.2f}, \"\n",
    "                        f\"Tỷ trọng {pos_size:.1f}%\\n\"\n",
    "                    )\n",
    "            else:\n",
    "                opened_txt = \"— Không có lệnh mở nào\\n\"\n",
    "\n",
    "            # Lệnh đóng\n",
    "            closed = sigs[sigs[\"action\"]==\"SELL\"].copy()\n",
    "            if len(closed)>0:\n",
    "                for _,r in closed.iterrows():\n",
    "                    entry_price = r.get(\"entry_price\", None)\n",
    "                    exit_price  = r.get(\"exit_price\", None)\n",
    "                    pos_size    = r.get(\"position_size_pct\", 0.0) * 100\n",
    "                    if pd.notna(entry_price) and pd.notna(exit_price) and entry_price>0:\n",
    "                        pnl_pct = (exit_price/entry_price - 1)*100\n",
    "                    else:\n",
    "                        pnl_pct = float(\"nan\")\n",
    "                    closed_txt += (\n",
    "                        f\"— {r['ticker']}: Mua {entry_price:.2f} → \"\n",
    "                        f\"Bán {exit_price:.2f}, \"\n",
    "                        f\"{'Lãi' if pnl_pct>0 else 'Lỗ'} {pnl_pct:.2f}%, \"\n",
    "                        f\"Tỷ trọng {pos_size:.1f}%\\n\"\n",
    "                    )\n",
    "            else:\n",
    "                closed_txt = \"— Không có lệnh đóng nào\\n\"\n",
    "\n",
    "            # Tín hiệu gốc\n",
    "            for _,r in sigs.iterrows():\n",
    "                if r[\"action\"]==\"BUY\":\n",
    "                    signals_txt += (\n",
    "                        f\"🟢 MUA {r['ticker']} giá {r['entry_price']:.2f}, \"\n",
    "                        f\"TP {r['tp_level']:.2f}, SL {r['sl_level']:.2f}\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    exit_price = r[\"exit_price\"] if pd.notna(r[\"exit_price\"]) else 0.0\n",
    "                    signals_txt += (\n",
    "                        f\"🔴 BÁN {r['ticker']} giá {exit_price:.2f}, \"\n",
    "                        f\"loại {r.get('exit_type','NA')}\\n\"\n",
    "                    )\n",
    "        else:\n",
    "            signals_txt = \"— Không có tín hiệu giao dịch hôm nay\\n\"\n",
    "    else:\n",
    "        signals_txt = \"— (Không tìm thấy file tín hiệu)\\n\"\n",
    "        opened_txt = \"— (Không tìm thấy file tín hiệu)\\n\"\n",
    "        closed_txt = \"— (Không tìm thấy file tín hiệu)\\n\"\n",
    "\n",
    "    # --- Chart equity đến ngày đó ---\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(pv.loc[:report_date], label=\"Chiến lược\")\n",
    "    plt.title(f\"Equity đến {report_date.date()}\")\n",
    "    plt.grid(True, alpha=0.3); plt.legend()\n",
    "    chart_path = os.path.join(OUTPUT_DIR, f\"equity_until_{report_date.date()}.png\")\n",
    "    plt.savefig(chart_path, dpi=150); plt.close()\n",
    "\n",
    "    # --- Compose message ---\n",
    "    msg = f\"\"\"\n",
    "📅 Ngày {report_date.date()}\n",
    "\n",
    "💰 Giá trị tài sản: {nav_today:,.0f}\n",
    "💵 Tiền mặt: {cash_today*100:.1f}%\n",
    "📈 Chế độ thị trường: {regime_today}\n",
    "\n",
    "📊 Danh mục hiện tại:\n",
    "{positions_txt}\n",
    "\n",
    "💡 Lệnh đóng hôm nay:\n",
    "{closed_txt}\n",
    "\n",
    "🟢 Lệnh mở hôm nay:\n",
    "{opened_txt}\n",
    "\n",
    "📌 Tín hiệu trong ngày:\n",
    "{signals_txt}\n",
    "\"\"\".strip()\n",
    "\n",
    "    # --- Gửi text ---\n",
    "    send_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendMessage\"\n",
    "    requests.post(send_url, data={\n",
    "        \"chat_id\": TG_CHAT_ID,\n",
    "        \"message_thread_id\": TG_THREAD_ID,\n",
    "        \"text\": msg\n",
    "    })\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    # --- Gửi chart ---\n",
    "    photo_url = f\"https://api.telegram.org/bot{TG_TOKEN}/sendPhoto\"\n",
    "    with open(chart_path,\"rb\") as f:\n",
    "        requests.post(photo_url, data={\n",
    "            \"chat_id\": TG_CHAT_ID,\n",
    "            \"message_thread_id\": TG_THREAD_ID,\n",
    "            \"caption\": f\"Equity Curve đến {report_date.date()}\"\n",
    "        }, files={\"photo\":f})\n",
    "    time.sleep(sleep_sec)\n",
    "\n",
    "    print(f\"✅ Đã gửi báo cáo Telegram cho ngày {report_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e98b3c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-26\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-27\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-28\n",
      "⚠️ 2025-03-29 không có trong NAV index\n",
      "⚠️ 2025-03-30 không có trong NAV index\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-03-31\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-01\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-02\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-03\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-04\n",
      "⚠️ 2025-04-05 không có trong NAV index\n",
      "⚠️ 2025-04-06 không có trong NAV index\n",
      "⚠️ 2025-04-07 không có trong NAV index\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-08\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-09\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-10\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-11\n",
      "⚠️ 2025-04-12 không có trong NAV index\n",
      "⚠️ 2025-04-13 không có trong NAV index\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-14\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-15\n",
      "✅ Đã gửi báo cáo Telegram cho ngày 2025-04-16\n"
     ]
    }
   ],
   "source": [
    "# Cell runner — gửi báo cáo Telegram từ 26/03 đến 16/04/2025\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "start = pd.Timestamp(\"2025-03-26\")\n",
    "end   = pd.Timestamp(\"2025-04-16\")\n",
    "\n",
    "for d in pd.date_range(start, end, freq=\"D\"):\n",
    "    try:\n",
    "        send_daily_report(d, sleep_sec=10)  # sleep 2 giây giữa mỗi tin nhắn\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi gửi ngày {d.date()}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
